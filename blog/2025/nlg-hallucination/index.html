<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hallucination in NLG - Hallelujah yappin' ya | Linh N.T Vo </title> <meta name="author" content="Linh N.T Vo"> <meta name="description" content="in this post we will discover the world of hallucination in NLG"> <meta name="keywords" content="personal-blog, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://vtrnnhlinh.github.io/blog/2025/nlg-hallucination/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Linh</span> N.T Vo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Hallucination in NLG - Hallelujah yappin' ya</h1> <p class="post-meta"> Created on September 03, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/cse"> <i class="fa-solid fa-hashtag fa-sm"></i> cse</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/english"> <i class="fa-solid fa-hashtag fa-sm"></i> english</a>   ·   <a href="/blog/category/linh-the-engineer"> <i class="fa-solid fa-tag fa-sm"></i> Linh-the-Engineer</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>God knows why I researched about NLG Hallucination, but here you are, bro!</p> <p>I will use <a class="citation" href="#ji2023survey">(Ji et al., 2023)</a> and <a class="citation" href="#zhu2025can">(Zhu et al., 2025)</a> as the main source of this post. I may also cite other works but I can’t make sure I read all of them carefully, <em>ehem</em>. And as my current work is focus on text generation, so I will mainly yap about hallucination in this field.</p> <p>Okay. Get into the main dish. With the power of deep learning technologies, NLG (Natural Language Generation) develops rapidly. That also leads to the attention for limitations and risks increased, and they find out NLG models often generate nonsense, unfaithful text!</p> <p>And they call it <strong>hallucination</strong>.</p> <p><strong>Hallucination</strong> is a problem that we need to be careful with. Because it hinders performance, raises safe concerns, and leads to potential privacy violations. Like, just imagine <em>hallucination</em> in medical application AI <img class="emoji" title=":skull:" alt=":skull:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f480.png" height="20" width="20">.</p> <h2 id="categorization">Categorization</h2> <p>From an interesting work that maybe helpful for my work <a class="citation" href="#dziri2021neural">(Dziri et al., 2021)</a>, we can divide <em>hallucination</em> into 2 categories.</p> <ol> <li> <strong>Intrinsic Hallucinations</strong>: the generated output <strong>contradicts</strong> the source content.</li> <li> <strong>Extrinsic Hallucinations</strong>: the generated output <strong>can’t be verified</strong> from source content.</li> </ol> <h3 id="factuality-vs-faithfulness">Factuality vs Faithfulness</h3> <p>According to the work <a class="citation" href="#maynez2020faithfulness">(Maynez et al., 2020)</a>, and <a class="citation" href="#ji2023survey">(Ji et al., 2023)</a>, we should seperate <strong>factuality</strong> and <strong>faithfulness</strong> to provide more clear understanding.</p> <table> <thead> <tr> <th>Factuality</th> <th>Faithfulness</th> </tr> </thead> <tbody> <tr> <td>Consistent, truthful to <strong>source knowledge</strong> </td> <td>Being actual or based on <strong>world knowledge</strong> </td> </tr> </tbody> </table> <h2 id="contributors-to-hallucinations">Contributors to Hallucinations</h2> <h3 id="from-data">From Data</h3> <p>Collecting heuristic data creates mismatches between source and data, that leads to <em>hallucination</em>. And in big datasets, there are cases the examples are duplicated or similar, lead the model to favor generating repeats of memorized phrases.</p> <p>Another problem from data is some doesn’t have factual knowledge, like some datasets for chit-chat style. That leads to extrinsic hallucinations. In this case I am not sure it’s a bug or a feature.</p> <h3 id="from-training-and-inference">From Training and Inference</h3> <p>Even when your dataset has very little divergence, <em>hallucination</em> will find you in another way around, in the way you train and inference your model :evil:.</p> <p>First is maybe your encoder isn’t suitable? The encoder will turn your input text into meaningful representations, if the encoder learns wrong correlations between different parts of the training data, everything will drift away, fast.</p> <p>Then the decoder, what if the decoder attends the wrong part of encoded input source? Or some decoding strategy improves the generation diversity, like <em>TopK</em> sampling strategy is positively correlated with increased hallucination. This is a point we need to find a middle ground to balance between hallucination and answer quality.</p> <p>The <em>exposure bias</em>, or the discrepancy between training and inference is also a problem. It’s like you when studying and in exam. But I think this one can be improved through some techniques <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20">.</p> <p>Pre-trained models can prioritize parametric knowledge over the provided input that leads to hallucination. About this information, I wonder about fine-tuned models <em>thonk</em>.</p> <h2 id="metrics">Metrics</h2> <p>Thank you, this is very valuable section, I love it. Added FRANK (cite frank) and TRUE (cite true) to my collection to assess my model later. These have the effort to quantifying hallucination?</p> <p>One of the simplest approaches is to leverage <strong>lexical features (n-grams)</strong>. This will calculate overlap information and contradictions between generated and reference texts.</p> <p>NLG tasks can have many outputs from the same input (one-to-many mapping), so to simplify the evaluation setup, they only rely on the source text as sole reference.</p> <p><strong>Model-based</strong> metrics measure the hallucination degree in generated text.</p> <p><em>Information Extraction (IE)-based</em> use IE models to represent the knowledge in tuplet format then verify against tuplets extracted from source/reference.</p> <p><em>QA-based</em> works in 3 steps:</p> <ul> <li>Question generation (QG) model generates set of question-answer pairs</li> <li>Question answering (QA) model answers generated questions - become the reference</li> <li>Hallucination score calculates base on <strong>similarity</strong> of corresponding answers.</li> </ul> <p>The problems of these 2 approaches are similar, it depends on the accuracy of the models using to evaluate.</p> <p><em>Natural Language Inference (NLI)</em> metrics determines whether a “hypothesis” is true, false or undetermined. NLI-based approach seems more robust, but also has bad performance in abstractive summarization task.</p> <p><em>Faithfulness Classification Metrics</em> are constructed to improve from the NLI-based metrics. But I don’t understand how superior it is (yet).</p> <p><em>LM-based Metrics</em>. This method uses 2 language models.</p> <ul> <li>Unconditional LM - only trained on the <strong>target data</strong>.</li> <li>Conditional LM - trained on <strong>both source and target data</strong>.</li> </ul> <p><em>Human Evaluation</em> is still one of the most commonly used approaches as current automatic evaluation of hallucinations is still imperfect. Two main forms of human evaluation:</p> <ul> <li> <strong>Scoring</strong>: human rate the hallucination level in a range</li> <li> <strong>Comparing</strong>: Human compare output texts with baseline</li> </ul> <h2 id="hallucination-mitigation-methods">Hallucination Mitigation Methods</h2> <h3 id="data-related">Data-Related</h3> <ul> <li>Building a faithful dataset. You can use “rice-fuel machines” to write clean and faithful targets, another way is use model to generate data and instruct annotators to label.</li> <li>Clean Data Automatically. This approach suitable for case has low or moderate level of noise in original data.</li> <li>Augment Information. Help model has better semantic understanding, enforce a stronger alignment between inputs and outputs.</li> </ul> <h3 id="modeling-and-inference">Modeling and Inference</h3> <p>In my understanding, we can reduce the hallucination by specific (or tailor-made) encoder, attention and decoder strategy. This seems an interesting aspect to discover? <img class="emoji" title=":wink:" alt=":wink:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png" height="20" width="20"> Ahehe.</p> <p>About Training strategy, there are a lot of approaches, we can consider some famous one like Reinforcement Learning (RL), Multi-task Learning, etc etc. An another way to reduce hallucination while requires less training data is post-processing. Some of works follow <strong>generate-then-define strategy</strong>. They use SOTA models to get the results, then correct it my using small amount of automatically generated training data.</p> <hr> <p>I plan to write longer but I think it’s long enough and I know enough to use in this topic, and I am afraid if I don’t publish now, I won’t have any motivations to finish like I planned. So here you are, a piece of my learning.</p> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#1d2021"> <a href="https://dl.acm.org/" rel="external nofollow noopener" target="_blank">ACM</a> </abbr> </div> <div id="ji2023survey" class="col-sm-8"> <div class="title">Survey of hallucination in natural language generation</div> <div class="author"> Ziwei Ji, Nayeon Lee, Rita Frieske, and <span class="more-authors" title="click to view 7 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '7 more authors' ? 'Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, Pascale Fung' : '7 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">7 more authors</span> </div> <div class="periodical"> <em>ACM computing surveys</em>, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#fb4934"> <a href="https://aclanthology.org/" rel="external nofollow noopener" target="_blank">ACL</a> </abbr> </div> <div id="zhu2025can" class="col-sm-8"> <div class="title">Can We Trust AI Doctors? A Survey of Medical Hallucination in Large Language and Large Vision-Language Models</div> <div class="author"> Zhihong Zhu, Yunyan Zhang, Xianwei Zhuang, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Fan Zhang, Zhongwei Wan, Yuyan Chen, QingqingLong QingqingLong, Yefeng Zheng, Xian Wu' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In Findings of the Association for Computational Linguistics: ACL 2025</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="dziri2021neural" class="col-sm-8"> <div class="title">Neural path hunter: Reducing hallucination in dialogue systems via path grounding</div> <div class="author"> Nouha Dziri, Andrea Madotto, Osmar Zaı̈ane, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Avishek Joey Bose' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2104.08455</em>, Sep 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="maynez2020faithfulness" class="col-sm-8"> <div class="title">On faithfulness and factuality in abstractive summarization</div> <div class="author"> Joshua Maynez, Shashi Narayan, Bernd Bohnet, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Ryan McDonald' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2005.00661</em>, Sep 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/aug-2025-reading-log/">Aug 2025 Reading Log</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ithya-in-system/">Ithya Magic Studies joins the gang</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/dat-nuoc-tron-niem-vui/">Đất nước trọn niềm vui, Đăng Dương và chuyện xưa</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/obsidian-aug-2025/">Obsidian Tour - August 2025</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/graphrag-theory-and-practice/">GraphRAG and Linh - theory and practice</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'vtrnnhlinh/vtrnnhlinh.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Linh N.T Vo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: September 11, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>