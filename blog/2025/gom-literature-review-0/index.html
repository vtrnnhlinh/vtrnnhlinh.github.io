<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Graph-of-Models - Literature Review 1 - Transformer and MoH | Linh N.T Vo </title> <meta name="author" content="Linh N.T Vo"> <meta name="description" content="the first literature review of my series writting about my work I called Graph-of-Models"> <meta name="keywords" content="personal-blog, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://vtrnnhlinh.github.io/blog/2025/gom-literature-review-0/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Linh</span> N.T Vo </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Graph-of-Models - Literature Review 1 - Transformer and MoH</h1> <p class="post-meta"> Created on June 27, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/cse"> <i class="fa-solid fa-hashtag fa-sm"></i> cse</a>   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/gom"> <i class="fa-solid fa-hashtag fa-sm"></i> gom</a>   <a href="/blog/tag/english"> <i class="fa-solid fa-hashtag fa-sm"></i> english</a>   ·   <a href="/blog/category/linh-the-engineer"> <i class="fa-solid fa-tag fa-sm"></i> Linh-the-Engineer</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>At work I am assigned to learn about Mixture-of-Experts (MoE) but my mentor wants another specific, tailor-made approach to our problem.</p> <p>I name it <strong>Graph-of-Models</strong> (GoM).</p> <p>I have a vague idea in my mind but I think I need a <strong>Literature Review</strong> to make my idea becomes realistic as most as possible.</p> <p>However, I don’t follow the ordinary literature review in academic research, as I am working in industry. I will try to adapt and build the code along the way.</p> <p>This is a series of posts about this project, and this first post is about 2 first literature review of mine.</p> <h2 id="plan">Plan</h2> <p>In my first plan, I want to use MoH structure <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> as the base development. Then I will apply the experts network based on <a class="citation" href="#du2024large">(Du et al., 2024)</a>.</p> <p>The Python project structure I apply is the <a href="https://github.com/navdeep-G/samplemod" rel="external nofollow noopener" target="_blank">navdeep-G/samplemod</a>.</p> <p>The training framework will be Megatron-LM <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> with continual, meta and multi-task learning. Federated Learning will be developed when the application is deployed for many users.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/plan_llm-480.webp 480w,/assets/img/plan_llm-800.webp 800w,/assets/img/plan_llm-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/plan_llm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>My imagined structure.</p> <h2 id="attention-is-all-you-need">Attention is all you need!!</h2> <p>We will start from <code class="language-plaintext highlighter-rouge">Transformer</code> Structure <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>I am pretty bad at Python, so I will learn and reference a lot of repositories, both in how they structure the file and the coding methodology.</p> <p>I reference <a href="https://github.com/SCCSMARTCODE/attention-is-all-you-need-from-scratch" rel="external nofollow noopener" target="_blank">SCCSMARTCODE/attention-is-all-you-need-from-scratch</a> and <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch" rel="external nofollow noopener" target="_blank">jadore801120/attention-is-all-you-need-pytorch</a> to re-made the <code class="language-plaintext highlighter-rouge">transformer</code> structure.</p> <p><code class="language-plaintext highlighter-rouge">Transformer</code> is an architecture rely entirely on an attention mechanism to draw <strong>global dependencies</strong> between I/O. <code class="language-plaintext highlighter-rouge">Transformer</code> allows significantly more parallelization.</p> <p>I believe my wanted structure is far from this work, but a thousand miles start from a step.</p> <h3 id="encoder--decoder">Encoder &amp; Decoder</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention_architecture-480.webp 480w,/assets/img/attention_architecture-800.webp 800w,/assets/img/attention_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/attention_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><code class="language-plaintext highlighter-rouge">Transformer</code> architecture. Source: <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>With the illustration, you can see the <code class="language-plaintext highlighter-rouge">Transformer</code> has 2 main modules are <strong>Encoder</strong> and <strong>Decoder</strong>. There is also something worth noticing is <strong>Positional Encoding</strong>.</p> <p><strong>Encoder</strong> and <strong>Decoder</strong> has a stack of $N=6$ layer. Encoder’s layer has 2 sub-layers:</p> <ul> <li>Multi-Head Attention</li> <li>Feed Forward</li> </ul> <p>While Decoder’s layer has 3 sub-layers:</p> <ul> <li>Multi-Head Attention</li> <li>Feed Forward</li> <li>Masked Multi-Head Attention</li> </ul> <p>We employ residual connection around each of sub-layers, followed by layer normalization. The dimension is $d_{model} = 512$.</p> <h3 id="attention">Attention</h3> <p>To me, this is the heart of this work.</p> <p>Attention function is mapping a query and a set of key-value pairs to vectors output.</p> <ul> <li>Output is weighted sum of values</li> <li>The weight has another compatibility function to calculate</li> </ul> <h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4> <ul> <li>Input: <ul> <li> <strong>Q</strong>: queries</li> <li> <strong>K</strong>: keys of dimension $d_k$</li> <li> <strong>V</strong>: values of dimension $d_v$</li> </ul> </li> </ul> \[Attention (Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\] <ul> <li>$\frac{1}{\sqrt{d_k}}$: scaling factor. Why? To avoid <code class="language-plaintext highlighter-rouge">softmax</code> is pushed into regions result extremely small gradients</li> <li> <strong>Dot-product attention</strong> faster and more space-efficient in practice than <strong>additive attention</strong> </li> </ul> <h4 id="multi-head-attention">Multi-Head Attention</h4> <p>They perform the attention function in parallel, yielding $d_v$-dim output values. They are concatenated and projected, make final values.</p> <p>\(MultiHead(Q,K,V) = Concat(head_1,.... head_n)W^O\) \(head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\)</p> <h4 id="attention-in-transformer">Attention in Transformer</h4> <p>We will remind the input of attention. We will have Q (Queries), K (keys) and V (values). <code class="language-plaintext highlighter-rouge">Transformer</code> uses attention in 3 ways:</p> <ul> <li>“encoder-decoder attention” layer: Q from previous decoder layer, K, V from output of encoder.</li> <li>“encoder self-attention” layer: Q, K, V from previous encoder layer.</li> <li>“decoder self-attention” layer: Similar to encoder one, however they masked out all values in the input of <code class="language-plaintext highlighter-rouge">softmax</code> (set to $-\infty$) in scaled dot-product attention to illegal connections.</li> </ul> <h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3> <p>FFN has 2 linear transformations with a ReLU activation<d-footnote>Rectified Linear Unit (ReLU) is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero</d-footnote>:</p> \[FFN(x) = max(0, xW_1+b_1)W_2 +b_2\] <h3 id="positional-encoding">Positional Encoding</h3> <p>This is the method to inject information about the relative or absolute position of the tokens in the sequence. <strong>Positional Encoding</strong> has the same dimension $d_{model}$ as the embeddedings. In this work, they use sine and cosine functions</p> \[PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\] \[PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\] <h2 id="moh-mixture-of-heads">MoH (Mixture-of-Heads)</h2> <p>In my understanding, MoH <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> is a mix of Mixture-of-Experts (MoE) and <code class="language-plaintext highlighter-rouge">transformer</code> <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>They made 2 important changes, one, there is a TopK router to activate heads for each token. They also replace the standard summation in multi-head attention to weighted sum.</p> <p>They believe that with changes, they made 2 significant advantages:</p> <ul> <li>First, allows each token select most relevant attention heads, improve efficiency without sacrificing accuracy or increasing the params.</li> <li>Second, with weighted sum, MoH enhances the flexibility of attention mechanism.</li> </ul> <h3 id="design">Design</h3> <p>The core of the work is <strong>MoH</strong>, which treats attention heads as experts.</p> \[MoH(X, X') = \sum^h_{i=1} g_i H^i W^i_O\] <ul> <li>$X, X’$: input tokens</li> <li>$g_i$: routing score</li> <li>$H^i$: Head ith</li> <li>$W^i_O$: output of projection matrix</li> </ul> <p>Inspired by DeepSeek <a class="citation" href="#dai2024deepseekmoe">(Dai et al., 2024)</a>, MoH designs a subset of heads as <strong>shared heads</strong> that remain always activated. This will consolidate common knowledge within shared heads.</p> <p><strong>Two-Stage Routing</strong> for dynamically balance the weights between shared and routed heads. Routing scores are determined by both the <strong>score of each individual head</strong> and <strong>score associated with the head type</strong>. To avoid the unbalanced load, MoH applies <strong>Load Balance Loss</strong>.</p> <h3 id="training">Training</h3> <p>Training LLMs from scratch, they use Megatron-LM <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> with public datasets.</p> <p>With Continual Learning, they tuned <code class="language-plaintext highlighter-rouge">LLaMA3-8B</code>. 3 challenges when doing this:</p> <ol> <li>Determine the shared attention heads</li> <li>Add head routers</li> <li>Weight attention heads</li> </ol> <hr> <p>That’s all for the day. The next post I will discuss about GaCLLM and how I imagine the system will work.</p> </div> </article> <h2>References</h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="jin2024moh" class="col-sm-8"> <div class="title">Moh: Multi-head attention as mixture-of-head attention</div> <div class="author"> Peng Jin, Bo Zhu, Li Yuan, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Shuicheng Yan' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2410.11842</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="du2024large" class="col-sm-8"> <div class="title">Large language model with graph convolution for recommendation</div> <div class="author"> Yingpeng Du, Ziyan Wang, Zhu Sun, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Haoyan Chua, Hongzhi Liu, Zhonghai Wu, Yining Ma, Jie Zhang, Youchen Sun' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2402.08859</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="shoeybi2019megatron" class="col-sm-8"> <div class="title">Megatron-lm: Training multi-billion parameter language models using model parallelism</div> <div class="author"> Mohammad Shoeybi, Mostofa Patwary, Raul Puri, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Patrick LeGresley, Jared Casper, Bryan Catanzaro' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:1909.08053</em>, Sep 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="vaswani2017attention" class="col-sm-8"> <div class="title">Attention is all you need</div> <div class="author"> Ashish Vaswani, Noam Shazeer, Niki Parmar, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>Advances in neural information processing systems</em>, Sep 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b31b1b"> <a href="https://arxiv.org/" rel="external nofollow noopener" target="_blank">arXiv</a> </abbr> </div> <div id="dai2024deepseekmoe" class="col-sm-8"> <div class="title">Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models</div> <div class="author"> Damai Dai, Chengqi Deng, Chenggang Zhao, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>arXiv preprint arXiv:2401.06066</em>, Sep 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/obsidian-aug-2025/">Obsidian Tour - August 2025</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/graphrag-theory-and-practice/">GraphRAG and Linh - theory and practice</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/gom-literature-review-3/">Graph-of-Models - Literature Review 4 - embracing the KGs</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/july-25-reading-log/">July 2025 Reading Log</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/codeLynn-0/">codeLynn - fantasies and MLOps</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'vtrnnhlinh/vtrnnhlinh.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Linh N.T Vo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: August 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>