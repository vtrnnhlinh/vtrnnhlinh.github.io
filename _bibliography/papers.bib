---
---

@article{mu2025comprehensive,
  title={A comprehensive survey of mixture-of-experts: Algorithms, theory, and applications},
  author={Mu, Siyuan and Lin, Sen},
  journal={arXiv preprint arXiv:2503.07137},
  year={2025},
  url={https://doi.org/10.48550/arXiv.2503.07137}
}

@article{cai2024survey,
  title={A survey on mixture of experts},
  author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2407.06204},
  year={2024},
  url={https://doi.org/10.48550/arXiv.2407.06204}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url={https://doi.org/10.48550/arXiv.1706.03762}
}

@inproceedings{wang2024moa,
  title={Moa: Mixture-of-attention for subject-context disentanglement in personalized image generation},
  author={Wang, Kuan-Chieh and Ostashev, Daniil and Fang, Yuwei and Tulyakov, Sergey and Aberman, Kfir},
  booktitle={SIGGRAPH Asia 2024 Conference Papers},
  pages={1--12},
  year={2024},
  url={https://doi.org/10.48550/arXiv.2404.11565}
}

@article{jin2024moh,
  title={Moh: Multi-head attention as mixture-of-head attention},
  author={Jin, Peng and Zhu, Bo and Yuan, Li and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2410.11842},
  year={2024},
  url={https://doi.org/10.48550/arXiv.2410.11842}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@misc {sanseviero2023moe,
    author       = { Omar Sanseviero and
                     Lewis Tunstall and
                     Philipp Schmid and
                     Sourab Mangrulkar and
                     Younes Belkada and
                     Pedro Cuenca
                   },
    title        = { Mixture of Experts Explained },
    year         = 2023,
    url          = { https://huggingface.co/blog/moe },
    publisher    = { Hugging Face Blog }
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019},
  url={https://doi.org/10.48550/arXiv.1909.08053}
}

@inproceedings{zhao2025swift,
  title={Swift: a scalable lightweight infrastructure for fine-tuning},
  author={Zhao, Yuze and Huang, Jintao and Hu, Jinghan and Wang, Xingjun and Mao, Yunlin and Zhang, Daoze and Jiang, Zeyinzi and Wu, Zhikai and Ai, Baole and Wang, Ang and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={28},
  pages={29733--29735},
  year={2025},
  url={https://doi.org/10.48550/arXiv.2408.05517}
}
@article{du2024large,
  title={Large language model with graph convolution for recommendation},
  author={Du, Yingpeng and Wang, Ziyan and Sun, Zhu and Chua, Haoyan and Liu, Hongzhi and Wu, Zhonghai and Ma, Yining and Zhang, Jie and Sun, Youchen},
  journal={arXiv preprint arXiv:2402.08859},
  year={2024}
}
