---
---

@article{mu2025comprehensive,
  title={A comprehensive survey of mixture-of-experts: Algorithms, theory, and applications},
  author={Mu, Siyuan and Lin, Sen},
  journal={arXiv preprint arXiv:2503.07137},
  year={2025},
  url={https://doi.org/10.48550/arXiv.2503.07137}
}

@article{cai2024survey,
  title={A survey on mixture of experts},
  author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2407.06204},
  year={2024},
  url={https://doi.org/10.48550/arXiv.2407.06204}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url={https://doi.org/10.48550/arXiv.1706.03762}
}

@inproceedings{wang2024moa,
  title={Moa: Mixture-of-attention for subject-context disentanglement in personalized image generation},
  author={Wang, Kuan-Chieh and Ostashev, Daniil and Fang, Yuwei and Tulyakov, Sergey and Aberman, Kfir},
  booktitle={SIGGRAPH Asia 2024 Conference Papers},
  pages={1--12},
  year={2024},
  url={https://doi.org/10.48550/arXiv.2404.11565}
}

@article{jin2024moh,
  title={Moh: Multi-head attention as mixture-of-head attention},
  author={Jin, Peng and Zhu, Bo and Yuan, Li and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2410.11842},
  year={2024},
  url={https://doi.org/10.48550/arXiv.2410.11842}
}

@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@misc {sanseviero2023moe,
    author       = { Omar Sanseviero and
                     Lewis Tunstall and
                     Philipp Schmid and
                     Sourab Mangrulkar and
                     Younes Belkada and
                     Pedro Cuenca
                   },
    title        = { Mixture of Experts Explained },
    year         = 2023,
    url          = { https://huggingface.co/blog/moe },
    publisher    = { Hugging Face Blog }
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019},
  url={https://doi.org/10.48550/arXiv.1909.08053}
}

@inproceedings{zhao2025swift,
  title={Swift: a scalable lightweight infrastructure for fine-tuning},
  author={Zhao, Yuze and Huang, Jintao and Hu, Jinghan and Wang, Xingjun and Mao, Yunlin and Zhang, Daoze and Jiang, Zeyinzi and Wu, Zhikai and Ai, Baole and Wang, Ang and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={28},
  pages={29733--29735},
  year={2025},
  url={https://doi.org/10.48550/arXiv.2408.05517}
}
@article{du2024large,
  title={Large language model with graph convolution for recommendation},
  author={Du, Yingpeng and Wang, Ziyan and Sun, Zhu and Chua, Haoyan and Liu, Hongzhi and Wu, Zhonghai and Ma, Yining and Zhang, Jie and Sun, Youchen},
  journal={arXiv preprint arXiv:2402.08859},
  year={2024}
}
@article{dai2024deepseekmoe,
  title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Yu and others},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}
@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}
@article{pan2024unifying,
  title={Unifying large language models and knowledge graphs: A roadmap},
  author={Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={36},
  number={7},
  pages={3580--3599},
  year={2024},
  publisher={IEEE}
}
@article{hogan2021knowledge,
  title={Knowledge graphs},
  author={Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and dâ€™Amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jos{\'e} Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and others},
  journal={ACM Computing Surveys (Csur)},
  volume={54},
  number={4},
  pages={1--37},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{cheng2024call,
  title={Call me when necessary: Llms can efficiently and faithfully reason over structured environments},
  author={Cheng, Sitao and Zhuang, Ziyuan and Xu, Yong and Yang, Fangkai and Zhang, Chaoyun and Qin, Xiaoting and Huang, Xiang and Chen, Ling and Lin, Qingwei and Zhang, Dongmei and others},
  journal={arXiv preprint arXiv:2403.08593},
  year={2024}
}
@article{zhang2024extract,
  title={Extract, define, canonicalize: An llm-based framework for knowledge graph construction},
  author={Zhang, Bowen and Soh, Harold},
  journal={arXiv preprint arXiv:2404.03868},
  year={2024}
}
@article{jiang2024kg,
  title={Kg-fit: Knowledge graph fine-tuning upon open-world knowledge},
  author={Jiang, Pengcheng and Cao, Lang and Xiao, Cao Danica and Bhatia, Parminder and Sun, Jimeng and Han, Jiawei},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={136220--136258},
  year={2024}
}
@article{bento2019dijkstra,
  title={Dijkstra graphs},
  author={Bento, Lucila MS and Boccardo, Davidson R and Machado, Raphael CS and Miyazawa, Fl{\'a}vio K and de S{\'a}, Vin{\'\i}cius G Pereira and Szwarcfiter, Jayme L},
  journal={Discrete Applied Mathematics},
  volume={261},
  pages={52--62},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{niwattanakul2013using,
  title={Using of Jaccard coefficient for keywords similarity},
  author={Niwattanakul, Suphakit and Singthongchai, Jatsada and Naenudorn, Ekkachai and Wanapu, Supachanun},
  booktitle={Proceedings of the international multiconference of engineers and computer scientists},
  volume={1},
  number={6},
  pages={380--384},
  year={2013}
}
@inproceedings{gunawan2018implementation,
  title={The implementation of cosine similarity to calculate text relevance between two documents},
  author={Gunawan, Dani and Sembiring, CA and Budiman, Mohammad Andri},
  booktitle={Journal of physics: conference series},
  volume={978},
  pages={012120},
  year={2018},
  organization={IOP Publishing}
}
@inproceedings{dehak2010cosine,
  title={Cosine similarity scoring without score normalization techniques.},
  author={Dehak, Najim and Dehak, Reda and Glass, James R and Reynolds, Douglas A and Kenny, Patrick and others},
  booktitle={Odyssey},
  volume={15},
  year={2010}
}
@misc{song2025gradientsys,
      title={Gradientsys: A Multi-Agent LLM Scheduler with ReAct Orchestration}, 
      author={Xinyuan Song and Zeyu Wang and Siyi Wu and Tianyu Shi and Lynn Ai},
      year={2025},
      eprint={2507.06520},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2507.06520}, 
}
@article{ivanov2020impact,
  title={The impact of columnar file formats on SQL-on-hadoop engine performance: A study on ORC and Parquet},
  author={Ivanov, Todor and Pergolesi, Matteo},
  journal={Concurrency and Computation: Practice and Experience},
  volume={32},
  number={5},
  pages={e5523},
  year={2020},
  publisher={Wiley Online Library}
}
@article{wang2025model,
  title={Model generalization on text attribute graphs: Principles with large language models},
  author={Wang, Haoyu and Liu, Shikun and Wei, Rongzhe and Li, Pan},
  journal={arXiv preprint arXiv:2502.11836},
  year={2025}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
