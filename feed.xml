<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://vtrnnhlinh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vtrnnhlinh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-30T03:22:38+00:00</updated><id>https://vtrnnhlinh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple logbook of Linh </subtitle><entry><title type="html">Graphs-of-Heads - The First Literature Review</title><link href="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0/" rel="alternate" type="text/html" title="Graphs-of-Heads - The First Literature Review"/><published>2025-06-27T11:30:00+00:00</published><updated>2025-06-27T11:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0/"><![CDATA[<p>At work I am assigned to learn about Mixture-of-Experts (MoE) but my mentor wants another specific, tailor-made approach to our problem.</p> <p>I name it <strong>Graphs-of-Heads</strong> (GoH).</p> <p>I have a vague idea in my mind but I think I need a <strong>Literature Review</strong> to make my idea becomes realistic as most as possible.</p> <p>However, I donâ€™t follow the ordinary literature review in academic research, as I am working in industry. I will try to adapt and build the code along the way.</p> <p>This is a series of posts about this project, and this first post is about 2 first literature review of mine.</p> <h2 id="plan">Plan</h2> <p>In my first plan, I want to use MoH structure <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> as the base development. Then I will apply the experts network based on <a class="citation" href="#du2024large">(Du et al., 2024)</a>.</p> <p>The Python project structure I apply is the <a href="https://github.com/navdeep-G/samplemod">navdeep-G/samplemod</a>.</p> <p>The training framework will be Megatron-LM <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> with continual, meta and multi-task learning. Federated Learning will be developed when the application is deployed for many users.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/plan_llm-480.webp 480w,/assets/img/plan_llm-800.webp 800w,/assets/img/plan_llm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/plan_llm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My imagined structure.</p> <h2 id="attention-is-all-you-need">Attention is all you need!!</h2> <p>We will start from <code class="language-plaintext highlighter-rouge">Transformer</code> Structure <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>I am pretty bad at Python, so I will learn and reference a lot of repositories, both in how they structure the file and the coding methodology.</p> <p>I reference <a href="https://github.com/SCCSMARTCODE/attention-is-all-you-need-from-scratch">SCCSMARTCODE/attention-is-all-you-need-from-scratch</a> and <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">jadore801120/attention-is-all-you-need-pytorch</a> to re-made the <code class="language-plaintext highlighter-rouge">transformer</code> structure.</p> <p><code class="language-plaintext highlighter-rouge">Transformer</code> is an architecture rely entirely on an attention mechanism to draw <strong>global dependencies</strong> between I/O. <code class="language-plaintext highlighter-rouge">Transformer</code> allows significantly more parallelization.</p> <p>I believe my wanted structure is far from this work, but a thousand miles start from a step.</p> <h3 id="encoder--decoder">Encoder &amp; Decoder</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention_architecture-480.webp 480w,/assets/img/attention_architecture-800.webp 800w,/assets/img/attention_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/attention_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><code class="language-plaintext highlighter-rouge">Transformer</code> architecture. Source: <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>With the illustration, you can see the <code class="language-plaintext highlighter-rouge">Transformer</code> has 2 main modules are <strong>Encoder</strong> and <strong>Decoder</strong>. There is also something worth noticing is <strong>Positional Encoding</strong>.</p> <p><strong>Encoder</strong> and <strong>Decoder</strong> has a stack of $N=6$ layer. Encoderâ€™s layer has 2 sub-layers:</p> <ul> <li>Multi-Head Attention</li> <li>Feed Forward</li> </ul> <p>While Decoderâ€™s layer has 3 sub-layers:</p> <ul> <li>Multi-Head Attention</li> <li>Feed Forward</li> <li>Masked Multi-Head Attention</li> </ul> <p>We employ residual connection around each of sub-layers, followed by layer normalization. The dimension is $d_{model} = 512$.</p> <h3 id="attention">Attention</h3> <p>To me, this is the heart of this work.</p> <p>Attention function is mapping a query and a set of key-value pairs to vectors output.</p> <ul> <li>Output is weighted sum of values</li> <li>The weight has another compatibility function to calculate</li> </ul> <h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4> <ul> <li>Input: <ul> <li><strong>Q</strong>: queries</li> <li><strong>K</strong>: keys of dimension $d_k$</li> <li><strong>V</strong>: values of dimension $d_v$</li> </ul> </li> </ul> \[Attention (Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\] <ul> <li>$\frac{1}{\sqrt{d_k}}$: scaling factor. Why? To avoid <code class="language-plaintext highlighter-rouge">softmax</code> is pushed into regions result extremely small gradients</li> <li><strong>Dot-product attention</strong> faster and more space-efficient in practice than <strong>additive attention</strong></li> </ul> <h4 id="multi-head-attention">Multi-Head Attention</h4> <p>They perform the attention function in parallel, yielding $d_v$-dim output values. They are concatenated and projected, make final values.</p> <p>\(MultiHead(Q,K,V) = Concat(head_1,.... head_n)W^O\) \(head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\)</p> <h4 id="attention-in-transformer">Attention in Transformer</h4> <p>We will remind the input of attention. We will have Q (Queries), K (keys) and V (values). <code class="language-plaintext highlighter-rouge">Transformer</code> uses attention in 3 ways:</p> <ul> <li>â€œencoder-decoder attentionâ€ layer: Q from previous decoder layer, K, V from output of encoder.</li> <li>â€œencoder self-attentionâ€ layer: Q, K, V from previous encoder layer.</li> <li>â€œdecoder self-attentionâ€ layer: Similar to encoder one, however they masked out all values in the input of <code class="language-plaintext highlighter-rouge">softmax</code> (set to $-\infty$) in scaled dot-product attention.</li> </ul> <h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3> <p>FFN has 2 linear transformations with a ReLU activation<d-footnote>Rectified Linear Unit (ReLU) is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero</d-footnote>:</p> \[FFN(x) = max(0, xW_1+b_1)W_2 +b_2\] <h3 id="positional-encoding">Positional Encoding</h3> <p>This is the method to inject information about the relative or absolute position of the tokens in the sequence. <strong>Positional Encoding</strong> has the same dimension $d_{model}$ as the embeddedings. In this work, they use sine and cosine functions</p> <p>\(PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\) \(PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\)</p> <h2 id="moh-mixture-of-heads">MoH (Mixture-of-Heads)</h2> <p>In my understanding, MoH <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> is a mix of Mixture-of-Experts (MoE) and <code class="language-plaintext highlighter-rouge">transformer</code> <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>They made 2 important changes, one, there is a TopK router to activate heads for each token. They also replace the standard summation in multi-head attention to weighted sum.</p> <p>They believe that with changes, they made 2 significant advantages:</p> <ul> <li>First, allows each token select most relevant attention heads, improve efficiency without sacrificing accuracy or increasing the params.</li> <li>Second, with weighted sum, MoH enhances the flexibility of attention mechanism.</li> </ul> <h3 id="design">Design</h3> <p>The core of the work is <strong>MoH</strong>, which treats attention heads as experts.</p> \[MoH(X, X') = \sum^h_{i=1} g_i H^i W^i_O\] <ul> <li>$X, Xâ€™$: input tokens</li> <li>$g_i$: routing score</li> <li>$H^i$: Head ith</li> <li>$W^i_O$: output of projection matrix</li> </ul> <p>Inspired by DeepSeek <a class="citation" href="#dai2024deepseekmoe">(Dai et al., 2024)</a>, MoH designs a subset of heads as <strong>shared heads</strong> that remain always activated. This will consolidate common knowledge within shared heads.</p> <p><strong>Two-Stage Routing</strong> for dynamically balance the weights between shared and routed heads. Routing scores are determined by both the <strong>score of each individual head</strong> and <strong>score associated with the head type</strong>. To avoid the unbalanced load, MoH applies <strong>Load Balance Loss</strong>.</p> <h3 id="training">Training</h3> <p>Training LLMs from scratch, they use Megatron-LM <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> with public datasets.</p> <p>With Continual Learning, they tuned <code class="language-plaintext highlighter-rouge">LLaMA3-8B</code>. 3 challenges when doing this:</p> <ol> <li>Determine the shared attention heads</li> <li>Add head routers</li> <li>Weight attention heads</li> </ol> <hr/> <p>Thatâ€™s all for the day. The next post I will discuss about GaCLLM and how I imagine the system will work.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="goh"/><category term="english"/><summary type="html"><![CDATA[the first literature review of my series writting about my work I called Graph-of-Heads]]></summary></entry><entry><title type="html">My Life Setup - Summer 2025</title><link href="https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25/" rel="alternate" type="text/html" title="My Life Setup - Summer 2025"/><published>2025-06-26T17:30:00+00:00</published><updated>2025-06-26T17:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25/"><![CDATA[<p>When I write this, my life is a mess. I am drown in endless fantasies. I write to encourage and clarify for myself. I hope that I can follow what I expect here.</p> <h2 id="routines-tasks-and-mental-health">Routines, Tasks and Mental Health</h2> <h3 id="routineflow">RoutineFlow</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://routineflow.app/">RoutineFlow App</a></li> </ul> <p>I highly recommend this app, the author is GOAT. I have used this app since beta, and purchased its lifetime premium after that shortly.</p> <p>Its clean UI and concept attract me greatly. If you have read <a href="https://jamesclear.com/atomic-habits">Atomic Habit</a> and want to have an app implemented to work with the principles of this book, this app is for you.</p> <p>The app is improved through the time, now it is really polished and smooth with template routines and widget.</p> <p>I use this app for morning and evening routine, mainly for personal hygiene and calm myself down.</p> <h3 id="ticktick">TickTick</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: <a href="https://ticktick.com/r?c=pfdhlnyn">TickTick</a></li> </ul> <p>The ultimate core of my system. I have use TickTick for more than 2 years, even though I have slacked off recently. But I am going to renew the yearly subcription soon! I use TickTick for everything and trust me bro, itâ€™s the best app out there.</p> <p>The features are rolled out frequently, the price is acceptable, it is all-in-one you need! Task lists, Timer, Calendar, Habits and now is Countdown!</p> <p>Some people complaint about the UI but I feel itâ€™s okay?</p> <ul> <li>Some tips and guides from TickTick Blog: <a href="https://www.ticktick.com/resources">TickTick Resources</a></li> </ul> <h3 id="tochi">Tochi</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://play.google.com/store/apps/details?id=com.lazyhippo.tochidiary&amp;hl=en-US&amp;pli=1">Tochi</a></li> </ul> <p>It is my Mood tracker, I also bought its lifetime premium. I love one-time payment app ğŸ˜† Itâ€™s cute, has tags, can attach images and the orbs are cute.</p> <p>I can track my mood a lot of time through the day. Some apps only allow an input a day.</p> <h3 id="1money">1Money</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://play.google.com/store/apps/details?id=org.pixelrush.moneyiq">1Money</a></li> </ul> <p>As I researched it seems has some drama around the app? But tbh I donâ€™t care, the app is solid enough to track your money expense. The UI is clean and the functions is enough.</p> <h2 id="self-study">Self-Study</h2> <h3 id="chatgpt">ChatGPT</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: Everyone knows it ğŸ˜„</li> </ul> <p>I guess this is kinda like a love-hate relationship? Sometimes I feel I depend on it, sometimes it helped me a lot. Recently, I find out I can be open with it more! I have spent a lot of my time talking with AI, from AI chatbot service for your fantasies to AI like BingAI or ChatGPT. I think that using it as an assistant, you provide options, I am the one who decide is good.</p> <h3 id="obsidian">Obsidian</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: <a href="https://obsidian.md/download">Obsidian</a></li> </ul> <p>Itâ€™s a famous and powerful free tool. Itâ€™s very beneficial when you know Markdown and community plugins. I can sync my Zotero notes with Obsidian. But ye, after finishing the Thesis, I deleted Zotero lol.</p> <p>I think beside storing your notes with easy markdown, I love the Spaced Repition of Obsidian. Unlike Anki, which I usually donâ€™t like because the flashcards can be out of context, I can make flashcards from my note with Obsidian. Works very well tho.</p> <h3 id="languages">Languages</h3> <p>For English, my biggest problem is Speaking Skill. I use <a href="https://play.google.com/store/apps/details?id=us.nobarriers.elsa&amp;hl=vi&amp;gl=US&amp;pli=1"><strong>Elsa Speak</strong></a> for this. I have a Lifetime Elsa Pro account. The Premium version has AI features but I donâ€™t think itâ€™s necessary. I only need to enhance my pronunciation.</p> <p>And with German, beside having a small textbook, I also use <a href="https://www.memrise.com/">Memrise</a>, again, a lifetime premium account. Memrise has a lot of updates recently, I think itâ€™s a solid app. I also used Busuu before, but after the yearly subcription, I didnâ€™t extend it. Just because I am lazy, not because of the app. I think itâ€™s also a good app for you to try. Duolingo? I stop using Duolingo as the free version is so limited and frustrated.</p> <h3 id="mooc">MOOC</h3> <p>Coursera is my go-to solution as I bought a Plus subcription :laughing:. I think itâ€™s more academic than Udemy. I have Harmonica and Calculus courses on Udemy but havenâ€™t finished (yet).</p> <p>I have some Python-related and Embedded Systems-related courses on Coursera right now. Hopefully I can finish some in this summer.</p> <h2 id="work">Work</h2> <p>Here I am, in the ecosystem of Microsoft.</p> <h3 id="ms-todo">MS Todo</h3> <p>MS Todo is a poor todo app to me comparing to my dear Ticktick. But itâ€™s also not so bad with My Day feature. I guess itâ€™s the only thing TickTick should adapt, Do Day is different than Due Day.</p> <p>Also, when you flag an Outlook email, it will appear in Todo, which helps you keep track of emails you need to take action.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="languages"/><category term="plans"/><category term="english"/><summary type="html"><![CDATA[my routines and tools used for summer 2025]]></summary></entry><entry><title type="html">Tutorial - Megatron-SWIFT and Qwen2.5 Installation</title><link href="https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation/" rel="alternate" type="text/html" title="Tutorial - Megatron-SWIFT and Qwen2.5 Installation"/><published>2025-06-24T09:45:00+00:00</published><updated>2025-06-24T09:45:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation/"><![CDATA[<p>This tutorial based a lot on my experience with my companyâ€™s servers. So maybe there is some things not applicable in your case. Leave comment if you need anything to discuss.</p> <h2 id="purpose">Purpose</h2> <p>I am trying to fine-tunning a model with Mixture-of-Experts (MoE) <a class="citation" href="#sanseviero2023moe">(Sanseviero et al., 2023)</a> methodology. I choose <strong>Megatron-LM</strong> <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> and <strong>SWIFT</strong> <a class="citation" href="#zhao2025swift">(Zhao et al., 2025)</a> as the framework.</p> <p>The tutorial I am following is: <a href="https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html">Megatron-SWIFT Training</a>.</p> <h2 id="prerequisites">Prerequisites</h2> <ul> <li>Operating System: <strong>Linux</strong></li> <li><strong>Python</strong> should be pre-installed. Check if your OS already has Python. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">--version</span>
</code></pre></div> </div> </li> <li>If your OS doesnâ€™t have Python yet, run below commands to install (this apply for Ubuntu, if you use different distro, google the tutorial for your OS). <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>python3 python3-pip
</code></pre></div> </div> </li> <li>Using virtual environment is a good practice for Python, I use <code class="language-plaintext highlighter-rouge">anaconda</code> for this. Follow <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html">this guide</a> to install <code class="language-plaintext highlighter-rouge">anaconda</code> on Linux.</li> <li>If you want to train with GPU, you need to install <code class="language-plaintext highlighter-rouge">cuda</code>: <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">CUDA Installation Guide for Linux</a>. <strong>Recommended version</strong>: <code class="language-plaintext highlighter-rouge">12.1.0</code>.</li> <li>With this framework, you also need to install <code class="language-plaintext highlighter-rouge">cuDNN</code>: <a href="https://docs.nvidia.com/deeplearning/cudnn/installation/latest/linux.html">Installing cuDNN Backend on Linux</a>. <strong>Recommended version</strong>: <code class="language-plaintext highlighter-rouge">9</code>.</li> </ul> <h2 id="install-megatron-swift">Install Megatron-SWIFT</h2> <p>First, we will create a virtual environment with <code class="language-plaintext highlighter-rouge">conda</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> &lt;ENV_NAME&gt; <span class="nv">python</span><span class="o">=</span>3.10
conda activate &lt;ENV_NAME&gt;
</code></pre></div></div> <p>Then we will install <code class="language-plaintext highlighter-rouge">pytorch</code> and <code class="language-plaintext highlighter-rouge">torchvision</code> first.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span><span class="nv">pytorch</span><span class="o">==</span>2.3.0 <span class="nv">torchvision</span><span class="o">==</span>0.18.0
</code></pre></div></div> <p>Next we need to install <code class="language-plaintext highlighter-rouge">apex</code>, <code class="language-plaintext highlighter-rouge">transformer-engine</code>, and <code class="language-plaintext highlighter-rouge">ms-swift</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/NVIDIA/apex
<span class="nb">cd </span>apex
pip <span class="nb">install</span> <span class="nt">-v</span> <span class="nt">--disable-pip-version-check</span> <span class="nt">--no-cache-dir</span> <span class="nt">--no-build-isolation</span> <span class="nt">--config-settings</span> <span class="s2">"--build-option=--cpp_ext"</span> <span class="nt">--config-settings</span> <span class="s2">"--build-option=--cuda_ext"</span> ./
pip <span class="nb">install </span>transformer-engine
pip <span class="nb">install </span>ms-swift
</code></pre></div></div> <h2 id="download-qwen25">Download Qwen2.5</h2> <h3 id="install-git-lfs">Install <code class="language-plaintext highlighter-rouge">git lfs</code></h3> <p>Check availability of <code class="language-plaintext highlighter-rouge">git</code> and <code class="language-plaintext highlighter-rouge">git lfs</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git <span class="nt">--version</span>
git lfs version
</code></pre></div></div> <p>If your enviroment still not have <code class="language-plaintext highlighter-rouge">git-lfs</code> you need to install it</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>conda-forge::git-lfs
</code></pre></div></div> <h3 id="clone-model-repo">Clone model repo</h3> <p><strong>Qwen2.5</strong> <a class="citation" href="#qwen2.5">(Team, 2024)</a> is the model I use to train. First we will visit <a href="https://huggingface.co/">HuggingFace</a> to create an account. Then visit <strong>Profile &gt; Access Tokens &gt; Create new token</strong>. Choose <strong>Token Type</strong> is <strong>Write</strong>. Remember to <strong>copy the token</strong>.</p> <p>Return to our activated conda environment. Install <code class="language-plaintext highlighter-rouge">huggingface_hub</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>huggingface_hub
</code></pre></div></div> <p>Then login into your <code class="language-plaintext highlighter-rouge">huggingface</code> token.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>huggingface-cli login <span class="nt">--token</span> &lt;your-token&gt;
</code></pre></div></div> <p>Finally, we can clone the model repo to our folder. Example: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> &lt;model folder&gt;
git lfs <span class="nb">install
</span>git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
</code></pre></div></div> <h2 id="test">Test</h2> <p>Create a <code class="language-plaintext highlighter-rouge">test.sh</code> file to run test.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>0 <span class="se">\</span>
swift <span class="nb">export</span> <span class="se">\</span>
    <span class="nt">--model</span> &lt;model_dir&gt;/Qwen2.5-7B-Instruct <span class="se">\</span>
    <span class="nt">--to_mcore</span> <span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--torch_dtype</span> bfloat16 <span class="se">\</span>
    <span class="nt">--output_dir</span> Qwen2.5-7B-Instruct-mcore
</code></pre></div></div> <hr/> <p>I am afraid that because I wrote the tutorial after finishing setup, so maybe there is some incompatible version and tweak steps that I forgot. So comment to tell me if you canâ€™t follow the tutorial.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="english"/><summary type="html"><![CDATA[my tutorial to install Megatron-SWIFT to train Qwen2.5 locally]]></summary></entry><entry><title type="html">Mixture-of-Experts - first digginâ€™</title><link href="https://vtrnnhlinh.github.io/blog/2025/moe-overview/" rel="alternate" type="text/html" title="Mixture-of-Experts - first digginâ€™"/><published>2025-06-18T08:30:00+00:00</published><updated>2025-06-18T08:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/moe-overview</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/moe-overview/"><![CDATA[<p>At Bosch, I have a chance to discover about Machine Learning, specifically LLM and MoE. In this post I will share the content of my first presentation about Mixture-of-Experts (MoE). I take the structure and content mainly from a survey in 2025 <a class="citation" href="#mu2025comprehensive">(Mu &amp; Lin, 2025)</a> and some information from a survey in 2024 <a class="citation" href="#cai2024survey">(Cai et al., 2024)</a>.</p> <blockquote> <p><strong>Disclaimer</strong>: I am very noob in this field, I am not sure what I wrote in this post is true. But if I find out any problems, I will update.</p> </blockquote> <h2 id="why-moe">Why MoE?</h2> <p>AI applications are developing fast, we can say some popular names like ChatGPT, Gemini, DeepSeek,â€¦ But developing it also faces some problems, 2 major problems are:</p> <ul> <li>Computational cost of training and deploying</li> <li>Integrating conflicting or heterogeneous knowledge within a single model</li> </ul> <p>So here we are, MoE is proposed to tackle these 2 problems. You can imagine MoE as a â€œdivide-and-conquerâ€ approach.</p> <h2 id="moe-components">MoE components</h2> <p>In MoE structure, we have two main parts: <strong>Experts</strong> and <strong>Router</strong>.</p> <h3 id="router">Router</h3> <p>Router works as a distributor to route data to suitable expert.</p> <p>We have <strong>Gating Function</strong> is the mathematical implementation of the Router. A good Gating Function meets 2 criteria:</p> <ul> <li>Accurately discern characteristics of both input data and experts</li> <li>Distribute evenly as possible among the predefined experts</li> </ul> <p>We can categorise Gating Function into 3 types:</p> <ul> <li><strong>Linear Gating</strong>: Using <code class="language-plaintext highlighter-rouge">softmax</code> function</li> <li><strong>Non-linear Gating</strong>: Using <strong>cosine similarity</strong> in assigning experts</li> <li><strong>Soft MoE</strong>: Combining tokens to avoid dropping tokens issues</li> </ul> <h3 id="experts">Experts</h3> <p>Experts are small LLM models that specialise in solving a defined dataset. The <strong>Experts Network</strong> based on Transformer <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a> structure.</p> <p>There are 3 popular experts network method:</p> <ul> <li>Replace FFN layer in Transformer with an MoE layer <ul> <li>Suitable to incorporate sparse activation mechanisms</li> <li>Ideal choice for introducing the MoE mechanism</li> </ul> </li> <li>Apply MoE to the attention module in Transformer <ul> <li><strong>MoA</strong> <a class="citation" href="#wang2024moa">(Wang et al., 2024)</a> Mixture-of-Attention â€“ gating network to dynamically select the most relevant attention</li> <li><strong>MoH</strong> <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> Mixture-of-Head attention â€“ has great potential</li> </ul> </li> <li>Apply MoE to CNN layer <ul> <li>Fully leverage CNNâ€™s strengths in local feature extraction</li> <li>Apply mainly in Computer Vision field</li> </ul> </li> </ul> <h2 id="moe-paths">MoE Paths</h2> <h3 id="routing-strategy">Routing Strategy</h3> <p><strong>Routing Strategy</strong> based on:</p> <ul> <li>Token-Level</li> <li>Modality-Level</li> <li>Task-Level</li> <li>Context-Level</li> <li>Attribute-Level</li> </ul> <h3 id="training-strategy">Training Strategy</h3> <p><strong>Training Strategy</strong> has 3 steps:</p> <ul> <li>Auxiliary Loss Function Design: balance usage and distribute load</li> <li>Expert Selection: choose expert for data input. Some popular methods like <em>TopK, Top1, TopP,â€¦</em></li> <li>Pipeline Design: optimize resource allocation and distribute data among experts</li> </ul> <h2 id="my-current-work">My current work</h2> <p>I am trying to use <a href="https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html">Megatron-SWIFT</a> Framework to train <strong>Qwen2.5-7B-Instruct</strong> <a class="citation" href="#qwen2.5">(Team, 2024)</a>. It is really strugging even from first step is setup environment, when have some results, I will write post sharing about that. Hopefully I can write proper tutorial the next time we meet.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="english"/><summary type="html"><![CDATA[my first take when discovering MoE]]></summary></entry><entry><title type="html">I am done my Thesis, so whatâ€™s next?</title><link href="https://vtrnnhlinh.github.io/blog/2025/thesis-reflection/" rel="alternate" type="text/html" title="I am done my Thesis, so whatâ€™s next?"/><published>2025-06-18T04:00:00+00:00</published><updated>2025-06-18T04:00:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/thesis-reflection</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/thesis-reflection/"><![CDATA[<p>I submitted the hardcover of my Thesis to my Faculty, so what will I do in near future? I once shared some details in <a href="https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection/">Quantum first reflection</a>.</p> <p>But I think there are a lot of changes comparing to the post, so I will share some details, to clarify myself and give you some more information.</p> <h2 id="past">past</h2> <p>I believe I didnâ€™t contribute much in my Thesis. That it prevents a lot of my vision if I pursue further education. My GPA is also too low in academic standard. I donâ€™t want to struggle financially, to me, finance stable is the first thing I consider. But without scholarship, everything will be harder.</p> <p>After having an intership at BGSV, I feel like I prefer the environment that I have to sit (more than) 8 hours a day. My discipline is too horrible, lol. So maybe I wonâ€™t learn master, at least for now. But I also have other plans.</p> <h2 id="current">current</h2> <p>I am researching about Mixture-of-Experts at company. At university, I still donâ€™t graduate yet, still have some subjects left.</p> <p>My plan is focusing on my work at Bosch and gain some certificates on Coursera in this summer. After the internship, I expect to make 2 CVs: one in Static Testing and one in Machine Learning.</p> <p>I believe I should finish all my study program within 2025.</p> <h2 id="future">future</h2> <p>I donâ€™t expect to be kept at Bosch, so I want to find a proper company and still accept that I donâ€™t graduate yet.</p> <p>I want to pursue another bachelor degree, maybe in Linguistics or Psychology, still not decide yet.</p> <p>Mathematics is also an aspect I believe I should spend time on.</p> <p>Thatâ€™s all.</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="cse"/><category term="plans"/><category term="english"/><summary type="html"><![CDATA[my sharing after finishing my thesis]]></summary></entry><entry><title type="html">I use Arch, btw</title><link href="https://vtrnnhlinh.github.io/blog/2025/arch/" rel="alternate" type="text/html" title="I use Arch, btw"/><published>2025-03-04T19:45:00+00:00</published><updated>2025-03-04T19:45:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/arch</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/arch/"><![CDATA[<p>At 2am, you know, holy hour of random ideas. I feel frustrated that I am using <strong>Pop!_OS 22.04</strong>. Itâ€™s already 2025, we should have 24.04 version. Took me a while to do the research, and I heard that the dev team is migrating Pop!_OS to COSMOS. A uncerntain feeling captured my mind.</p> <blockquote> <p>â€œWhat if they just drop out half way?â€</p> </blockquote> <h2 id="what-are-my-options">What are my options?</h2> <p>My mind wants to change to a new OS. What about Ubuntu? But itâ€™s too basic and Gnome has too much flaws, the idea of installing extensions and stuffs make me exhausted.</p> <p>Fedora? Kali Linux? Debian? Too many distros to choose, life is too short.</p> <p>I want a fun and challenging OS to play with. And yes, I choose <strong>Arch</strong>.</p> <p>Finally becoming an Arch user seems cool tbh.</p> <h2 id="embrace-the-adventure">Embrace the adventure</h2> <p>It wasnâ€™t easy to install arch. I tried to follow the tutorial from chatGPT but it sucks when come to partitioning part.</p> <p>I had to install arch twice to be able to boot in. The dual boot story made me stuck for a while.</p> <p>When I remove the media, boot into arch. <strong>Boom</strong>. Itâ€™s a black screen, no GUI no internet. Yup, I didnâ€™t believe that I have to install internet service manually.</p> <p>Then I had to insert the USB third time to install internet and GUI. I choose KDE as it seems better than GNOME. (Or maybe the grass is greener on the other side).</p> <h2 id="my-arch-setup">My Arch Setup</h2> <p>I use <strong>Sweet Theme and Candy icons</strong> of this chad <a href="https://github.com/EliverLara">EliverLara</a></p> <p>My main coding editor: <strong>nvim with AstroNvim+Konsole</strong>. I donâ€™t reuse my nvim setup at Pop!_OS. I tried to switch to Nvchad but I am not familiar with its workflow. Kitty terminalâ€™s NerdFont problem made me feel tired.</p> <p>Terminal emulator: <strong>Alacritty</strong>. I love its opacity :wink:</p> <p>My note-taking and research setup: <strong>Zotero+Obsidian</strong>. I tried to use Mendeley but itâ€™s too slow to me. And maybe I will write a tutorial another day to show my workflow of Zotero+Obsidian.</p> <p>Mail Client: <strong>Thunderbird</strong>. Canâ€™t find better solution.</p> <p>Browser: <strong>Firefox</strong>. But with the drama recently about its privacy. I am considering other options.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="nerdies"/><category term="english"/><summary type="html"><![CDATA[a brief story of my try on archlinux]]></summary></entry><entry><title type="html">Äáº¡i Ä‘á»™i trÆ°á»Ÿng cá»§a tÃ´i - cá»§a anh, cá»§a chÃºng ta</title><link href="https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi/" rel="alternate" type="text/html" title="Äáº¡i Ä‘á»™i trÆ°á»Ÿng cá»§a tÃ´i - cá»§a anh, cá»§a chÃºng ta"/><published>2024-11-03T09:15:00+00:00</published><updated>2024-11-03T09:15:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi/"><![CDATA[<p>ÄÃ¢y lÃ  vá»Ÿ chÃ¨o Ä‘Æ°á»£c khÃ¡ch VIP nhÃ  ta vÃ´ cÃ¹ng tÃ¢m Ä‘áº¯c giá»›i thiá»‡u nÃªn mÃ¬nh khÃ´ng thá»ƒ khÃ´ng xem. VÃ  ngÆ°á»i áº¥y quáº£ thá»±c Ä‘Ã£ khÃ´ng khiáº¿n mÃ¬nh tháº¥t vá»ng, má»™t vá»Ÿ chÃ¨o sÃ¢u sáº¯c vá» máº·t ná»™i dung vÃ  áº¥n tÆ°á»£ng vá» máº·t dÃ n dá»±ng sÃ¢n kháº¥u.</p> <p>Äáº§u tiÃªn pháº£i nÃ³i Ä‘Ã¢y khÃ´ng pháº£i lÃ  ká»‹ch báº£n gá»‘c mÃ  Ä‘Æ°á»£c chuyá»ƒn thá»ƒ tá»« vá»Ÿ ká»‹ch cÃ¹ng tÃªn, mÃ¬nh sáº½ khÃ´ng xem ná»‘t vá»Ÿ ká»‹ch rá»“i Ä‘Ã¢m bang so sÃ¡nh.</p> <p>Vá» ká»‹ch báº£n, tÃ¡c pháº©m lÃ m vá» Ä‘á» tÃ i chiáº¿n tranh nhÆ°ng khÃ´ng cá»¥ thá»ƒ má»™t tráº­n chiáº¿n nÃ o cáº£. NÃ³ giá»‘ng nhÆ° má»™t lÃ¡t cáº¯t ráº¥t nhá» trong hÆ¡n 20 nÄƒm trá»i KhÃ¡ng chiáº¿n chá»‘ng Má»¹. MÃ  vÃ¬ váº­y, ta tháº¥y Ä‘Æ°á»£c sÃ¢u sáº¯c hÆ¡n vá» ná»™i tÃ¢m cá»§a cÃ¡c chiáº¿n sÄ©. Bá»Ÿi cuá»™c chiáº¿n lá»›n nÃ o cÅ©ng váº­y, há» dá»… bá»‹ coi lÃ  con sá»‘ hÆ¡n lÃ  con ngÆ°á»i.</p> <p>CÃ¡ch dÃ n dá»±ng cÃ³ hai nhÃ¢n váº­t trung tÃ¢m vá»›i má»™t con ngÆ°á»i cÃ³ tÃ i nÄƒng nhÆ°ng tÆ° tÆ°á»Ÿng váº«n cÃ³ pháº§n khiáº¿m khuyáº¿t cÃ¹ng má»™t con ngÆ°á»i tuy cÃ³ thá»ƒ khÃ´ng giá»i báº±ng nhÆ°ng tháº³ng tháº¯n, chÃ¢n tÃ¬nh khiáº¿n mÃ¬nh khÃ´ng khá»i nhá»› tá»›i tÃ¡c pháº©m Chiáº¿n tranh vÃ  HÃ²a bÃ¬nh cá»§a Lev Tolstoy.</p> <p>Vá»Ÿ chÃ¨o lÃ  diá»…n biáº¿n, phÃ¡t triá»ƒn ná»™i tÃ¢m cá»§a nhiá»u nhÃ¢n váº­t, tá»« anh Ä‘áº¡i Ä‘á»™i trÆ°á»Ÿng tá»›i sÆ° Ä‘oÃ n trÆ°á»Ÿng, tá»« ngÆ°á»i con tá»›i ngÆ°á»i cha. Vá»Ÿ chÃ¨o cÃ²n lÃ  cÃ¡ch xá»­ lÃ­ cÃ¡c má»‘i quan há»‡ giá»¯a ngÆ°á»i vá»›i ngÆ°á»i trong thá»i Ä‘iá»ƒm Ä‘áº¡n bom Ã¡c liá»‡t, giá»¯a anh yÃªu em yÃªu, giá»¯a cáº¥p trÃªn cáº¥p dÆ°á»›i, giá»¯a cha con, vÃ  giá»¯a Ä‘á»“ng Ä‘á»™i vá»›i nhau. Äá»ƒ rá»“i báº±ng tÃ¬nh cáº£m yÃªu thÆ°Æ¡ng, thÃ¡i Ä‘á»™ tháº³ng tháº¯n chÃ¢n thÃ nh mÃ  cÃ¡c nhÃ¢n váº­t cá»§a chÃºng ta trÆ°á»Ÿng thÃ nh hÆ¡n, cÃ¹ng cá»‘ng hiáº¿n cho tháº¯ng lá»£i chung.</p> <p>Vá» dÃ n dá»±ng sÃ¢n kháº¥u, mÃ¬nh áº¥n tÆ°á»£ng nháº¥t á»Ÿ 3 phÃ¢n Ä‘oáº¡n. Äáº§u tiÃªn lÃ  2 nam chÃ­nh Ä‘ang cÃ£i nhau hÄƒng mÃ¡u thÃ¬ há»“i tÆ°á»Ÿng rá»“i sang sÃ´ng luÃ´n, Ã´i thá» lÃ  nÃ³ mÆ°á»£t nhÆ° sunsilk. Thá»© hai lÃ  Ä‘oáº¡n miÃªu táº£ ná»™i tÃ¢m cá»§a Ä‘á»©a con nhá»› máº¹, bá»Ÿi vÃ¬ loáº¡i hÃ¬nh sÃ¢n kháº¥u thÆ°á»ng náº·ng vá» ká»ƒ, lÃ m nhá»¯ng Ä‘oáº¡n Ä‘á»™c thoáº¡i cho hay khÃ´ng pháº£i dá»… vÃ  há» lÃ m hay tá»›i má»©c mÃ¬nh báº¥t ngá». Thá»© ba lÃ  phÃ¢n cáº£nh 16+ nhá» cá»§a Ä‘Ã´i chim cu dÆ°á»›i háº§m, dá»±ng ráº¥t khÃ©o, khÃ¡n giáº£ xem biáº¿t ngÆ°á»i ta Ä‘ang chim chuá»™t nhÆ°ng khÃ´ng bá»‹ thÃ´ thiá»ƒn. ÄÃ¡nh Ä‘Ã¨n cÅ©ng ráº¥t cÃ³ Ã½ tá»©, nhiá»u cáº£nh thÃ nh báº¡i lÃ  nhá» Ã¡nh sÃ¡ng luÃ´n. CÃ¡ch Ä‘iá»u khiá»ƒn nhá»‹p Ä‘á»™ cÃ¢n báº±ng giá»¯a bi vÃ  hÃ i, giá»¯a lÃºc cÄƒng tháº³ng vÃ  bÃ´ng lÆ¡i cÅ©ng lÃ  má»™t Ä‘iá»ƒm Ä‘Ã¡ng khen ngá»£i, khÃ´ng khiáº¿n ngÆ°á»i xem tháº¥y nhÃ m chÃ¡n hay cÄƒng tháº³ng quÃ¡ Ä‘á»™.</p> <p>Tá»±u chung, Äáº¡i Ä‘á»™i trÆ°á»Ÿng cá»§a tÃ´i cÃ³ hÃ m lÆ°á»£ng nghá»‡ thuáº­t Ä‘á»§ cao, cÃ³ ná»™i dung Ä‘á»§ chiá»u sÃ¢u vÃ  phá»©c táº¡p Ä‘á»ƒ ngáº«m nghÄ©. Má»™t tÃ¡c pháº©m Ä‘Ã¡ng xem.</p> <p>Cáº£m Æ¡n vÃ¬ Ä‘áº¡i ca Ä‘Ã£ giá»›i thiá»‡u thÃªm má»™t tÃ¡c pháº©m Ä‘Ã¡ng nhá»› vá» máº·t ná»™i dung láº«n nghá»‡ thuáº­t chá»© khÃ´ng pháº£i vÃ¬ á»©c cháº¿ nÃªn Ä‘Ã¡ng nhá»›. ÄÃºng lÃ  Ä‘áº¡i ca cá»§a bá»n em cÃ³ khÃ¡c, trá»™m vÃ­a, trá»™m vÃ­a.</p>]]></content><author><name></name></author><category term="Stories-of-Culture"/><category term="chÃ¨o"/><category term="vietnamese"/><summary type="html"><![CDATA[Cáº£m nháº­n vá» vá»Ÿ chÃ¨o Äáº¡i Ä‘á»™i trÆ°á»Ÿng cá»§a tÃ´i]]></summary></entry><entry><title type="html">KhÄƒn PiÃªu rÆ¡i á»Ÿ chá»‘n nÃ o - Äá»ƒ ngÆ°á»i khÃ¡ch cÅ© lao xao trong lÃ²ng</title><link href="https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon/" rel="alternate" type="text/html" title="KhÄƒn PiÃªu rÆ¡i á»Ÿ chá»‘n nÃ o - Äá»ƒ ngÆ°á»i khÃ¡ch cÅ© lao xao trong lÃ²ng"/><published>2024-09-10T14:14:00+00:00</published><updated>2024-09-10T14:14:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon/"><![CDATA[<p>Äáº¡i vÄƒn hÃ o Victor Hugo tá»«ng cÃ³ má»™t cÃ¢u nÃ³i mÃ  mÃ¬nh táº¡m dá»‹ch lÃ :<br/> â€œ<em>Lá»‹ch sá»­ lÃ  gÃ¬? LÃ  tiáº¿ng vá»ng cá»§a quÃ¡ khá»© tá»›i tÆ°Æ¡ng lai; lÃ  pháº£n chiáº¿u cá»§a tÆ°Æ¡ng lai vá» quÃ¡ khá»©</em>â€.</p> <p>NgÆ°á»£c dÃ²ng thá»i gian, quay vá» vá»›i â€œChÃ­n nÄƒm lÃ m má»™t Äiá»‡n BiÃªn/ NÃªn cÃ nh hoa Ä‘á» nÃªn thiÃªn sá»­ vÃ ngâ€, ta cÃ³ nháº¡c sÄ© DoÃ£n Nho thá»i trai tráº» phá»¥ng sá»± cho CÃ¡ch máº¡ng, cho Äá»™c láº­p, Tá»± do cá»§a DÃ¢n tá»™c. Khi áº¥y, tÃ¬nh yÃªu quÃª hÆ°Æ¡ng Ä‘áº¥t nÆ°á»›c, tÃ¬nh yÃªu Ä‘á»“ng bÃ o Ä‘Ã£ vÆ°á»£t lÃªn trÃªn táº¥t cáº£ thá»© tÃ¬nh cáº£m khÃ¡c.</p> <p>MÃ  nháº¯c tá»›i Ä‘Ã¢y, ai láº¡i khÃ´ng nhá»› tá»›i bÃ i thÆ¡ <em>Viá»‡t Báº¯c</em> cá»§a Tá»‘ Há»¯u:</p> <blockquote> <p>â€- MÃ¬nh vá» mÃ¬nh cÃ³ nhá»› ta?<br/> MÆ°á»i lÄƒm nÄƒm áº¥y thiáº¿t tha máº·n ná»“ng.<br/> MÃ¬nh vá» mÃ¬nh cÃ³ nhá»› khÃ´ng<br/> NhÃ¬n cÃ¢y nhá»› nÃºi, nhÃ¬n sÃ´ng nhá»› nguá»“n?â€¦â€</p> </blockquote> <p>Äáº¿n 1â€“2 nÄƒm sau chiáº¿n tháº¯ng, nháº¡c sÄ© má»›i quay láº¡i chá»‘n xÆ°a, lÃ²ng láº¡i rá»™n chÃºt â€œtÃ¬nh riÃªngâ€ tá»« bÃ i dÃ¢n ca dÃ¢n tá»™c XÃ¡ <em>TÄƒng A Tim</em> Ä‘á»ƒ viáº¿t bÃ i hÃ¡t â€œChiáº¿c khÄƒn rÆ¡iâ€, sau Ä‘á»•i tÃªn thÃ nh â€œChiáº¿c khÄƒn PiÃªuâ€. KhÄƒn PiÃªu lÃ  chiáº¿c khÄƒn cá»§a cÃ´ gÃ¡i dÃ¢n tá»™c ThÃ¡i dÃ¹ng Ä‘á»ƒ lÃ m tÃ­n váº­t cho tÃ¬nh yÃªu cá»§a mÃ¬nh.</p> <p>BÃ i hÃ¡t Ä‘Æ°á»£c phÃ¡t triá»ƒn tá»« dÃ¢n ca dÃ¢n tá»™c XÃ¡, hÃ¬nh tÆ°á»£ng chÃ­nh lÃ  chiáº¿c khÄƒn cá»§a dÃ¢n tá»™c ThÃ¡i, pháº£i chÄƒng Ä‘Ã¢y chÃ­nh lÃ  má»™t cÃ¢u tráº£ lá»i cho cÃ¢u â€œMÃ¬nh vá» mÃ¬nh cÃ³ nhá»› ta?â€ cá»§a ngÆ°á»i miá»n xuÃ´i cho ngÆ°á»i miá»n ngÆ°á»£c?</p> <hr/> <p><strong>Quay vá» hiá»‡n táº¡i tá»›i vá»›i tiáº¿t má»¥c cá»§a NhÃ  CÃ¡ Lá»›n.</strong></p> <blockquote> <p>â€œKhÄƒn PiÃªu dá»‡t giÃ³ vÆ°Æ¡n cÃ nh,</p> <p>TrÃ¡i tim ngÆ°á»i lÃ­nh quÃ¢n hÃ nh xá»‘n xang.</p> <p>BÆ°á»›c chÃ¢n máº¥y áº£i quan san,</p> <p>Sau lÆ°ng Ä‘iá»ƒm tá»±a báº£n lÃ ng quÃª hÆ°Æ¡ng!â€</p> </blockquote> <p>4 cÃ¢u thá»ƒ lá»¥c bÃ¡t, thá»ƒ thÆ¡ truyá»n thá»‘ng dÃ¢n tá»™c ta má»Ÿ Ä‘áº§u cho tiáº¿t má»¥c NhÃ  CÃ¡ Lá»›n.</p> <p>Nghe chÃº Tá»± Long ngÃ¢m, lÃ²ng láº¡i nghÄ© tá»›i máº¥y cÃ¢u trong <em>Chinh Phá»¥ NgÃ¢m</em> cá»§a Äáº·ng Tráº§n CÃ´n:</p> <blockquote> <p>â€œTrang phong lÆ°u Ä‘ang chá»«ng niÃªn thiáº¿u,</p> <p>SÃ¡nh nhau cÃ¹ng dan dÃ­u chá»¯ duyÃªn.</p> <p>Ná»¡ nÃ o Ä‘Ã´i lá»©a thiáº¿u niÃªn,</p> <p>Quan san Ä‘á»ƒ cÃ¡ch hÃ n huyÃªn cho Ä‘Ã nh!â€</p> </blockquote> <p><em>Chinh Phá»¥ NgÃ¢m</em> lÃ  khÃºc ai oÃ¡n cá»§a ngÆ°á»i thiáº¿u phá»¥ cÃ³ chá»“ng ra tráº­n, mang Ä‘áº­m ná»—i lo cho tÃ¬nh yÃªu lá»©a Ä‘Ã´i cá»§a mÃ¬nh. Vá»›i NhÃ  CÃ¡ Lá»›n, tÃ¬nh yÃªu Ä‘Ã´i lá»©a khÃ´ng dá»«ng láº¡i á»Ÿ chÃºt tÃ¬nh riÃªng nam ná»¯, mÃ  Ä‘Æ°á»£c phÃ¡t triá»ƒn lÃªn thÃ nh tÃ¬nh yÃªu quÃª hÆ°Æ¡ng Ä‘áº¥t nÆ°á»›c.</p> <hr/> <p>Trong tÃ¡c pháº©m <em>HoÃ ng tá»­ bÃ©</em> cá»§a Antoine de Saint-ExupÃ©ry cÃ³ má»™t cÃ¢u nhÆ° tháº¿ nÃ y:<br/> â€œ<em>â€¦Khi Ã´ng nhÃ¬n trá»i, bá»Ÿi vÃ¬ á»Ÿ má»™t trong nhá»¯ng ngÃ´i sao Ä‘Ã³ cÃ³ tÃ´i, bá»Ÿi vÃ¬ trong má»™t ngÃ´i sao Ä‘Ã³ cÃ³ tÃ´i cÆ°á»i, nÃªn Ã´ng sáº½ tÆ°á»Ÿng chá»«ng táº¥t cáº£ cÃ¡c ngÃ´i sao Ä‘á»u cÆ°á»iâ€¦</em>â€</p> <p>Pháº£i chÄƒng á»Ÿ Ä‘Ã¢y NhÃ  CÃ¡ Lá»›n cÅ©ng nghÄ© nhÆ° váº­y â€” khi nghÄ© vá» báº£n lÃ ng, bá»Ÿi vÃ¬ á»Ÿ má»™t trong nhá»¯ng báº£n lÃ ng Ä‘Ã³ cÃ³ ngÆ°á»i con gÃ¡i trao anh chiáº¿c khÄƒn PiÃªu, nÃªn táº¥t cáº£ báº£n lÃ ng trá»Ÿ thÃ nh Ä‘iá»ƒm tá»±a, lÃ­ do Ä‘á»ƒ anh chiáº¿n Ä‘áº¥u.</p> <hr/> <p><strong>VÃ  Ä‘oáº¡n X-part cá»§a Soobin Ä‘Æ°á»£c viáº¿t nhÆ° sau:</strong></p> <blockquote> <p>â€œTÃ´i bÆ°á»›c trÃªn con Ä‘Æ°á»ng gai Ä‘áº§y</p> <p>Mang theo cáº£ Tá»• quá»‘c trÃªn vai váº«n cÃ²n Ä‘ong Ä‘áº§y</p> <p>ThÃ¢n nam mÆ°á»i táº¥c anh cháº³ng quáº£n náº¯ng trÆ°a</p> <p>BÄƒng ngÃ n dáº·m Ä‘á»“i trÃ¹ng má»‹t mÃ¹ng cháº³ng ngáº¡i giÃ³ mÆ°a</p> <p>KhÄƒn PiÃªu chá» ai, nÆ¡i Ä‘Ã¢y cá» hÆ°Æ¡ng Ä‘Ã¢m chá»“i rÃ³t máº­t vÃ o tai</p> <p>Chá» má»™t ngÃ y hai ta chung Ä‘Ã´i dÃ¹ cho hai nÆ¡i xa xÃ´iâ€</p> </blockquote> <p>NgoÃ i tÃ­nh biá»ƒu trÆ°ng ráº¥t cao vá» nhá»¯ng ngÆ°á»i lÃ­nh biÃªn phÃ²ng, vá» khÃ¡t vá»ng tÃ¬nh yÃªu lá»©a Ä‘Ã´i, mÃ¬nh cÃ²n khÃ´ng khá»i liÃªn tÆ°á»Ÿng tá»›i hÃ¬nh áº£nh vá» má»™t báº­c cÃ¡i tháº¿ anh hÃ¹ng â€” cá»¥ thá»ƒ lÃ  Tá»« Háº£i trong <em>Truyá»‡n Kiá»u</em> cá»§a Ä‘áº¡i thi hÃ o Nguyá»…n Du:</p> <blockquote> <p>â€œRÃ¢u hÃ¹m, hÃ m Ã©n, mÃ y ngÃ i</p> <p>Vai nÄƒm táº¥c rá»™ng, thÃ¢n mÆ°á»i thÆ°á»›c cao.â€</p> </blockquote> <p>MÃ¬nh ráº¥t yÃªu thÃ­ch cÃ¡ch nhá»¯ng bÃ i hÃ¡t cÃ³ bá» dÃ y lá»‹ch sá»­ Ä‘Æ°á»£c viáº¿t thÃªm pháº§n X-part váº«n hÃ²a há»£p vá»›i bÃ i gá»‘c bá»Ÿi sá»± tinh táº¿ vÃ  chiá»u sÃ¢u trong tá»«ng chá»¯ Ä‘Æ°á»£c thÃªm vÃ o.</p> <hr/> <p>NgÃ y xÆ°a nháº¡c sÄ© DoÃ£n Nho, chÃ ng lÃ­nh tráº» gáº¯n bÃ³ vá»›i Viá»‡t Báº¯c náº£y ná»Ÿ rung Ä‘á»™ng chÃºt tÃ¬nh riÃªng tá»« cÃ¡i tÃ¬nh chung viáº¿t ra bÃ i hÃ¡t nÃ y. Máº¥y mÆ°Æ¡i nÄƒm sau, NhÃ  CÃ¡ Lá»›n thá»ƒ hiá»‡n láº¡i dá»±a vÃ o cÃ¡i tÃ¬nh riÃªng Ä‘Ã³ dáº«n Ä‘áº¿n tÃ¬nh chung, cÃ²n Ä‘Æ°á»£c viáº¿t dÃ nh táº·ng cho nhá»¯ng ngÆ°á»i lÃ­nh biÃªn phÃ²ng.</p> <p><strong>CÃ³ pháº£i Ä‘Ã¢y chÃ­nh lÃ  cÃ¡ch quÃ¡ khá»© vá»ng tá»›i tÆ°Æ¡ng lai, tÆ°Æ¡ng lai pháº£n chiáº¿u vá» quÃ¡ khá»©?</strong></p> <p><strong>CÃ³ pháº£i Ä‘Ã¢y chÃ­nh lÃ  lÃ­ do nháº¡c sÄ© DoÃ£n Nho â€œráº¥t xÃºc Ä‘á»™ngâ€ khi Ä‘Æ°á»£c nghe tÃ¡c pháº©m nÃ y?</strong></p> <p><strong>CÃ³ pháº£i Ä‘Ã¢y chÃ­nh lÃ  cÃ¡ch má»™t tÃ¡c pháº©m vÆ°á»£t qua thá»­ thÃ¡ch cá»§a thá»i gian vÃ  cÃ¡ch ngÆ°á»i nghá»‡ sÄ© thá»±c thá»¥ lÃ m nghá»?</strong></p> <hr/> <p>CÃ²n ti tá»‰ thá»© Ä‘á»ƒ khen vá» cÃ¡c anh NhÃ  CÃ¡ Lá»›n trong bÃ i hÃ¡t nÃ y nhÆ°ng mÃ¬nh xin phÃ©p dá»«ng láº¡i á»Ÿ Ä‘Ã¢y.</p> <p>Ráº¥t xÃºc Ä‘á»™ng khi nghe bÃ i hÃ¡t, ráº¥t tá»± hÃ o vá» chÃº Tá»± Long khi Ä‘Æ°á»£c nghe chÃº chia sáº» vá» quÃ¡ trÃ¬nh lÃ m bÃ i hÃ¡t nÃ y.</p> <p><strong>P/S</strong>: NgÆ°á»i viáº¿t khÃ´ng cÃ³ chuyÃªn mÃ´n vá» vÄƒn hÃ³a - nghá»‡ thuáº­t nÃªn khÃ´ng trÃ¡nh khá»i sai sÃ³t, ráº¥t má»Ÿ lÃ²ng Ä‘Ã³n nháº­n gÃ³p Ã½.</p>]]></content><author><name></name></author><category term="Stories-of-Culture"/><category term="music"/><category term="vietnamese"/><summary type="html"><![CDATA[Cáº£m nháº­n vá» tiáº¿t má»¥c Chiáº¿c KhÄƒn PiÃªu cá»§a nhÃ  CÃ¡ Lá»›n trong chÆ°Æ¡ng trÃ¬nh ATVNCG 2024]]></summary></entry><entry><title type="html">Quantum first reflection</title><link href="https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection/" rel="alternate" type="text/html" title="Quantum first reflection"/><published>2024-06-09T22:20:00+00:00</published><updated>2024-06-09T22:20:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection/"><![CDATA[<p>Dear readers,</p> <p>Itâ€™s my honor to share you about this. Last night, I received the email took me days waiting in worries. I passed the interview and successfully join the High Performance Quantum Computing Team at my university.</p> <p><em>Now I have to rewrite my LinkedIn profile, About section of this blog and fulfill the oath I took.</em></p> <p>What will I do in this post? Maybe sharing a little detail about my true thoughts on this intersection. I hope that through this, I can sort my memories and ideas in a structural way.</p> <h2 id="what-do-i-truly-want-to-do">What do I truly want to do?</h2> <p>I have decided to pursue IT since grade 8th? Or even earlier, but I announced that to my parents in grade 8.</p> <p>When I do the interview for my internship, I will say that I love it since a CNET article about smart house. Yes, thatâ€™s the factor I want to do IoT, Embedded Systems instead of web development but not the answer for â€œwhy do you choose IT?â€. The another factor that I love C/C++ and I hate JS with a passion.</p> <p>The real start for my obsession with this engineering is a long story involving a lot of old friends and foes. So I prefer not telling it. And honestly, I forgot the story.</p> <p>But I am really grateful of this decision. I love the feeling when I code, when I have bugs, when I spend hours just to make my terminal a little bit more shiny.</p> <p>So why do you apply into HPQC Lab? Did you lie about your motives to get into the lab?</p> <p>I apply it, because I once thought about quantum computers. And in my faraway dream, I wanted to become a researcher.</p> <blockquote> <p>â€œBachelor 4 years. Master 2 years. PhD 3 years. Then die.â€ - Once Linhâ€™s plan of life.</p> </blockquote> <p>When I entered university, that dream became hopeless. So I feel really excited when I see the announcement. Itâ€™s my last chance.</p> <p>As my <em>bot</em> Lord Elrond once said:</p> <blockquote> <p>â€œGreatness is often achieved not because one feels entirely ready, but because one is willing to rise to the occasion despite uncertainties. Even if the road seems steep, your willingness to ascend is a testament to your readiness for the journey.â€</p> </blockquote> <p>I am proud that I wonâ€™t make myself regret. I truly have a genuine interest for quantum computing, because it is hard, it is exciting and it can shake the foundation of Computer Science. Doing elegant things like that makes me feel alive.</p> <p>And I vow that I will pursue this path until the end of the world if I pass into the lab. So, waiting for my weird posts about Quantum Computing and more â€œnerdyâ€ stuffs.</p> <h2 id="what-will-you-do">What will you do?</h2> <p>I have to follow the onboarding process of the lab while doing my internship. Beside that, I have to save money for my Master. I also need to improve my GPA. So that means I have no time to waste. I have to allocate my time for my hobbies, my entertainment and my studies in a suitable way. I have to come to the war prepared.</p> <p>German is also a higher priority now. Because you know, you are pretty sure that you will go to Germany someday if you pursue this path. Donâ€™t be afraid to get rid of some other languages with poetic purposes. You can start it later if one day your German level is similar to your English one.</p> <p>That leads to I need to stop staying up late permanently. The plan in detail I wonâ€™t share here, but this post will prepare mentally for myself.</p> <hr/> <p>Thank you so much for staying with me. Thank you for not giving up, my dear Linh.</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="cse"/><category term="qc"/><category term="english"/><summary type="html"><![CDATA[my first feeling when get into HPQC Team]]></summary></entry><entry><title type="html">my neovim kickstart - new tool that I love</title><link href="https://vtrnnhlinh.github.io/blog/2024/nvim-kickstart/" rel="alternate" type="text/html" title="my neovim kickstart - new tool that I love"/><published>2024-06-07T18:08:00+00:00</published><updated>2024-06-07T18:08:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/nvim-kickstart</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/nvim-kickstart/"><![CDATA[<p>Hello my dears,</p> <p>Itâ€™s fun to announce that I am officially moving from VSC to nvim. It isnâ€™t a smooth, comfortable experience. So I am here to share my journey and hope you can find itâ€™s inspiring to try nvim yourself!</p> <h2 id="story">story</h2> <p>I had a dream to play with vim some years ago. But itâ€™s really struggling to do. I heard about the power of vim, but when I try it, I ask myself: How they can make a terminal text editor that â€œpowerfulâ€? I canâ€™t see any advantages from vim and the commands are confusing like hell. And that time, I also had a big love for VSC. So â€œusing vim is elegant and chadâ€ idea stuck in my mind, but I donâ€™t want to mess with it.</p> <p>Further on my university journey, I had to use terminal editor sometimes. I also avoided vim at all cost and used Nano instead. But then during my internship, something happen!</p> <p>My mentor is better than me, of course. I donâ€™t mention about coding skills and knowledge, but even his skill when using VSC is far better than me. And then he recommended me to use the keyboard more. So when he had to go to an exhibition and I am alone in the office, I spent my whole day dedicating for nvim, to become a chad dev so my mentor wonâ€™t make fun of me anymore!</p> <p>I did it, with a lot of pain and mistakes. So now I am here to give you some advice based on my experience.</p> <h2 id="kickstart">kickstart</h2> <p>First, I use Ubuntu. So that means this article is heavily based on this distro, I think itâ€™s similar to other platforms but idk.</p> <p>I donâ€™t use <code class="language-plaintext highlighter-rouge">sudo apt install nvim</code> to install nvim. Instead of, I use PPA repo in <code class="language-plaintext highlighter-rouge">unstable</code>. Why? Cause when I try to install plugins for nvim, I realize that my nvim version is too old. Maybe Ubuntu 23.04 and 24.04 have better updates but whatever.</p> <p>So after installing, you will find that nvim has nothing! Except some cool commands! Before digging deeper to custom vim, I highly recommend you to use <code class="language-plaintext highlighter-rouge">:Tutor</code> to start with the power of vim.</p> <p>Now you have some options to do. You can explore <a href="https://www.lazyvim.org/">LazyVim</a> or <a href="https://nvchad.com/">Nvchad</a> for easy experience. I am currently using <a href="https://github.com/nvim-lua/kickstart.nvim">kickstart</a>. But I am really considering moving to Nvchad. Haha. <em>Tried to move to Nvchad while writing this post but itâ€™s not a good experience comparing to kickstart</em>.</p> <p>So I use <a href="https://sw.kovidgoyal.net/kitty/">kitty terminal</a> as the home for nvim. Cause when you click on the nvim icon, it will open the terminal instead, lol. Because my terminal is zsh so here is the code to automatically open nvim when click kitty.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check if the terminal is Kitty and launch Neovim</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$TERM</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"xterm-kitty"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">exec </span>nvim
<span class="k">fi</span>

</code></pre></div></div> <p>Also remember to install <a href="https://www.nerdfonts.com/font-downloads">nerdfonts</a>, I personally only download the Icon Only package.</p> <h2 id="customize-kickstart">customize kickstart</h2> <p>After following the guide to install kickstart. You will open its config file by typing: <code class="language-plaintext highlighter-rouge">:e $MYVIMRC</code>.</p> <p>You can follow the file to custom your vim. I installed some other plugins like dashboard, git-flog, some shortcuts and stuffs. If you want to take a peek, visit at my github repo: <a href="https://github.com/vtrnnhlinh/kickstart.nvim">vtrnnhlinh/kickstart.nvim</a>.</p> <hr/> <p>If you have any questions, feel free to comment or send me an email. Danke Viele! I feel like this is a bad tutorial whatsoeverâ€¦</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="nerdies"/><category term="english"/><summary type="html"><![CDATA[a brief story of my try on archlinux]]></summary></entry></feed>