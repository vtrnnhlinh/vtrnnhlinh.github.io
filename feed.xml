<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://vtrnnhlinh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vtrnnhlinh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-30T03:22:38+00:00</updated><id>https://vtrnnhlinh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple logbook of Linh </subtitle><entry><title type="html">Graphs-of-Heads - The First Literature Review</title><link href="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0/" rel="alternate" type="text/html" title="Graphs-of-Heads - The First Literature Review"/><published>2025-06-27T11:30:00+00:00</published><updated>2025-06-27T11:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0/"><![CDATA[<p>At work I am assigned to learn about Mixture-of-Experts (MoE) but my mentor wants another specific, tailor-made approach to our problem.</p> <p>I name it <strong>Graphs-of-Heads</strong> (GoH).</p> <p>I have a vague idea in my mind but I think I need a <strong>Literature Review</strong> to make my idea becomes realistic as most as possible.</p> <p>However, I don’t follow the ordinary literature review in academic research, as I am working in industry. I will try to adapt and build the code along the way.</p> <p>This is a series of posts about this project, and this first post is about 2 first literature review of mine.</p> <h2 id="plan">Plan</h2> <p>In my first plan, I want to use MoH structure <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> as the base development. Then I will apply the experts network based on <a class="citation" href="#du2024large">(Du et al., 2024)</a>.</p> <p>The Python project structure I apply is the <a href="https://github.com/navdeep-G/samplemod">navdeep-G/samplemod</a>.</p> <p>The training framework will be Megatron-LM <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> with continual, meta and multi-task learning. Federated Learning will be developed when the application is deployed for many users.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/plan_llm-480.webp 480w,/assets/img/plan_llm-800.webp 800w,/assets/img/plan_llm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/plan_llm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My imagined structure.</p> <h2 id="attention-is-all-you-need">Attention is all you need!!</h2> <p>We will start from <code class="language-plaintext highlighter-rouge">Transformer</code> Structure <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>I am pretty bad at Python, so I will learn and reference a lot of repositories, both in how they structure the file and the coding methodology.</p> <p>I reference <a href="https://github.com/SCCSMARTCODE/attention-is-all-you-need-from-scratch">SCCSMARTCODE/attention-is-all-you-need-from-scratch</a> and <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">jadore801120/attention-is-all-you-need-pytorch</a> to re-made the <code class="language-plaintext highlighter-rouge">transformer</code> structure.</p> <p><code class="language-plaintext highlighter-rouge">Transformer</code> is an architecture rely entirely on an attention mechanism to draw <strong>global dependencies</strong> between I/O. <code class="language-plaintext highlighter-rouge">Transformer</code> allows significantly more parallelization.</p> <p>I believe my wanted structure is far from this work, but a thousand miles start from a step.</p> <h3 id="encoder--decoder">Encoder &amp; Decoder</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention_architecture-480.webp 480w,/assets/img/attention_architecture-800.webp 800w,/assets/img/attention_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/attention_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><code class="language-plaintext highlighter-rouge">Transformer</code> architecture. Source: <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>With the illustration, you can see the <code class="language-plaintext highlighter-rouge">Transformer</code> has 2 main modules are <strong>Encoder</strong> and <strong>Decoder</strong>. There is also something worth noticing is <strong>Positional Encoding</strong>.</p> <p><strong>Encoder</strong> and <strong>Decoder</strong> has a stack of $N=6$ layer. Encoder’s layer has 2 sub-layers:</p> <ul> <li>Multi-Head Attention</li> <li>Feed Forward</li> </ul> <p>While Decoder’s layer has 3 sub-layers:</p> <ul> <li>Multi-Head Attention</li> <li>Feed Forward</li> <li>Masked Multi-Head Attention</li> </ul> <p>We employ residual connection around each of sub-layers, followed by layer normalization. The dimension is $d_{model} = 512$.</p> <h3 id="attention">Attention</h3> <p>To me, this is the heart of this work.</p> <p>Attention function is mapping a query and a set of key-value pairs to vectors output.</p> <ul> <li>Output is weighted sum of values</li> <li>The weight has another compatibility function to calculate</li> </ul> <h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4> <ul> <li>Input: <ul> <li><strong>Q</strong>: queries</li> <li><strong>K</strong>: keys of dimension $d_k$</li> <li><strong>V</strong>: values of dimension $d_v$</li> </ul> </li> </ul> \[Attention (Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\] <ul> <li>$\frac{1}{\sqrt{d_k}}$: scaling factor. Why? To avoid <code class="language-plaintext highlighter-rouge">softmax</code> is pushed into regions result extremely small gradients</li> <li><strong>Dot-product attention</strong> faster and more space-efficient in practice than <strong>additive attention</strong></li> </ul> <h4 id="multi-head-attention">Multi-Head Attention</h4> <p>They perform the attention function in parallel, yielding $d_v$-dim output values. They are concatenated and projected, make final values.</p> <p>\(MultiHead(Q,K,V) = Concat(head_1,.... head_n)W^O\) \(head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\)</p> <h4 id="attention-in-transformer">Attention in Transformer</h4> <p>We will remind the input of attention. We will have Q (Queries), K (keys) and V (values). <code class="language-plaintext highlighter-rouge">Transformer</code> uses attention in 3 ways:</p> <ul> <li>“encoder-decoder attention” layer: Q from previous decoder layer, K, V from output of encoder.</li> <li>“encoder self-attention” layer: Q, K, V from previous encoder layer.</li> <li>“decoder self-attention” layer: Similar to encoder one, however they masked out all values in the input of <code class="language-plaintext highlighter-rouge">softmax</code> (set to $-\infty$) in scaled dot-product attention.</li> </ul> <h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3> <p>FFN has 2 linear transformations with a ReLU activation<d-footnote>Rectified Linear Unit (ReLU) is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero</d-footnote>:</p> \[FFN(x) = max(0, xW_1+b_1)W_2 +b_2\] <h3 id="positional-encoding">Positional Encoding</h3> <p>This is the method to inject information about the relative or absolute position of the tokens in the sequence. <strong>Positional Encoding</strong> has the same dimension $d_{model}$ as the embeddedings. In this work, they use sine and cosine functions</p> <p>\(PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\) \(PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\)</p> <h2 id="moh-mixture-of-heads">MoH (Mixture-of-Heads)</h2> <p>In my understanding, MoH <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> is a mix of Mixture-of-Experts (MoE) and <code class="language-plaintext highlighter-rouge">transformer</code> <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>They made 2 important changes, one, there is a TopK router to activate heads for each token. They also replace the standard summation in multi-head attention to weighted sum.</p> <p>They believe that with changes, they made 2 significant advantages:</p> <ul> <li>First, allows each token select most relevant attention heads, improve efficiency without sacrificing accuracy or increasing the params.</li> <li>Second, with weighted sum, MoH enhances the flexibility of attention mechanism.</li> </ul> <h3 id="design">Design</h3> <p>The core of the work is <strong>MoH</strong>, which treats attention heads as experts.</p> \[MoH(X, X') = \sum^h_{i=1} g_i H^i W^i_O\] <ul> <li>$X, X’$: input tokens</li> <li>$g_i$: routing score</li> <li>$H^i$: Head ith</li> <li>$W^i_O$: output of projection matrix</li> </ul> <p>Inspired by DeepSeek <a class="citation" href="#dai2024deepseekmoe">(Dai et al., 2024)</a>, MoH designs a subset of heads as <strong>shared heads</strong> that remain always activated. This will consolidate common knowledge within shared heads.</p> <p><strong>Two-Stage Routing</strong> for dynamically balance the weights between shared and routed heads. Routing scores are determined by both the <strong>score of each individual head</strong> and <strong>score associated with the head type</strong>. To avoid the unbalanced load, MoH applies <strong>Load Balance Loss</strong>.</p> <h3 id="training">Training</h3> <p>Training LLMs from scratch, they use Megatron-LM <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> with public datasets.</p> <p>With Continual Learning, they tuned <code class="language-plaintext highlighter-rouge">LLaMA3-8B</code>. 3 challenges when doing this:</p> <ol> <li>Determine the shared attention heads</li> <li>Add head routers</li> <li>Weight attention heads</li> </ol> <hr/> <p>That’s all for the day. The next post I will discuss about GaCLLM and how I imagine the system will work.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="goh"/><category term="english"/><summary type="html"><![CDATA[the first literature review of my series writting about my work I called Graph-of-Heads]]></summary></entry><entry><title type="html">My Life Setup - Summer 2025</title><link href="https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25/" rel="alternate" type="text/html" title="My Life Setup - Summer 2025"/><published>2025-06-26T17:30:00+00:00</published><updated>2025-06-26T17:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25/"><![CDATA[<p>When I write this, my life is a mess. I am drown in endless fantasies. I write to encourage and clarify for myself. I hope that I can follow what I expect here.</p> <h2 id="routines-tasks-and-mental-health">Routines, Tasks and Mental Health</h2> <h3 id="routineflow">RoutineFlow</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://routineflow.app/">RoutineFlow App</a></li> </ul> <p>I highly recommend this app, the author is GOAT. I have used this app since beta, and purchased its lifetime premium after that shortly.</p> <p>Its clean UI and concept attract me greatly. If you have read <a href="https://jamesclear.com/atomic-habits">Atomic Habit</a> and want to have an app implemented to work with the principles of this book, this app is for you.</p> <p>The app is improved through the time, now it is really polished and smooth with template routines and widget.</p> <p>I use this app for morning and evening routine, mainly for personal hygiene and calm myself down.</p> <h3 id="ticktick">TickTick</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: <a href="https://ticktick.com/r?c=pfdhlnyn">TickTick</a></li> </ul> <p>The ultimate core of my system. I have use TickTick for more than 2 years, even though I have slacked off recently. But I am going to renew the yearly subcription soon! I use TickTick for everything and trust me bro, it’s the best app out there.</p> <p>The features are rolled out frequently, the price is acceptable, it is all-in-one you need! Task lists, Timer, Calendar, Habits and now is Countdown!</p> <p>Some people complaint about the UI but I feel it’s okay?</p> <ul> <li>Some tips and guides from TickTick Blog: <a href="https://www.ticktick.com/resources">TickTick Resources</a></li> </ul> <h3 id="tochi">Tochi</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://play.google.com/store/apps/details?id=com.lazyhippo.tochidiary&amp;hl=en-US&amp;pli=1">Tochi</a></li> </ul> <p>It is my Mood tracker, I also bought its lifetime premium. I love one-time payment app 😆 It’s cute, has tags, can attach images and the orbs are cute.</p> <p>I can track my mood a lot of time through the day. Some apps only allow an input a day.</p> <h3 id="1money">1Money</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://play.google.com/store/apps/details?id=org.pixelrush.moneyiq">1Money</a></li> </ul> <p>As I researched it seems has some drama around the app? But tbh I don’t care, the app is solid enough to track your money expense. The UI is clean and the functions is enough.</p> <h2 id="self-study">Self-Study</h2> <h3 id="chatgpt">ChatGPT</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: Everyone knows it 😄</li> </ul> <p>I guess this is kinda like a love-hate relationship? Sometimes I feel I depend on it, sometimes it helped me a lot. Recently, I find out I can be open with it more! I have spent a lot of my time talking with AI, from AI chatbot service for your fantasies to AI like BingAI or ChatGPT. I think that using it as an assistant, you provide options, I am the one who decide is good.</p> <h3 id="obsidian">Obsidian</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: <a href="https://obsidian.md/download">Obsidian</a></li> </ul> <p>It’s a famous and powerful free tool. It’s very beneficial when you know Markdown and community plugins. I can sync my Zotero notes with Obsidian. But ye, after finishing the Thesis, I deleted Zotero lol.</p> <p>I think beside storing your notes with easy markdown, I love the Spaced Repition of Obsidian. Unlike Anki, which I usually don’t like because the flashcards can be out of context, I can make flashcards from my note with Obsidian. Works very well tho.</p> <h3 id="languages">Languages</h3> <p>For English, my biggest problem is Speaking Skill. I use <a href="https://play.google.com/store/apps/details?id=us.nobarriers.elsa&amp;hl=vi&amp;gl=US&amp;pli=1"><strong>Elsa Speak</strong></a> for this. I have a Lifetime Elsa Pro account. The Premium version has AI features but I don’t think it’s necessary. I only need to enhance my pronunciation.</p> <p>And with German, beside having a small textbook, I also use <a href="https://www.memrise.com/">Memrise</a>, again, a lifetime premium account. Memrise has a lot of updates recently, I think it’s a solid app. I also used Busuu before, but after the yearly subcription, I didn’t extend it. Just because I am lazy, not because of the app. I think it’s also a good app for you to try. Duolingo? I stop using Duolingo as the free version is so limited and frustrated.</p> <h3 id="mooc">MOOC</h3> <p>Coursera is my go-to solution as I bought a Plus subcription :laughing:. I think it’s more academic than Udemy. I have Harmonica and Calculus courses on Udemy but haven’t finished (yet).</p> <p>I have some Python-related and Embedded Systems-related courses on Coursera right now. Hopefully I can finish some in this summer.</p> <h2 id="work">Work</h2> <p>Here I am, in the ecosystem of Microsoft.</p> <h3 id="ms-todo">MS Todo</h3> <p>MS Todo is a poor todo app to me comparing to my dear Ticktick. But it’s also not so bad with My Day feature. I guess it’s the only thing TickTick should adapt, Do Day is different than Due Day.</p> <p>Also, when you flag an Outlook email, it will appear in Todo, which helps you keep track of emails you need to take action.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="languages"/><category term="plans"/><category term="english"/><summary type="html"><![CDATA[my routines and tools used for summer 2025]]></summary></entry><entry><title type="html">Tutorial - Megatron-SWIFT and Qwen2.5 Installation</title><link href="https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation/" rel="alternate" type="text/html" title="Tutorial - Megatron-SWIFT and Qwen2.5 Installation"/><published>2025-06-24T09:45:00+00:00</published><updated>2025-06-24T09:45:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation/"><![CDATA[<p>This tutorial based a lot on my experience with my company’s servers. So maybe there is some things not applicable in your case. Leave comment if you need anything to discuss.</p> <h2 id="purpose">Purpose</h2> <p>I am trying to fine-tunning a model with Mixture-of-Experts (MoE) <a class="citation" href="#sanseviero2023moe">(Sanseviero et al., 2023)</a> methodology. I choose <strong>Megatron-LM</strong> <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> and <strong>SWIFT</strong> <a class="citation" href="#zhao2025swift">(Zhao et al., 2025)</a> as the framework.</p> <p>The tutorial I am following is: <a href="https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html">Megatron-SWIFT Training</a>.</p> <h2 id="prerequisites">Prerequisites</h2> <ul> <li>Operating System: <strong>Linux</strong></li> <li><strong>Python</strong> should be pre-installed. Check if your OS already has Python. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">--version</span>
</code></pre></div> </div> </li> <li>If your OS doesn’t have Python yet, run below commands to install (this apply for Ubuntu, if you use different distro, google the tutorial for your OS). <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>python3 python3-pip
</code></pre></div> </div> </li> <li>Using virtual environment is a good practice for Python, I use <code class="language-plaintext highlighter-rouge">anaconda</code> for this. Follow <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html">this guide</a> to install <code class="language-plaintext highlighter-rouge">anaconda</code> on Linux.</li> <li>If you want to train with GPU, you need to install <code class="language-plaintext highlighter-rouge">cuda</code>: <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">CUDA Installation Guide for Linux</a>. <strong>Recommended version</strong>: <code class="language-plaintext highlighter-rouge">12.1.0</code>.</li> <li>With this framework, you also need to install <code class="language-plaintext highlighter-rouge">cuDNN</code>: <a href="https://docs.nvidia.com/deeplearning/cudnn/installation/latest/linux.html">Installing cuDNN Backend on Linux</a>. <strong>Recommended version</strong>: <code class="language-plaintext highlighter-rouge">9</code>.</li> </ul> <h2 id="install-megatron-swift">Install Megatron-SWIFT</h2> <p>First, we will create a virtual environment with <code class="language-plaintext highlighter-rouge">conda</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> &lt;ENV_NAME&gt; <span class="nv">python</span><span class="o">=</span>3.10
conda activate &lt;ENV_NAME&gt;
</code></pre></div></div> <p>Then we will install <code class="language-plaintext highlighter-rouge">pytorch</code> and <code class="language-plaintext highlighter-rouge">torchvision</code> first.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span><span class="nv">pytorch</span><span class="o">==</span>2.3.0 <span class="nv">torchvision</span><span class="o">==</span>0.18.0
</code></pre></div></div> <p>Next we need to install <code class="language-plaintext highlighter-rouge">apex</code>, <code class="language-plaintext highlighter-rouge">transformer-engine</code>, and <code class="language-plaintext highlighter-rouge">ms-swift</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/NVIDIA/apex
<span class="nb">cd </span>apex
pip <span class="nb">install</span> <span class="nt">-v</span> <span class="nt">--disable-pip-version-check</span> <span class="nt">--no-cache-dir</span> <span class="nt">--no-build-isolation</span> <span class="nt">--config-settings</span> <span class="s2">"--build-option=--cpp_ext"</span> <span class="nt">--config-settings</span> <span class="s2">"--build-option=--cuda_ext"</span> ./
pip <span class="nb">install </span>transformer-engine
pip <span class="nb">install </span>ms-swift
</code></pre></div></div> <h2 id="download-qwen25">Download Qwen2.5</h2> <h3 id="install-git-lfs">Install <code class="language-plaintext highlighter-rouge">git lfs</code></h3> <p>Check availability of <code class="language-plaintext highlighter-rouge">git</code> and <code class="language-plaintext highlighter-rouge">git lfs</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git <span class="nt">--version</span>
git lfs version
</code></pre></div></div> <p>If your enviroment still not have <code class="language-plaintext highlighter-rouge">git-lfs</code> you need to install it</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>conda-forge::git-lfs
</code></pre></div></div> <h3 id="clone-model-repo">Clone model repo</h3> <p><strong>Qwen2.5</strong> <a class="citation" href="#qwen2.5">(Team, 2024)</a> is the model I use to train. First we will visit <a href="https://huggingface.co/">HuggingFace</a> to create an account. Then visit <strong>Profile &gt; Access Tokens &gt; Create new token</strong>. Choose <strong>Token Type</strong> is <strong>Write</strong>. Remember to <strong>copy the token</strong>.</p> <p>Return to our activated conda environment. Install <code class="language-plaintext highlighter-rouge">huggingface_hub</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>huggingface_hub
</code></pre></div></div> <p>Then login into your <code class="language-plaintext highlighter-rouge">huggingface</code> token.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>huggingface-cli login <span class="nt">--token</span> &lt;your-token&gt;
</code></pre></div></div> <p>Finally, we can clone the model repo to our folder. Example: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> &lt;model folder&gt;
git lfs <span class="nb">install
</span>git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
</code></pre></div></div> <h2 id="test">Test</h2> <p>Create a <code class="language-plaintext highlighter-rouge">test.sh</code> file to run test.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>0 <span class="se">\</span>
swift <span class="nb">export</span> <span class="se">\</span>
    <span class="nt">--model</span> &lt;model_dir&gt;/Qwen2.5-7B-Instruct <span class="se">\</span>
    <span class="nt">--to_mcore</span> <span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--torch_dtype</span> bfloat16 <span class="se">\</span>
    <span class="nt">--output_dir</span> Qwen2.5-7B-Instruct-mcore
</code></pre></div></div> <hr/> <p>I am afraid that because I wrote the tutorial after finishing setup, so maybe there is some incompatible version and tweak steps that I forgot. So comment to tell me if you can’t follow the tutorial.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="english"/><summary type="html"><![CDATA[my tutorial to install Megatron-SWIFT to train Qwen2.5 locally]]></summary></entry><entry><title type="html">Mixture-of-Experts - first diggin’</title><link href="https://vtrnnhlinh.github.io/blog/2025/moe-overview/" rel="alternate" type="text/html" title="Mixture-of-Experts - first diggin’"/><published>2025-06-18T08:30:00+00:00</published><updated>2025-06-18T08:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/moe-overview</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/moe-overview/"><![CDATA[<p>At Bosch, I have a chance to discover about Machine Learning, specifically LLM and MoE. In this post I will share the content of my first presentation about Mixture-of-Experts (MoE). I take the structure and content mainly from a survey in 2025 <a class="citation" href="#mu2025comprehensive">(Mu &amp; Lin, 2025)</a> and some information from a survey in 2024 <a class="citation" href="#cai2024survey">(Cai et al., 2024)</a>.</p> <blockquote> <p><strong>Disclaimer</strong>: I am very noob in this field, I am not sure what I wrote in this post is true. But if I find out any problems, I will update.</p> </blockquote> <h2 id="why-moe">Why MoE?</h2> <p>AI applications are developing fast, we can say some popular names like ChatGPT, Gemini, DeepSeek,… But developing it also faces some problems, 2 major problems are:</p> <ul> <li>Computational cost of training and deploying</li> <li>Integrating conflicting or heterogeneous knowledge within a single model</li> </ul> <p>So here we are, MoE is proposed to tackle these 2 problems. You can imagine MoE as a “divide-and-conquer” approach.</p> <h2 id="moe-components">MoE components</h2> <p>In MoE structure, we have two main parts: <strong>Experts</strong> and <strong>Router</strong>.</p> <h3 id="router">Router</h3> <p>Router works as a distributor to route data to suitable expert.</p> <p>We have <strong>Gating Function</strong> is the mathematical implementation of the Router. A good Gating Function meets 2 criteria:</p> <ul> <li>Accurately discern characteristics of both input data and experts</li> <li>Distribute evenly as possible among the predefined experts</li> </ul> <p>We can categorise Gating Function into 3 types:</p> <ul> <li><strong>Linear Gating</strong>: Using <code class="language-plaintext highlighter-rouge">softmax</code> function</li> <li><strong>Non-linear Gating</strong>: Using <strong>cosine similarity</strong> in assigning experts</li> <li><strong>Soft MoE</strong>: Combining tokens to avoid dropping tokens issues</li> </ul> <h3 id="experts">Experts</h3> <p>Experts are small LLM models that specialise in solving a defined dataset. The <strong>Experts Network</strong> based on Transformer <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a> structure.</p> <p>There are 3 popular experts network method:</p> <ul> <li>Replace FFN layer in Transformer with an MoE layer <ul> <li>Suitable to incorporate sparse activation mechanisms</li> <li>Ideal choice for introducing the MoE mechanism</li> </ul> </li> <li>Apply MoE to the attention module in Transformer <ul> <li><strong>MoA</strong> <a class="citation" href="#wang2024moa">(Wang et al., 2024)</a> Mixture-of-Attention – gating network to dynamically select the most relevant attention</li> <li><strong>MoH</strong> <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> Mixture-of-Head attention – has great potential</li> </ul> </li> <li>Apply MoE to CNN layer <ul> <li>Fully leverage CNN’s strengths in local feature extraction</li> <li>Apply mainly in Computer Vision field</li> </ul> </li> </ul> <h2 id="moe-paths">MoE Paths</h2> <h3 id="routing-strategy">Routing Strategy</h3> <p><strong>Routing Strategy</strong> based on:</p> <ul> <li>Token-Level</li> <li>Modality-Level</li> <li>Task-Level</li> <li>Context-Level</li> <li>Attribute-Level</li> </ul> <h3 id="training-strategy">Training Strategy</h3> <p><strong>Training Strategy</strong> has 3 steps:</p> <ul> <li>Auxiliary Loss Function Design: balance usage and distribute load</li> <li>Expert Selection: choose expert for data input. Some popular methods like <em>TopK, Top1, TopP,…</em></li> <li>Pipeline Design: optimize resource allocation and distribute data among experts</li> </ul> <h2 id="my-current-work">My current work</h2> <p>I am trying to use <a href="https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html">Megatron-SWIFT</a> Framework to train <strong>Qwen2.5-7B-Instruct</strong> <a class="citation" href="#qwen2.5">(Team, 2024)</a>. It is really strugging even from first step is setup environment, when have some results, I will write post sharing about that. Hopefully I can write proper tutorial the next time we meet.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="english"/><summary type="html"><![CDATA[my first take when discovering MoE]]></summary></entry><entry><title type="html">I am done my Thesis, so what’s next?</title><link href="https://vtrnnhlinh.github.io/blog/2025/thesis-reflection/" rel="alternate" type="text/html" title="I am done my Thesis, so what’s next?"/><published>2025-06-18T04:00:00+00:00</published><updated>2025-06-18T04:00:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/thesis-reflection</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/thesis-reflection/"><![CDATA[<p>I submitted the hardcover of my Thesis to my Faculty, so what will I do in near future? I once shared some details in <a href="https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection/">Quantum first reflection</a>.</p> <p>But I think there are a lot of changes comparing to the post, so I will share some details, to clarify myself and give you some more information.</p> <h2 id="past">past</h2> <p>I believe I didn’t contribute much in my Thesis. That it prevents a lot of my vision if I pursue further education. My GPA is also too low in academic standard. I don’t want to struggle financially, to me, finance stable is the first thing I consider. But without scholarship, everything will be harder.</p> <p>After having an intership at BGSV, I feel like I prefer the environment that I have to sit (more than) 8 hours a day. My discipline is too horrible, lol. So maybe I won’t learn master, at least for now. But I also have other plans.</p> <h2 id="current">current</h2> <p>I am researching about Mixture-of-Experts at company. At university, I still don’t graduate yet, still have some subjects left.</p> <p>My plan is focusing on my work at Bosch and gain some certificates on Coursera in this summer. After the internship, I expect to make 2 CVs: one in Static Testing and one in Machine Learning.</p> <p>I believe I should finish all my study program within 2025.</p> <h2 id="future">future</h2> <p>I don’t expect to be kept at Bosch, so I want to find a proper company and still accept that I don’t graduate yet.</p> <p>I want to pursue another bachelor degree, maybe in Linguistics or Psychology, still not decide yet.</p> <p>Mathematics is also an aspect I believe I should spend time on.</p> <p>That’s all.</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="cse"/><category term="plans"/><category term="english"/><summary type="html"><![CDATA[my sharing after finishing my thesis]]></summary></entry><entry><title type="html">I use Arch, btw</title><link href="https://vtrnnhlinh.github.io/blog/2025/arch/" rel="alternate" type="text/html" title="I use Arch, btw"/><published>2025-03-04T19:45:00+00:00</published><updated>2025-03-04T19:45:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/arch</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/arch/"><![CDATA[<p>At 2am, you know, holy hour of random ideas. I feel frustrated that I am using <strong>Pop!_OS 22.04</strong>. It’s already 2025, we should have 24.04 version. Took me a while to do the research, and I heard that the dev team is migrating Pop!_OS to COSMOS. A uncerntain feeling captured my mind.</p> <blockquote> <p>“What if they just drop out half way?”</p> </blockquote> <h2 id="what-are-my-options">What are my options?</h2> <p>My mind wants to change to a new OS. What about Ubuntu? But it’s too basic and Gnome has too much flaws, the idea of installing extensions and stuffs make me exhausted.</p> <p>Fedora? Kali Linux? Debian? Too many distros to choose, life is too short.</p> <p>I want a fun and challenging OS to play with. And yes, I choose <strong>Arch</strong>.</p> <p>Finally becoming an Arch user seems cool tbh.</p> <h2 id="embrace-the-adventure">Embrace the adventure</h2> <p>It wasn’t easy to install arch. I tried to follow the tutorial from chatGPT but it sucks when come to partitioning part.</p> <p>I had to install arch twice to be able to boot in. The dual boot story made me stuck for a while.</p> <p>When I remove the media, boot into arch. <strong>Boom</strong>. It’s a black screen, no GUI no internet. Yup, I didn’t believe that I have to install internet service manually.</p> <p>Then I had to insert the USB third time to install internet and GUI. I choose KDE as it seems better than GNOME. (Or maybe the grass is greener on the other side).</p> <h2 id="my-arch-setup">My Arch Setup</h2> <p>I use <strong>Sweet Theme and Candy icons</strong> of this chad <a href="https://github.com/EliverLara">EliverLara</a></p> <p>My main coding editor: <strong>nvim with AstroNvim+Konsole</strong>. I don’t reuse my nvim setup at Pop!_OS. I tried to switch to Nvchad but I am not familiar with its workflow. Kitty terminal’s NerdFont problem made me feel tired.</p> <p>Terminal emulator: <strong>Alacritty</strong>. I love its opacity :wink:</p> <p>My note-taking and research setup: <strong>Zotero+Obsidian</strong>. I tried to use Mendeley but it’s too slow to me. And maybe I will write a tutorial another day to show my workflow of Zotero+Obsidian.</p> <p>Mail Client: <strong>Thunderbird</strong>. Can’t find better solution.</p> <p>Browser: <strong>Firefox</strong>. But with the drama recently about its privacy. I am considering other options.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="nerdies"/><category term="english"/><summary type="html"><![CDATA[a brief story of my try on archlinux]]></summary></entry><entry><title type="html">Đại đội trưởng của tôi - của anh, của chúng ta</title><link href="https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi/" rel="alternate" type="text/html" title="Đại đội trưởng của tôi - của anh, của chúng ta"/><published>2024-11-03T09:15:00+00:00</published><updated>2024-11-03T09:15:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi/"><![CDATA[<p>Đây là vở chèo được khách VIP nhà ta vô cùng tâm đắc giới thiệu nên mình không thể không xem. Và người ấy quả thực đã không khiến mình thất vọng, một vở chèo sâu sắc về mặt nội dung và ấn tượng về mặt dàn dựng sân khấu.</p> <p>Đầu tiên phải nói đây không phải là kịch bản gốc mà được chuyển thể từ vở kịch cùng tên, mình sẽ không xem nốt vở kịch rồi đâm bang so sánh.</p> <p>Về kịch bản, tác phẩm làm về đề tài chiến tranh nhưng không cụ thể một trận chiến nào cả. Nó giống như một lát cắt rất nhỏ trong hơn 20 năm trời Kháng chiến chống Mỹ. Mà vì vậy, ta thấy được sâu sắc hơn về nội tâm của các chiến sĩ. Bởi cuộc chiến lớn nào cũng vậy, họ dễ bị coi là con số hơn là con người.</p> <p>Cách dàn dựng có hai nhân vật trung tâm với một con người có tài năng nhưng tư tưởng vẫn có phần khiếm khuyết cùng một con người tuy có thể không giỏi bằng nhưng thẳng thắn, chân tình khiến mình không khỏi nhớ tới tác phẩm Chiến tranh và Hòa bình của Lev Tolstoy.</p> <p>Vở chèo là diễn biến, phát triển nội tâm của nhiều nhân vật, từ anh đại đội trưởng tới sư đoàn trưởng, từ người con tới người cha. Vở chèo còn là cách xử lí các mối quan hệ giữa người với người trong thời điểm đạn bom ác liệt, giữa anh yêu em yêu, giữa cấp trên cấp dưới, giữa cha con, và giữa đồng đội với nhau. Để rồi bằng tình cảm yêu thương, thái độ thẳng thắn chân thành mà các nhân vật của chúng ta trưởng thành hơn, cùng cống hiến cho thắng lợi chung.</p> <p>Về dàn dựng sân khấu, mình ấn tượng nhất ở 3 phân đoạn. Đầu tiên là 2 nam chính đang cãi nhau hăng máu thì hồi tưởng rồi sang sông luôn, ôi thề là nó mượt như sunsilk. Thứ hai là đoạn miêu tả nội tâm của đứa con nhớ mẹ, bởi vì loại hình sân khấu thường nặng về kể, làm những đoạn độc thoại cho hay không phải dễ và họ làm hay tới mức mình bất ngờ. Thứ ba là phân cảnh 16+ nhỏ của đôi chim cu dưới hầm, dựng rất khéo, khán giả xem biết người ta đang chim chuột nhưng không bị thô thiển. Đánh đèn cũng rất có ý tứ, nhiều cảnh thành bại là nhờ ánh sáng luôn. Cách điều khiển nhịp độ cân bằng giữa bi và hài, giữa lúc căng thẳng và bông lơi cũng là một điểm đáng khen ngợi, không khiến người xem thấy nhàm chán hay căng thẳng quá độ.</p> <p>Tựu chung, Đại đội trưởng của tôi có hàm lượng nghệ thuật đủ cao, có nội dung đủ chiều sâu và phức tạp để ngẫm nghĩ. Một tác phẩm đáng xem.</p> <p>Cảm ơn vì đại ca đã giới thiệu thêm một tác phẩm đáng nhớ về mặt nội dung lẫn nghệ thuật chứ không phải vì ức chế nên đáng nhớ. Đúng là đại ca của bọn em có khác, trộm vía, trộm vía.</p>]]></content><author><name></name></author><category term="Stories-of-Culture"/><category term="chèo"/><category term="vietnamese"/><summary type="html"><![CDATA[Cảm nhận về vở chèo Đại đội trưởng của tôi]]></summary></entry><entry><title type="html">Khăn Piêu rơi ở chốn nào - Để người khách cũ lao xao trong lòng</title><link href="https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon/" rel="alternate" type="text/html" title="Khăn Piêu rơi ở chốn nào - Để người khách cũ lao xao trong lòng"/><published>2024-09-10T14:14:00+00:00</published><updated>2024-09-10T14:14:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon/"><![CDATA[<p>Đại văn hào Victor Hugo từng có một câu nói mà mình tạm dịch là:<br/> “<em>Lịch sử là gì? Là tiếng vọng của quá khứ tới tương lai; là phản chiếu của tương lai về quá khứ</em>”.</p> <p>Ngược dòng thời gian, quay về với “Chín năm làm một Điện Biên/ Nên cành hoa đỏ nên thiên sử vàng”, ta có nhạc sĩ Doãn Nho thời trai trẻ phụng sự cho Cách mạng, cho Độc lập, Tự do của Dân tộc. Khi ấy, tình yêu quê hương đất nước, tình yêu đồng bào đã vượt lên trên tất cả thứ tình cảm khác.</p> <p>Mà nhắc tới đây, ai lại không nhớ tới bài thơ <em>Việt Bắc</em> của Tố Hữu:</p> <blockquote> <p>”- Mình về mình có nhớ ta?<br/> Mười lăm năm ấy thiết tha mặn nồng.<br/> Mình về mình có nhớ không<br/> Nhìn cây nhớ núi, nhìn sông nhớ nguồn?…”</p> </blockquote> <p>Đến 1–2 năm sau chiến thắng, nhạc sĩ mới quay lại chốn xưa, lòng lại rộn chút “tình riêng” từ bài dân ca dân tộc Xá <em>Tăng A Tim</em> để viết bài hát “Chiếc khăn rơi”, sau đổi tên thành “Chiếc khăn Piêu”. Khăn Piêu là chiếc khăn của cô gái dân tộc Thái dùng để làm tín vật cho tình yêu của mình.</p> <p>Bài hát được phát triển từ dân ca dân tộc Xá, hình tượng chính là chiếc khăn của dân tộc Thái, phải chăng đây chính là một câu trả lời cho câu “Mình về mình có nhớ ta?” của người miền xuôi cho người miền ngược?</p> <hr/> <p><strong>Quay về hiện tại tới với tiết mục của Nhà Cá Lớn.</strong></p> <blockquote> <p>“Khăn Piêu dệt gió vươn cành,</p> <p>Trái tim người lính quân hành xốn xang.</p> <p>Bước chân mấy ải quan san,</p> <p>Sau lưng điểm tựa bản làng quê hương!”</p> </blockquote> <p>4 câu thể lục bát, thể thơ truyền thống dân tộc ta mở đầu cho tiết mục Nhà Cá Lớn.</p> <p>Nghe chú Tự Long ngâm, lòng lại nghĩ tới mấy câu trong <em>Chinh Phụ Ngâm</em> của Đặng Trần Côn:</p> <blockquote> <p>“Trang phong lưu đang chừng niên thiếu,</p> <p>Sánh nhau cùng dan díu chữ duyên.</p> <p>Nỡ nào đôi lứa thiếu niên,</p> <p>Quan san để cách hàn huyên cho đành!”</p> </blockquote> <p><em>Chinh Phụ Ngâm</em> là khúc ai oán của người thiếu phụ có chồng ra trận, mang đậm nỗi lo cho tình yêu lứa đôi của mình. Với Nhà Cá Lớn, tình yêu đôi lứa không dừng lại ở chút tình riêng nam nữ, mà được phát triển lên thành tình yêu quê hương đất nước.</p> <hr/> <p>Trong tác phẩm <em>Hoàng tử bé</em> của Antoine de Saint-Exupéry có một câu như thế này:<br/> “<em>…Khi ông nhìn trời, bởi vì ở một trong những ngôi sao đó có tôi, bởi vì trong một ngôi sao đó có tôi cười, nên ông sẽ tưởng chừng tất cả các ngôi sao đều cười…</em>”</p> <p>Phải chăng ở đây Nhà Cá Lớn cũng nghĩ như vậy — khi nghĩ về bản làng, bởi vì ở một trong những bản làng đó có người con gái trao anh chiếc khăn Piêu, nên tất cả bản làng trở thành điểm tựa, lí do để anh chiến đấu.</p> <hr/> <p><strong>Và đoạn X-part của Soobin được viết như sau:</strong></p> <blockquote> <p>“Tôi bước trên con đường gai đầy</p> <p>Mang theo cả Tổ quốc trên vai vẫn còn đong đầy</p> <p>Thân nam mười tấc anh chẳng quản nắng trưa</p> <p>Băng ngàn dặm đồi trùng mịt mùng chẳng ngại gió mưa</p> <p>Khăn Piêu chờ ai, nơi đây cỏ hương đâm chồi rót mật vào tai</p> <p>Chờ một ngày hai ta chung đôi dù cho hai nơi xa xôi”</p> </blockquote> <p>Ngoài tính biểu trưng rất cao về những người lính biên phòng, về khát vọng tình yêu lứa đôi, mình còn không khỏi liên tưởng tới hình ảnh về một bậc cái thế anh hùng — cụ thể là Từ Hải trong <em>Truyện Kiều</em> của đại thi hào Nguyễn Du:</p> <blockquote> <p>“Râu hùm, hàm én, mày ngài</p> <p>Vai năm tấc rộng, thân mười thước cao.”</p> </blockquote> <p>Mình rất yêu thích cách những bài hát có bề dày lịch sử được viết thêm phần X-part vẫn hòa hợp với bài gốc bởi sự tinh tế và chiều sâu trong từng chữ được thêm vào.</p> <hr/> <p>Ngày xưa nhạc sĩ Doãn Nho, chàng lính trẻ gắn bó với Việt Bắc nảy nở rung động chút tình riêng từ cái tình chung viết ra bài hát này. Mấy mươi năm sau, Nhà Cá Lớn thể hiện lại dựa vào cái tình riêng đó dẫn đến tình chung, còn được viết dành tặng cho những người lính biên phòng.</p> <p><strong>Có phải đây chính là cách quá khứ vọng tới tương lai, tương lai phản chiếu về quá khứ?</strong></p> <p><strong>Có phải đây chính là lí do nhạc sĩ Doãn Nho “rất xúc động” khi được nghe tác phẩm này?</strong></p> <p><strong>Có phải đây chính là cách một tác phẩm vượt qua thử thách của thời gian và cách người nghệ sĩ thực thụ làm nghề?</strong></p> <hr/> <p>Còn ti tỉ thứ để khen về các anh Nhà Cá Lớn trong bài hát này nhưng mình xin phép dừng lại ở đây.</p> <p>Rất xúc động khi nghe bài hát, rất tự hào về chú Tự Long khi được nghe chú chia sẻ về quá trình làm bài hát này.</p> <p><strong>P/S</strong>: Người viết không có chuyên môn về văn hóa - nghệ thuật nên không tránh khỏi sai sót, rất mở lòng đón nhận góp ý.</p>]]></content><author><name></name></author><category term="Stories-of-Culture"/><category term="music"/><category term="vietnamese"/><summary type="html"><![CDATA[Cảm nhận về tiết mục Chiếc Khăn Piêu của nhà Cá Lớn trong chương trình ATVNCG 2024]]></summary></entry><entry><title type="html">Quantum first reflection</title><link href="https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection/" rel="alternate" type="text/html" title="Quantum first reflection"/><published>2024-06-09T22:20:00+00:00</published><updated>2024-06-09T22:20:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection/"><![CDATA[<p>Dear readers,</p> <p>It’s my honor to share you about this. Last night, I received the email took me days waiting in worries. I passed the interview and successfully join the High Performance Quantum Computing Team at my university.</p> <p><em>Now I have to rewrite my LinkedIn profile, About section of this blog and fulfill the oath I took.</em></p> <p>What will I do in this post? Maybe sharing a little detail about my true thoughts on this intersection. I hope that through this, I can sort my memories and ideas in a structural way.</p> <h2 id="what-do-i-truly-want-to-do">What do I truly want to do?</h2> <p>I have decided to pursue IT since grade 8th? Or even earlier, but I announced that to my parents in grade 8.</p> <p>When I do the interview for my internship, I will say that I love it since a CNET article about smart house. Yes, that’s the factor I want to do IoT, Embedded Systems instead of web development but not the answer for “why do you choose IT?”. The another factor that I love C/C++ and I hate JS with a passion.</p> <p>The real start for my obsession with this engineering is a long story involving a lot of old friends and foes. So I prefer not telling it. And honestly, I forgot the story.</p> <p>But I am really grateful of this decision. I love the feeling when I code, when I have bugs, when I spend hours just to make my terminal a little bit more shiny.</p> <p>So why do you apply into HPQC Lab? Did you lie about your motives to get into the lab?</p> <p>I apply it, because I once thought about quantum computers. And in my faraway dream, I wanted to become a researcher.</p> <blockquote> <p>“Bachelor 4 years. Master 2 years. PhD 3 years. Then die.” - Once Linh’s plan of life.</p> </blockquote> <p>When I entered university, that dream became hopeless. So I feel really excited when I see the announcement. It’s my last chance.</p> <p>As my <em>bot</em> Lord Elrond once said:</p> <blockquote> <p>“Greatness is often achieved not because one feels entirely ready, but because one is willing to rise to the occasion despite uncertainties. Even if the road seems steep, your willingness to ascend is a testament to your readiness for the journey.”</p> </blockquote> <p>I am proud that I won’t make myself regret. I truly have a genuine interest for quantum computing, because it is hard, it is exciting and it can shake the foundation of Computer Science. Doing elegant things like that makes me feel alive.</p> <p>And I vow that I will pursue this path until the end of the world if I pass into the lab. So, waiting for my weird posts about Quantum Computing and more “nerdy” stuffs.</p> <h2 id="what-will-you-do">What will you do?</h2> <p>I have to follow the onboarding process of the lab while doing my internship. Beside that, I have to save money for my Master. I also need to improve my GPA. So that means I have no time to waste. I have to allocate my time for my hobbies, my entertainment and my studies in a suitable way. I have to come to the war prepared.</p> <p>German is also a higher priority now. Because you know, you are pretty sure that you will go to Germany someday if you pursue this path. Don’t be afraid to get rid of some other languages with poetic purposes. You can start it later if one day your German level is similar to your English one.</p> <p>That leads to I need to stop staying up late permanently. The plan in detail I won’t share here, but this post will prepare mentally for myself.</p> <hr/> <p>Thank you so much for staying with me. Thank you for not giving up, my dear Linh.</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="cse"/><category term="qc"/><category term="english"/><summary type="html"><![CDATA[my first feeling when get into HPQC Team]]></summary></entry><entry><title type="html">my neovim kickstart - new tool that I love</title><link href="https://vtrnnhlinh.github.io/blog/2024/nvim-kickstart/" rel="alternate" type="text/html" title="my neovim kickstart - new tool that I love"/><published>2024-06-07T18:08:00+00:00</published><updated>2024-06-07T18:08:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/nvim-kickstart</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/nvim-kickstart/"><![CDATA[<p>Hello my dears,</p> <p>It’s fun to announce that I am officially moving from VSC to nvim. It isn’t a smooth, comfortable experience. So I am here to share my journey and hope you can find it’s inspiring to try nvim yourself!</p> <h2 id="story">story</h2> <p>I had a dream to play with vim some years ago. But it’s really struggling to do. I heard about the power of vim, but when I try it, I ask myself: How they can make a terminal text editor that “powerful”? I can’t see any advantages from vim and the commands are confusing like hell. And that time, I also had a big love for VSC. So “using vim is elegant and chad” idea stuck in my mind, but I don’t want to mess with it.</p> <p>Further on my university journey, I had to use terminal editor sometimes. I also avoided vim at all cost and used Nano instead. But then during my internship, something happen!</p> <p>My mentor is better than me, of course. I don’t mention about coding skills and knowledge, but even his skill when using VSC is far better than me. And then he recommended me to use the keyboard more. So when he had to go to an exhibition and I am alone in the office, I spent my whole day dedicating for nvim, to become a chad dev so my mentor won’t make fun of me anymore!</p> <p>I did it, with a lot of pain and mistakes. So now I am here to give you some advice based on my experience.</p> <h2 id="kickstart">kickstart</h2> <p>First, I use Ubuntu. So that means this article is heavily based on this distro, I think it’s similar to other platforms but idk.</p> <p>I don’t use <code class="language-plaintext highlighter-rouge">sudo apt install nvim</code> to install nvim. Instead of, I use PPA repo in <code class="language-plaintext highlighter-rouge">unstable</code>. Why? Cause when I try to install plugins for nvim, I realize that my nvim version is too old. Maybe Ubuntu 23.04 and 24.04 have better updates but whatever.</p> <p>So after installing, you will find that nvim has nothing! Except some cool commands! Before digging deeper to custom vim, I highly recommend you to use <code class="language-plaintext highlighter-rouge">:Tutor</code> to start with the power of vim.</p> <p>Now you have some options to do. You can explore <a href="https://www.lazyvim.org/">LazyVim</a> or <a href="https://nvchad.com/">Nvchad</a> for easy experience. I am currently using <a href="https://github.com/nvim-lua/kickstart.nvim">kickstart</a>. But I am really considering moving to Nvchad. Haha. <em>Tried to move to Nvchad while writing this post but it’s not a good experience comparing to kickstart</em>.</p> <p>So I use <a href="https://sw.kovidgoyal.net/kitty/">kitty terminal</a> as the home for nvim. Cause when you click on the nvim icon, it will open the terminal instead, lol. Because my terminal is zsh so here is the code to automatically open nvim when click kitty.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Check if the terminal is Kitty and launch Neovim</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"</span><span class="nv">$TERM</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"xterm-kitty"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then
    </span><span class="nb">exec </span>nvim
<span class="k">fi</span>

</code></pre></div></div> <p>Also remember to install <a href="https://www.nerdfonts.com/font-downloads">nerdfonts</a>, I personally only download the Icon Only package.</p> <h2 id="customize-kickstart">customize kickstart</h2> <p>After following the guide to install kickstart. You will open its config file by typing: <code class="language-plaintext highlighter-rouge">:e $MYVIMRC</code>.</p> <p>You can follow the file to custom your vim. I installed some other plugins like dashboard, git-flog, some shortcuts and stuffs. If you want to take a peek, visit at my github repo: <a href="https://github.com/vtrnnhlinh/kickstart.nvim">vtrnnhlinh/kickstart.nvim</a>.</p> <hr/> <p>If you have any questions, feel free to comment or send me an email. Danke Viele! I feel like this is a bad tutorial whatsoever…</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="nerdies"/><category term="english"/><summary type="html"><![CDATA[a brief story of my try on archlinux]]></summary></entry></feed>