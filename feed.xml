<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://vtrnnhlinh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vtrnnhlinh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-18T02:31:14+00:00</updated><id>https://vtrnnhlinh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple logbook of Linh </subtitle><entry><title type="html">Obsidian Tour - August 2025</title><link href="https://vtrnnhlinh.github.io/blog/2025/obsidian-aug-2025/" rel="alternate" type="text/html" title="Obsidian Tour - August 2025"/><published>2025-08-12T06:50:00+00:00</published><updated>2025-08-12T06:50:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/obsidian-aug-2025</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/obsidian-aug-2025/"><![CDATA[<p>It was a long time since I start using Obsidian again, only when I recognize that I can install Obsidian on my company laptop without admin rights :skull:. I decide to give myself a fresh start, let check out what we will install and take it cool :sunglasses:.</p> <h2 id="theme">Theme</h2> <p>My choice is <strong>gruvbox</strong>, always <strong>gruvbox</strong>, <strong>gruvbox</strong> is da bezt.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gruvbox_obsidian-480.webp 480w,/assets/img/gruvbox_obsidian-800.webp 800w,/assets/img/gruvbox_obsidian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gruvbox_obsidian.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My gruvbox theme on my Arch üòÜ</p> <p>There are a lot of great great choices about themes on Obsidian, but keep in mind to make it simple or you can be overwhelmed.</p> <h2 id="default-settings">Default Settings</h2> <p>First thing to do ever when you use Obsidian: <code class="language-plaintext highlighter-rouge">Files &amp; Links &gt; Automatically update internal links: ON</code></p> <p>Then check out the Daily Note and prepare the template. You don‚Äôt need a fancy one, but focus on what will be meaningful to you. Like mine I need a reflection on what I should do.</p> <p>I avoid creating too much folders, as we can structure the notes through tags and links. Why? I believing creating too much folders will make friction to my creating notes as I have to choose which folder I should put the note on. Without folders and only tags, I can use shortcut to quickly create notes, and I can apply templates to have pre-fined tags and structure for each specialised note. I have 2 folders, one is for <strong>Templates</strong> and one is for <strong>Daily</strong> as I can automate Daily folder.</p> <h2 id="community-plugins">Community Plugins</h2> <h3 id="dataview">Dataview</h3> <p>First one is <strong>Dataview</strong>, I use this to have an overview for my reading list. The useful resource is <a href="https://blacksmithgu.github.io/obsidian-dataview/resources/examples/">Examples - Dataview</a>.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dataviewjs_obsidian-480.webp 480w,/assets/img/dataviewjs_obsidian-800.webp 800w,/assets/img/dataviewjs_obsidian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/dataviewjs_obsidian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I use Dataview to tracking my books like this.</p> <h3 id="spaced-repetition">Spaced Repetition</h3> <p>Second one is <strong>Spaced Repetition</strong>. This plugin makes Obsidian more lively and more interactive. With this plugin, you can review knowledge easier without using a seperated flashcard app like Anki or Quizlet.</p> <p>Some recommended settings by me:</p> <ul> <li><strong>Cloze</strong>: Use <code class="language-plaintext highlighter-rouge">Convert ==highlights== to clozes</code> setting. Bold settings can be messy because sometimes you don‚Äôt intently want the bold to be cloze. Using curly brackets is the default Cloze setting on Anki.</li> <li><strong>Inline flashcard</strong>: I use <code class="language-plaintext highlighter-rouge">;</code> instead of <code class="language-plaintext highlighter-rouge">:</code> cause sometimes you don‚Äôt mean to make flashcard when using <code class="language-plaintext highlighter-rouge">:</code>. <code class="language-plaintext highlighter-rouge">;</code> is more rarely used and you have clearly intention when using it making flashcards.</li> <li><strong>Multiline flashcard</strong>: Because of multiline flashcard‚Äôs format, when <code class="language-plaintext highlighter-rouge">?</code> must lies in a seperate line. I have no problem using it.</li> </ul> <p>I don‚Äôt use <strong>Notes</strong> feature, I find it is not effective and passive.</p> <h3 id="tag-wrangler">Tag Wrangler</h3> <p>Tags is a powerful tool of Obsidian, and <strong>Tag Wrangler</strong> is the one saves your tags life. It will help your life easier to modify your tags in bulk or enhance searching with tags.</p> <h3 id="chronology">Chronology</h3> <p>Final one is <strong>Chronology</strong>, if you search ‚ÄúCalendar‚Äù on Obsidian Community Plugin page, it won‚Äôt be so impressive. But it‚Äôs my always first choice for Calendar on Obsidian. I love its heatmap, very aesthetic :sparkles: And I can easily search or trace back the notes I created or modified that day.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/chronology_obsidian-480.webp 480w,/assets/img/chronology_obsidian-800.webp 800w,/assets/img/chronology_obsidian-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/chronology_obsidian.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Chronology in my Obsidian.</p> <hr/> <p>Will update when I have some dramatic change!</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="nerdies"/><category term="english"/><summary type="html"><![CDATA[my obsidian theme and setup at August 2025]]></summary></entry><entry><title type="html">GraphRAG and Linh - theory and practice</title><link href="https://vtrnnhlinh.github.io/blog/2025/graphrag-theory-and-practice/" rel="alternate" type="text/html" title="GraphRAG and Linh - theory and practice"/><published>2025-08-11T10:50:00+00:00</published><updated>2025-08-11T10:50:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/graphrag-theory-and-practice</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/graphrag-theory-and-practice/"><![CDATA[<p>This will be a mixture of theory and real-life experience. When I type this line, god knows what I am doing. Unlike previous times, I jump into doing immediately like blind in the darkness, even though I made progress and basically <em>it runs!</em>. But I feel like I am missing a lot of powerful points in this concept, so I write this post, to force myself systemize the <em>shiet</em> I am doing.</p> <h2 id="the-theory-and-the-idea">The Theory and The Idea</h2> <p><strong>GraphRAG</strong> is the work of Microsoft <a class="citation" href="#edge2024local">(Edge et al., 2024)</a>, it can be described like below:</p> <pre><code class="language-mermaid">graph TD
    A[ü§ñ Use LLM to Construct Knowledge Graph from Data/Text] --&gt; B[üóÇ Partition Graph into Hierarchical Communities]
    B --&gt; C[üìù Use LLM to Generate Summaries for Each Community]
    C --&gt; D[üì¶ Store Community Summaries in Vector Store]
    D --&gt; E[‚ùì Receive User Query]
    E --&gt; F[üîç Retrieve Relevant Community Summaries via Semantic Search]
    F --&gt; G[üó∫ Map-Reduce Reasoning: Map ‚Üí Process Each Summary Independently]
    G --&gt; H[üß† Reduce ‚Üí Aggregate &amp; Refine Answers]
    H --&gt; I[‚úÖ Return Final Answer to User]
    
    I --&gt;|Feedback/Refinement| A
</code></pre> <p>To me, the hardest point of this work is graphing the knowledge graph, which is heavier about the technique, not the idea.</p> <h2 id="ramblin-ramblin">ramblin‚Äô ramblin‚Äô</h2> <p>In my mindset, RAG has 3 steps: prepare the database, retrieve the data and reasoning with the data. This post only shares 2 first step, as the last step maybe involves other‚Äôs work. I will talk vaguely about last one in the <strong>retrievin‚Äô the graph</strong> section.</p> <h3 id="shapin-the-graph">shapin‚Äô the graph</h3> <p>At this very moment, I have my graph already~, we use <a href="https://neo4j.com/">neo4j</a> to become our graph database service.</p> <p>I use <code class="language-plaintext highlighter-rouge">localhost</code>, virgin uses cloud service, chad uses localhost :wink:</p> <p>We will show final result first, to show some aesthetic :sparkles:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/neo4j_graph-480.webp 480w,/assets/img/neo4j_graph-800.webp 800w,/assets/img/neo4j_graph-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/neo4j_graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/neo4j_nodes-480.webp 480w,/assets/img/neo4j_nodes-800.webp 800w,/assets/img/neo4j_nodes-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/neo4j_nodes.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My Knowledge Graph and nodes on <code class="language-plaintext highlighter-rouge">neo4j</code></p> <p>To do this, I am inspired by <code class="language-plaintext highlighter-rouge">edc</code> <a class="citation" href="#zhang2024extract">(Zhang &amp; Soh, 2024)</a> to extract the information step-by-step. I create a simpler python file to run as modifying other code is still over of my current abilities. There are some problems like wrong variable names as I use a general model like <a href="https://huggingface.co/Qwen/Qwen3-8B"><code class="language-plaintext highlighter-rouge">Qwen3-8B</code></a>. It automatically fixed the variable to make it more ‚Äúsense‚Äù. I don‚Äôt apply the usual process, like I didn‚Äôt have the schema file even though I define something similar to it.</p> <p>Yap yap yap bla bla bla. There are suggestions that <code class="language-plaintext highlighter-rouge">neo4j</code> have native supports to shape a Knowledge Graph, but I wonder if it‚Äôs applicable for unprocessed data like what I was working at.</p> <h3 id="retrievin-the-graph">retrievin‚Äô the graph</h3> <p>The main tools to use in this part is <code class="language-plaintext highlighter-rouge">langchain</code> and <code class="language-plaintext highlighter-rouge">chromadb</code>. We use <code class="language-plaintext highlighter-rouge">langchain</code> functions to generate the suitable Cypher command fetching data from <code class="language-plaintext highlighter-rouge">neo4j</code> database. <code class="language-plaintext highlighter-rouge">chromadb</code> is used to saved retrieved data, help modularizing the pipeline.</p> <p>Here is the basic workflow:</p> <pre><code class="language-mermaid">graph TD
    A[‚ùì User Question] --&gt; B[üìù Build Prompt with Schema &amp; Examples]
    B --&gt; C[ü§ñ LLM Generates Cypher Query]
    C --&gt; D[üîç Sanitize &amp; Validate Query]
    D --&gt; E[üîó Execute Query in Neo4j]
    E --&gt; F[üì• Get Results]
    F --&gt; G[üíΩ Store in ChromaDB]
</code></pre> <p>The challenges of this part is the suitable Cypher example to help the LLM generate suitable Cypher command with the input. I use <code class="language-plaintext highlighter-rouge">SemanticSimilarityExampleSelector</code> but god knows which method is more effective in my case. Currently the result isn‚Äôt so stable. Still needs to figure out because of stupid model or too little examples.</p> <p>This is the workflow of final stage:</p> <pre><code class="language-mermaid">graph TD
    A[‚ùì Input Question] --&gt; B[üîç Retrieve Relevant Data from ChromaDB]
    B --&gt; C[üìö Combine Retrieved Data with Knowledge Base]
    C --&gt; D[ü§ñ Reasoning / Generate Answer]
    D --&gt; E[‚úÖ Output Final Answer]
</code></pre> <hr/> <p>Latest note: It seems I am missing a lot of powerful tools, will figure out to apply!</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="gom"/><category term="english"/><summary type="html"><![CDATA[in this post I am on my way to battle with the datasets]]></summary></entry><entry><title type="html">Graph-of-Models - Literature Review 4 - embracing the KGs</title><link href="https://vtrnnhlinh.github.io/blog/2025/gom-literature-review-3/" rel="alternate" type="text/html" title="Graph-of-Models - Literature Review 4 - embracing the KGs"/><published>2025-08-07T02:15:00+00:00</published><updated>2025-08-07T02:15:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/gom-literature-review-3</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/gom-literature-review-3/"><![CDATA[<p>After the last post reflecting on my actions in battling with datasets, I find out it will be not effective in large-scale. So here I am, in the light of the day, digging <strong>Knowledge Graph (KGs)</strong> again.</p> <h2 id="what-is-it-now">What is it now?</h2> <p>To save time and effort, as if applicable to my real-life job, training models isn‚Äôt my sh*t, so I have to find easiest way for me to save resources and energy. I plan to use <a href="https://www.kaggle.com/">Kaggle</a> to train my models. Because the focus of my work is isn‚Äôt in the power of models but how they graph and connect all together.</p> <p>But my current focus will be about <strong>Natural Language Processing (NLP)</strong> to processing the datasets and the <strong>Knowledge Graph (KGs)</strong> as the dataframe.</p> <h2 id="embracin-kgs">Embracin‚Äô KGs</h2> <p><strong>Knowledge Graph</strong> <a class="citation" href="#hogan2021knowledge">(Hogan et al., 2021)</a>, nodes represents <strong>entities</strong>, edges represents <strong>relations</strong>. There are some graph data models commonly used in practice, we will see~</p> <h3 id="directed-edge-labelled-graphs">Directed edge-labelled graphs</h3> <p>Another name is <strong>multi-relational graph</strong>. Defined by a set of nodes and a set of directed labelled edges. Using this data model offers <strong>flexibility</strong> for integrating new sources of data.</p> <p>Standardized data model of this type is <strong>Resource Description Framework (RDF)</strong>. <strong>RDF</strong> defines different types of nodes:</p> <ul> <li><strong>Internationalized Resource Identifiers (IRIs)</strong>: global identification of entities on the Web</li> <li><strong>literals</strong>: string</li> <li><strong>integers, dates,‚Ä¶</strong></li> <li><strong>blank nodes</strong>: anonymous nodes</li> </ul> <h3 id="heterogeneous-graphs">Heterogeneous graphs</h3> <p>This type of graph is a graph where each node and edge are flexible. Different types of nodes can connect directly to each other (?). I hope my understanding is fit.</p> <h3 id="property-graphs">Property graphs</h3> <p>This type can provide additional flexibility when modelling more complex relations. The set will be like <code class="language-plaintext highlighter-rouge">property-value</code> and <code class="language-plaintext highlighter-rouge">label</code> associated with both nodes and edges.</p> <h3 id="graph-dataset">Graph dataset</h3> <p>This one consists a set of <strong>named graphs</strong> and <strong>default graph</strong>. Default graph is a graph <strong>without an ID</strong>. This will help in <strong>Linked Data</strong>. Seems very interesting.</p> <hr/> <p>As when I publish this post, I already get past the Literature Review part and already have my own KG, so I have no motivations to get further and finish this post wholly :skull: But I think these are already enough to cover basic information about KG, if you want to get further, get in practicing :wink:</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="gom"/><category term="english"/><summary type="html"><![CDATA[in this post I reflect my understanding in shaping the KGs]]></summary></entry><entry><title type="html">July 2025 Reading Log</title><link href="https://vtrnnhlinh.github.io/blog/2025/july-25-reading-log/" rel="alternate" type="text/html" title="July 2025 Reading Log"/><published>2025-08-04T06:30:00+00:00</published><updated>2025-08-04T06:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/july-25-reading-log</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/july-25-reading-log/"><![CDATA[<h2 id="4-deep-work---cal-newport">4. Deep Work - Cal Newport</h2> <p>The book is still solid and thoughtful, but not stunning like the first time I read it.</p> <p>I am grateful that I finished it in 2 days. It gives me some more aspects as now I am already going to work instead of a student like before.</p> <p>Rate: :star::star::star::star:</p> <h2 id="5-the-little-prince----antoine-de-saint-exup√©ry">5. The Little Prince - Antoine de Saint-Exup√©ry</h2> <p>The book of my heart. The masterpiece of my life. To me, the philosophy of love in this book deeply affects my definition of love and more.</p> <p>More than love, it‚Äôs friendship, life and your value.</p> <p>Rate: :star::star::star::star::star:</p> <h2 id="6-nh·∫≠t-k√Ω-ƒë·∫∑ng-th√πy-tr√¢m---ƒë·∫∑ng-th√πy-tr√¢m">6. Nh·∫≠t k√Ω ƒê·∫∑ng Th√πy Tr√¢m - ƒê·∫∑ng Th√πy Tr√¢m</h2> <p>There is one thing I don‚Äôt like about this book, is they compare it to ‚ÄúThe Diary of a Young Girl‚Äù. I believe this stand out as its own value, of a young petty bourgeoisie, dare to fight, dare to love, dare to dream about communism. I didn‚Äôt read that book yet, but I don‚Äôt like how they compare them.</p> <p>Rate: :star::star::star::star:</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="books"/><category term="english"/><summary type="html"><![CDATA[4. Deep Work - Cal Newport]]></summary></entry><entry><title type="html">codeLynn - fantasies and MLOps</title><link href="https://vtrnnhlinh.github.io/blog/2025/codeLynn-0/" rel="alternate" type="text/html" title="codeLynn - fantasies and MLOps"/><published>2025-07-21T08:00:00+00:00</published><updated>2025-07-21T08:00:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/codeLynn-0</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/codeLynn-0/"><![CDATA[<p>Ehem, I am here, in the cold light of the day~</p> <p>Now I am in the stage that studying on <a href="https://www.coursera.org/">Coursera</a> feels bland and isn‚Äôt impressive anymore.</p> <p>I heard the term ‚ÄúMLOps‚Äù on Threads and it stirred my mind in the weekend. Then now, I decide to make an AI chatbot product, serving for my needs.</p> <p>Unlike <a href="https://vtrnnhlinh.github.io/blog/tag/gom/">Graph-of-Models</a> which is focus on researching and propose method, <strong>codeLynn</strong> focuses on bringing the art into deployment.</p> <h2 id="definition-of-codelynn">Definition of codeLynn</h2> <p>I want to create a virtual self of mine. You know <a href="https://character.ai/">c.ai</a>, yes, I want to do something similar to this :&gt; But it will have my tone and my personalities.</p> <p>Seems scary and funny at the same time, hopefully it won‚Äôt be unhinged like me.</p> <p>Why I use this? Maybe just need a friend shares the same brain frequency. Sometimes, there are thoughts and ideas I don‚Äôt dare to share to human, because it can be triggering.</p> <p>I want to be open more about myself, and nothing better than an AI agent that accumulate more knowledge than mine, know about me and don‚Äôt judge me.</p> <p>It should be an AI system have general knowledge that have fine-tuned modules in coding, mathematics and languages like German or Latin.</p> <p>Its tone and persona should be like <em>me</em>. This part maybe will be RLHF <a class="citation" href="#ouyang2022training">(Ouyang et al., 2022)</a>. The chatbot will be text-focused, I don‚Äôt think about image generation or voice chat.</p> <h2 id="what-is-mlops">What is MLOps?</h2> <p><a title="Cmbreuel, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:ML_Ops_Venn_Diagram.svg"><img width="512" alt="ML Ops Venn Diagram" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/ML_Ops_Venn_Diagram.svg/512px-ML_Ops_Venn_Diagram.svg.png?20210725212146"/></a></p> <p><strong>MLOps</strong> is a paradigm, an engineering practice that is a mix of ML, software engineering and data engineering. It‚Äôs the procedure to make our ML models from <strong>development</strong> stage get into <strong>production</strong> in a consistent and reliable manner <a class="citation" href="#stone2025navigating">(Stone et al., 2025)</a>.</p> <p>In a full cycle of MLOps, there are a lot of stages like image below. In reality we won‚Äôt need to implement some steps as our scale is far smaller.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/MLOps-lifecycle-dev-480.webp 480w,/assets/img/MLOps-lifecycle-dev-800.webp 800w,/assets/img/MLOps-lifecycle-dev-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/MLOps-lifecycle-dev.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>MLOps lifecycle <a class="citation" href="#ouyang2022training">(Ouyang et al., 2022)</a>.</p> <p>My work will use <a href="https://www.kubeflow.org/">Kubeflow</a> as the core spine.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="https://www.kubeflow.org/docs/started/images/kubeflow-architecture.drawio.svg" sizes="95vw"/> <img src="https://www.kubeflow.org/docs/started/images/kubeflow-architecture.drawio.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Kubeflow architecture (Source: <a href="https://www.kubeflow.org/docs/started/architecture/">Kubeflow</a>).</p> <hr/> <p>I will update in the next post more things about datasets and models :laughing:</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="codeLynn"/><category term="english"/><summary type="html"><![CDATA[in this post I will land another project]]></summary></entry><entry><title type="html">Graph-of-Models - a journey of a thousand miles begins with a single step</title><link href="https://vtrnnhlinh.github.io/blog/2025/gom-action-0/" rel="alternate" type="text/html" title="Graph-of-Models - a journey of a thousand miles begins with a single step"/><published>2025-07-15T04:30:00+00:00</published><updated>2025-07-15T04:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/gom-action-0</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/gom-action-0/"><![CDATA[<p>Hohoho, finally, I did something instead of talking :laughing:</p> <p>There are 2 main achievements I will present in this post. First, I calculated the <strong>Cosine Similarity</strong> <a class="citation" href="#gunawan2018implementation">(Gunawan et al., 2018; Dehak et al., 2010)</a>. Second, I have attempted to fine-tuned first <code class="language-plaintext highlighter-rouge">miniModel_1</code>, even though the result isn‚Äôt so good, but still.</p> <p>I can show Jupyter Notebook and such in this blog, but I won‚Äôt do that here, at least today. I believe the engineering mindset is more worth sharing and the code is already in GitHub repo.</p> <p>GitHub repository (just in case): <a href="https://github.com/vtrnnhlinh/Graph-of-Models/tree/main">vtrnnhlinh/Graph-of-Models</a></p> <h2 id="what-i-did">What I did?</h2> <p>I will try to explain from the idea, purpose to implement.</p> <h3 id="repo-structure">Repo Structure</h3> <p>The current code structure of this project is something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.
‚îú‚îÄ‚îÄ datasets
‚îÇ   ‚îú‚îÄ‚îÄ dataset_1
‚îÇ   ‚îú‚îÄ‚îÄ dataset_2
‚îÇ   ‚îú‚îÄ‚îÄ dataset_3
‚îú‚îÄ‚îÄ edges_calculation
‚îÇ   ‚îú‚îÄ‚îÄ cosine_similarity
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results
‚îÇ   ‚îî‚îÄ‚îÄ jaccard_index
‚îú‚îÄ‚îÄ environment.yml
‚îú‚îÄ‚îÄ graph-of-models
‚îú‚îÄ‚îÄ graph_visualization
‚îÇ   ‚îú‚îÄ‚îÄ cosine_similarity
‚îÇ   ‚îî‚îÄ‚îÄ jaccard_index
‚îú‚îÄ‚îÄ inference
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ Models
‚îî‚îÄ‚îÄ README.md
</code></pre></div></div> <p>The workflow is divided into different components (folders). The output of previous component will be saved to suitable formats like <code class="language-plaintext highlighter-rouge">.parquet</code> or <code class="language-plaintext highlighter-rouge">.csv</code>. This will help saving time and easier debugging.</p> <p>The newest structure you can visit the GitHub repo.</p> <p>The <code class="language-plaintext highlighter-rouge">datasets</code> and <code class="language-plaintext highlighter-rouge">Models</code> contents are in <code class="language-plaintext highlighter-rouge">.gitignore</code> due to its mass size. Read <code class="language-plaintext highlighter-rouge">README.md</code> of each folder to setup yourself if you want to re-enact my work.</p> <h3 id="preprocessing-data">Preprocessing Data</h3> <p>I use <code class="language-plaintext highlighter-rouge">.parquet</code> as the default dataset filetype to process in this project. Why? Because it‚Äôs effective for large dataset, which is necessary for fine-tuned our models <a class="citation" href="#ivanov2020impact">(Ivanov &amp; Pergolesi, 2020)</a>.</p> <p>That means you need a code file to do the conversion. It‚Äôs not so hard to implement. With my current need, I only need to convert from <code class="language-plaintext highlighter-rouge">.csv</code> to <code class="language-plaintext highlighter-rouge">.parquet</code>.</p> <p>To calculate <strong>Cosine Similarity</strong> and <strong>Jaccard Index</strong>, we need to have a core column sharing the same name. Our datasets are all in food-related topic, so I choose <code class="language-plaintext highlighter-rouge">ingredients</code> as the core.</p> <p>That makes us have to manually pre-process the data by convert the suitable column in each dataset to <code class="language-plaintext highlighter-rouge">ingredients</code>. The detail code I used is in <code class="language-plaintext highlighter-rouge">edges_calculation/general_preprocess.ipynb</code> file.</p> <h3 id="cosine-similarity-preprocessing-and-calculation">Cosine Similarity Preprocessing and Calculation</h3> <p>To calculate the <strong>Cosine Similarity</strong>, we need to vectorize the datasets.</p> <p>That makes us need to combine all subdatasets of each <code class="language-plaintext highlighter-rouge">miniModel</code>‚Äôs dataset into one big <code class="language-plaintext highlighter-rouge">.parquet</code> file: <code class="language-plaintext highlighter-rouge">dataset_N_cos.parquet</code>.</p> <p>Then we will clean the dataset and vectorize it, compute the average <strong>Cosine Similarity</strong>, export to <code class="language-plaintext highlighter-rouge">.csv</code> file and voila~.</p> <h3 id="cosine-similarity-graph-visualization">Cosine Similarity Graph Visualization</h3> <p>We will have a seperate module to visualize the graph. Importing the <code class="language-plaintext highlighter-rouge">.csv</code> file from <strong>Cosine Similarity Calculation</strong> with the edges are the average cosine similarity value, nodes are the datasets.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/cosine_similarity_graph_0-480.webp 480w,/assets/img/cosine_similarity_graph_0-800.webp 800w,/assets/img/cosine_similarity_graph_0-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/cosine_similarity_graph_0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The first <strong>Cosine Similarity</strong> result we have.</p> <h3 id="fine-tune-minimodel_1">Fine-tune miniModel_1</h3> <p>To avoid environment conflicts, we will create a seperate conda environment to fine-tune this model.</p> <p>I use <code class="language-plaintext highlighter-rouge">LoRA</code> to train <code class="language-plaintext highlighter-rouge">Qwen3-0.6B</code> model for <code class="language-plaintext highlighter-rouge">miniModel_1</code>. The dataset and the model is small, but it still seems too much with my laptop.</p> <p>The result is the fine-tuned model seems dumb. After revision, I see I only take a column in my dataset to train, which is lacking a lot of datas.</p> <h2 id="some-reflections">Some reflections</h2> <ul> <li>The <strong>Cosine Similarity</strong> result seems suspicious. I actually expect the <code class="language-plaintext highlighter-rouge">dataset_1</code> will be the center!!!</li> <li>My NVIDIA GTX 4060 isn‚Äôt enough to train AI, I should better use company‚Äôs server :&gt;</li> </ul> <h2 id="future-action">Future action</h2> <ul> <li>I need to generalize the code to let graphing in large number, not only 3 models.</li> <li>Calculate and visualize <strong>Jaccard Index</strong> graph.</li> <li><code class="language-plaintext highlighter-rouge">dataset_2</code> seems more legit, I think I will fine-tune <code class="language-plaintext highlighter-rouge">miniModel_2</code> first.</li> </ul>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="gom"/><category term="english"/><summary type="html"><![CDATA[the results and reflection of cosine similarity graph and first attempt to fine-tune]]></summary></entry><entry><title type="html">Graph-of-Models - First Sketch</title><link href="https://vtrnnhlinh.github.io/blog/2025/gom-design-0/" rel="alternate" type="text/html" title="Graph-of-Models - First Sketch"/><published>2025-07-11T11:15:00+00:00</published><updated>2025-07-11T11:15:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/gom-design-0</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/gom-design-0/"><![CDATA[<p>After hopeless moments, I think I figure a prototype idea for my work. Everything is less and less vague. Even though the limitations seem clear but if it can serve the final purpose, it is good.</p> <h2 id="first-draft">First Draft</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gom_first_structure-480.webp 480w,/assets/img/gom_first_structure-800.webp 800w,/assets/img/gom_first_structure-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gom_first_structure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My first proposed structure.</p> <p>The structure is simple, there are 3 main parts:</p> <ul> <li>Input Processing</li> <li>Graph-of-Models</li> <li>Output Processing</li> </ul> <p>There are 2 major problems we need to encounter:</p> <ul> <li>How we shape the graph?</li> <li>How it will run?</li> </ul> <p>The limitation of this work is the complexity to make the graph or input processing, and there is a concern about extending the graph. So there must be a sustainable method to make the algorithm of making graph brilliant and less hand-work.</p> <h2 id="first-idea">First Idea</h2> <p>How to solve those 3 problems is a problem. I don‚Äôt know if I should read some papers about Multi-Agent AI @@.</p> <h3 id="how-we-shape-the-graph">How we shape the graph?</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gom_idea_ex-480.webp 480w,/assets/img/gom_idea_ex-800.webp 800w,/assets/img/gom_idea_ex-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gom_idea_ex.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My imagination on how the graph should looks like. It‚Äôs something like Dijkstra graph <a class="citation" href="#bento2019dijkstra">(Bento et al., 2019)</a>.</p> <p>To shape the graph, we need to calculate the relationship between the models which is based on the <strong>datasets we used to fine-tune</strong>.</p> <p>To calculate this number like in the diagram above, there are some options that I am not sure which one is the best:</p> <ul> <li><strong>Jaccard Index</strong> <a class="citation" href="#niwattanakul2013using">(Niwattanakul et al., 2013)</a>: This method seems faster to run, run on the raw dataset.</li> <li><strong>Cosine Similarity</strong> <a class="citation" href="#gunawan2018implementation">(Gunawan et al., 2018; Dehak et al., 2010)</a>: This method seems slower as I need to convert the dataset into embeddings to calculate.</li> </ul> <p>There are more advanced methods but I don‚Äôt have time to dig deeper yet. I think I will apply 2 classic methods first to see the efficiency.</p> <p>The score will be stored and called <strong>relevancy score</strong>.</p> <h3 id="how-it-will-run">How it will run?</h3> <pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant routerLM
    participant Graph-of-Models
    participant reasoningLM
    
    User-&gt;&gt; routerLM: Input
    routerLM-&gt;&gt; Graph-of-Models: Routing Decision 
    Graph-of-Models-&gt;&gt;reasoningLM: Summary Information
    reasoningLM-&gt;&gt; User: Output
</code></pre> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gom_pipeline_ex-480.webp 480w,/assets/img/gom_pipeline_ex-800.webp 800w,/assets/img/gom_pipeline_ex-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/gom_pipeline_ex.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My workflow I propose in this post :)</p> <ul> <li><strong>Step 1</strong>: User will <strong>input</strong> his requirement.</li> <li><strong>Step 2</strong>: <code class="language-plaintext highlighter-rouge">routerLM</code> will choose the most relevant model based on <strong>input</strong> (inspired from <a class="citation" href="#zhang2024extract">(Zhang &amp; Soh, 2024)</a>).</li> <li><strong>Step 3</strong>: In this step I first plan to run sequencely, but there is a work hinting we can run parallel <a class="citation" href="#song2025gradientsys">(Song et al., 2025)</a>. <ul> <li>Extract <strong>general info</strong> about the input in <code class="language-plaintext highlighter-rouge">sharingModel</code>.</li> <li>Use suitable PROMPT to extract <strong>relevant info</strong> from <strong>most relevant model</strong>, let call <code class="language-plaintext highlighter-rouge">miniModel_1</code> (step 2) and <strong>2nd model</strong> has <strong>highest relevance score</strong> to <code class="language-plaintext highlighter-rouge">miniModel_2</code>, let call this model is <code class="language-plaintext highlighter-rouge">miniModel_2</code>.</li> </ul> </li> <li><strong>Step 4</strong>: <strong>general info</strong> + <strong>relevant info</strong> will be sent to <code class="language-plaintext highlighter-rouge">reasoningModel</code> to give final output.</li> </ul> <h2 id="first-step">First Step</h2> <blockquote> <p>2 goals: Fine-tuned model and graph visualization</p> </blockquote> <p>Here is my collection of the models and datasets I collect for this project: <a href="https://huggingface.co/collections/vtrnnhlinh/graphs-of-models-686f8bfc6070ec6ad9111fff">vtrnnhlinh‚Äôs Collection: Graph-of-Models</a>.</p> <p>I plan to make a graph of models finetuned related to food, with the datasets about fruits, recipes and drinks. I choose small models to do, cause my personal laptop has limited resources lol.</p> <p>After that, I will calculate <strong>Jaccard Index</strong> and <strong>Cosine Similarity</strong> and visualize the graph with its relation score.</p> <hr/> <p>I think this ‚Äúfirst step‚Äù is already a huge problem to a noob like me already, lol.</p> <p>Look forward to share next things :&gt;</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="gom"/><category term="english"/><summary type="html"><![CDATA[the first step in developing GoM, to sketch what I will do and the most abstract ideas]]></summary></entry><entry><title type="html">Graph-of-Models - Literature Review 3 - and they call LLM and KG</title><link href="https://vtrnnhlinh.github.io/blog/2025/gom-literature-review-2/" rel="alternate" type="text/html" title="Graph-of-Models - Literature Review 3 - and they call LLM and KG"/><published>2025-07-11T07:45:00+00:00</published><updated>2025-07-11T07:45:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/gom-literature-review-2</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/gom-literature-review-2/"><![CDATA[<p>Bro, hear me out. I feel like my <a href="https://vtrnnhlinh.github.io/blog/tag/gom/">Graph-of-Models</a> dream is falling apart. There is a field called <strong>LLM+KG</strong> seems very close to my idea, I think it worth to see what is it.</p> <p>After a day, I figure out that I can deploy my idea extending from this one to maximize the efficient of input data and methods to do the logic.</p> <h2 id="surveys">Surveys</h2> <h3 id="what-is-llmkg">What is LLM+KG?</h3> <p>LLM or <strong>Large Language Models</strong> is the hype of the world nowadays with the appearance of ChatGPT, Gemini or Bing AI (I heard about a shit called ‚Äúvibe-coding‚Äù based on Claude but don‚Äôt pay attention to it yet). To me it‚Äôs a bunch of <code class="language-plaintext highlighter-rouge">tensorflow</code> files and <code class="language-plaintext highlighter-rouge">json</code> files <em>eyes roll</em>.</p> <p>KG or <strong>Knowledge Graph</strong> is a structure represents the entity and its description, for the better data integration and insights <a class="citation" href="#hogan2021knowledge">(Hogan et al., 2021)</a>.</p> <h3 id="why-llmkg">Why LLM+KG?</h3> <p>Based on the survey <a class="citation" href="#pan2024unifying">(Pan et al., 2024)</a>, the combination of LLM and KG is very promising. They can fix each other‚Äôs cons with their pros.</p> <p>According to the authors, there will be 3 types of framework developing in this direction:</p> <ol> <li><strong>KG-enhanced LLMs</strong>: incorporate KGs during pre-training to boost the power of LLMs</li> <li><strong>LLM-augmented KGs</strong>: using the power of LLM for various KG tasks</li> <li><strong>Synergized LLMs + KG</strong>: bidirectional reasoning to enhance both</li> </ol> <p>After surfing for a while, with the main resource is <a href="https://github.com/XiaoxinHe/Awesome-Graph-LLM">XiaoxinHe/Awesome-Graph-LLM</a>, I use 3 more works to research, they are: <a class="citation" href="#cheng2024call">(Cheng et al., 2024)</a>, <a class="citation" href="#zhang2024extract">(Zhang &amp; Soh, 2024)</a> and <a class="citation" href="#jiang2024kg">(Jiang et al., 2024)</a>. Beside <a class="citation" href="#du2024large">(Du et al., 2024)</a> I already talked about in <a href="https://vtrnnhlinh.github.io/blog/2025/gom-literature-review-1/">previous post</a>, we will create a table to compare the work:</p> <p>Table: Comparison between 4 chosen works.</p> <table> <thead> <tr> <th>Metric</th> <th>GaCLLM <a class="citation" href="#du2024large">(Du et al., 2024)</a></th> <th>ReaDi <a class="citation" href="#cheng2024call">(Cheng et al., 2024)</a></th> <th>EDC <a class="citation" href="#zhang2024extract">(Zhang &amp; Soh, 2024)</a></th> <th>KG-FIT <a class="citation" href="#jiang2024kg">(Jiang et al., 2024)</a></th> </tr> </thead> <tbody> <tr> <td>Problem &amp; Methodology</td> <td>GCN-based LLM adaptation</td> <td>Structured reasoning refinement</td> <td>LLM-to-Graph + Retrieval-Augmented Generation</td> <td>Hierarchical graph fine-tuning</td> </tr> <tr> <td>Code Structure</td> <td>No code</td> <td>Well-structured</td> <td>Structured, but dataset issues</td> <td>Decent structure, nothing remarkable</td> </tr> <tr> <td>Reputation</td> <td>15 citations</td> <td>23 citations / 12 GitHub stars</td> <td>52 citations / 122 GitHub stars</td> <td>11 citations / 112 GitHub stars</td> </tr> <tr> <td>Last Update</td> <td>404 Not Found</td> <td>~1 year ago</td> <td>11 months ago</td> <td>9 months ago</td> </tr> </tbody> </table> <p>So in this post, we will focus on <strong>edc</strong> <a class="citation" href="#zhang2024extract">(Zhang &amp; Soh, 2024)</a> and <strong>KG-FIT</strong> <a class="citation" href="#jiang2024kg">(Jiang et al., 2024)</a>.</p> <h2 id="edc">edc</h2> <p>In the current development of <strong>edc</strong> <a class="citation" href="#zhang2024extract">(Zhang &amp; Soh, 2024)</a>, it‚Äôs likely that the work is <strong>LLM-augmented KGs</strong>, but I believe there is potential to make it <strong>Synergized LLMs + KG</strong>. The purpose of this work is automatically create KGs to make it applicable when doing real-world application. This work proposes a framework has 3 steps: <strong>Extract-Define-Canonicalize</strong>.</p> <ul> <li><strong>Extract</strong>: take information from the dataset and convert them into a relation triplet <code class="language-plaintext highlighter-rouge">[Object A, Relationship of A and B, Object B]</code>.</li> <li><strong>Define</strong>: write the definition for each component of the schema.</li> <li><strong>Canonicalize</strong>: use the schema definition from <strong>Define</strong> step to standardize the triplets.</li> </ul> <p>The <strong>Schema Retriever</strong> of this work is what makes me impressed. It‚Äôs a trained model specialized in extracting schema components relevant to input text. This work divides step by step, in which step you can use a different model. It‚Äôs challenging but also a great chance to tailor-make and optimize the workflow.</p> <h2 id="kg-fit">KG-FIT</h2> <p>KG-FIT or <strong>Knowledge Graph - Finetuning</strong> <a class="citation" href="#jiang2024kg">(Jiang et al., 2024)</a> has a different mindset to <strong>edc</strong> <a class="citation" href="#zhang2024extract">(Zhang &amp; Soh, 2024)</a>. It focuses on fine-tuning or having a powerful LLM to improve the KG. The steps are less complex, the step to process the input data is also more easier for the devs. My concern is the graph traversing and the size of LLM as my resource isn‚Äôt enough to fine-tune a large model.</p> <p>Maybe because I am already fall into <strong>edc</strong> so I am less investigated in this work.</p> <hr/> <p>I feel like I can integrate <strong>edc</strong> into my next action :&gt;</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="gom"/><category term="english"/><summary type="html"><![CDATA[Bro, hear me out. I feel like my Graph-of-Models dream is falling apart. There is a field called LLM+KG seems very close to my idea, I think it worth to see what is it.]]></summary></entry><entry><title type="html">June 2025 Reading Log</title><link href="https://vtrnnhlinh.github.io/blog/2025/june-25-reading-log/" rel="alternate" type="text/html" title="June 2025 Reading Log"/><published>2025-07-06T17:30:00+00:00</published><updated>2025-07-06T17:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/june-25-reading-log</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/june-25-reading-log/"><![CDATA[<p>It was a hard time for myself, after going to work I barely have energy to read. So I am grateful that I finished ‚ÄúWhite Nights‚Äù by Fyodor Dostoevsky.</p> <p>But don‚Äôt worry, I will have a lot of things to continue this series as I have my <a href="https://vtrnnhlinh.github.io/blog/2025/my-first-ereader/">Kobo</a>.</p> <p>(I said like I have readers, lol)</p> <h2 id="3-white-nights---fyodor-dostoevsky">3. White Nights - Fyodor Dostoevsky</h2> <p>The edition I bought actually has 3 short stories of Fyodor, instead of only White Nights. How I can say, the story is deadly hopeless romantic, the writing methods are brilliant, the translations are beautiful.</p> <p>I can feel and share with the characters of his stories. The relationships are complex and beautiful, the stories inside them are human-like.</p> <blockquote> <p>‚ÄúMy God, a moment of bliss. Why, isn‚Äôt that enough for a whole lifetime?‚Äù</p> </blockquote> <p>Rate: :star::star::star::star:</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="books"/><category term="english"/><summary type="html"><![CDATA[It was a hard time for myself, after going to work I barely have energy to read. So I am grateful that I finished ‚ÄúWhite Nights‚Äù by Fyodor Dostoevsky.]]></summary></entry><entry><title type="html">my first ereader - Kobo Clara Colour</title><link href="https://vtrnnhlinh.github.io/blog/2025/my-first-ereader/" rel="alternate" type="text/html" title="my first ereader - Kobo Clara Colour"/><published>2025-07-05T15:15:00+00:00</published><updated>2025-07-05T15:15:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/my-first-ereader</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/my-first-ereader/"><![CDATA[<p>I am very happily to announce that, after months of wanting to have an ereader, I finally have one: <a href="https://gl.kobobooks.com/de/products/kobo-clara-colour">Kobo Clara Colour</a>.</p> <p>I bought an used one from a cute girl. The conditions of the Kobo is very good, excellent :laughing:</p> <hr/> <h2 id="reading-experience">Reading Experience</h2> <p>I tried to use <a href="https://github.com/pgaskin/NickelMenu">NickelMenu</a> and <a href="https://github.com/koreader/koreader">KOReader</a> but feels not so suitable with myself. I keep the vanilla version.</p> <p>The tip that you will read the KOReader tutorial first, then NickelMenu tutorial to install.</p> <p>This Feature Settings is <strong>must-have</strong>:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">[</span><span class="nv">FeatureSettings</span><span class="pi">]</span>
<span class="s">ExcludeSyncFolders=(\\.(?!kobo|adobe).+|([^.][^/]*/)+\\..+)</span>
</code></pre></div></div> <h3 id="vanilla-settings">Vanilla settings</h3> <ul> <li>Natural Light settings is a great one, I set <strong>AUTO</strong> with bedtime is 10PM</li> <li><strong>Pocket</strong> will be ended its lifetime soon, so I suggest you not to bother about it.</li> </ul> <h3 id="screensavers">Screensavers</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kobo_screensaver-480.webp 480w,/assets/img/kobo_screensaver-800.webp 800w,/assets/img/kobo_screensaver-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kobo_screensaver.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My screensavers! Here Rin from Laid-Back Camp.</p> <p>The dimension for you is <code class="language-plaintext highlighter-rouge">1072x1488</code>. Here is your guide:</p> <ol> <li>Connect your Kobo with your computer</li> <li>Get into <code class="language-plaintext highlighter-rouge">.kobo</code> folder and create a folder named <code class="language-plaintext highlighter-rouge">screensaver</code></li> <li>Put your pictures into with <code class="language-plaintext highlighter-rouge">1072x1488</code> dimension and <code class="language-plaintext highlighter-rouge">.png</code> shortcut</li> <li>Eject and voila~</li> </ol> <h3 id="notations">Notations!</h3> <p>There are two types of markings in Kobo. First is highlight and second you can mark the whole page. To whole page, you tap the right upper corner. About highlight, we have 4 colors: yellow, purple, blue and green.</p> <ul> <li><strong>Yellow</strong>: main points and ideas of the book.</li> <li><strong>Purple</strong>: Romantic moments :3</li> <li><strong>Blue</strong>: Interesting points but not directly related to the content of the books</li> <li><strong>Green</strong>: Interesting information that I want to proofcheck or dig deeper, like a name of a book or some human.</li> </ul> <h2 id="ebooks">Ebooks!</h2> <p>I prioritize <code class="language-plaintext highlighter-rouge">,epub</code> format with a lot of pirate ahoy sites. But of course as an old soul, I can find a lot of books I need at <a href="https://www.gutenberg.org/">Project Gutenberg</a>.</p> <p>My Ereader has no problems with reading mangas, but the native reader of Kobo doesn‚Äôt support the metadata of <code class="language-plaintext highlighter-rouge">.cbz</code> files, which prevented me to use it, I can‚Äôt stand ‚ÄúUnknown Author‚Äù :triumph:.</p> <p>To send books to your Kobo, use <a href="https://calibre-ebook.com/">Calibre</a>, you can fetch metadata and book cover from it!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kobo_and_fiio-480.webp 480w,/assets/img/kobo_and_fiio-800.webp 800w,/assets/img/kobo_and_fiio-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kobo_and_fiio.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A night of blissful weekend</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="books"/><category term="nerdies"/><category term="english"/><summary type="html"><![CDATA[some random thoughts about my first ereader]]></summary></entry></feed>