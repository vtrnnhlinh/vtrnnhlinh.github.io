<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://vtrnnhlinh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vtrnnhlinh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-06T17:07:51+00:00</updated><id>https://vtrnnhlinh.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple logbook of Linh </subtitle><entry><title type="html">my first ereader - Kobo Clara Colour</title><link href="https://vtrnnhlinh.github.io/blog/2025/my-first-ereader/" rel="alternate" type="text/html" title="my first ereader - Kobo Clara Colour"/><published>2025-07-05T15:15:00+00:00</published><updated>2025-07-05T15:15:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/my-first-ereader</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/my-first-ereader/"><![CDATA[<p>I am very happily to announce that, after months of wanting to have an ereader, I finally have one: <a href="https://gl.kobobooks.com/de/products/kobo-clara-colour">Kobo Clara Colour</a>.</p> <p>I bought an used one from a cute girl. The conditions of the Kobo is very good, excellent :laughing:</p> <hr/> <h2 id="reading-experience">Reading Experience</h2> <p>I tried to use <a href="https://github.com/pgaskin/NickelMenu">NickelMenu</a> and <a href="https://github.com/koreader/koreader">KOReader</a> but feels not so suitable with myself. I keep the vanilla version.</p> <p>The tip that you will read the KOReader tutorial first, then NickelMenu tutorial to install.</p> <p>This Feature Settings is <strong>must-have</strong>:</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="pi">[</span><span class="nv">FeatureSettings</span><span class="pi">]</span>
<span class="s">ExcludeSyncFolders=(\\.(?!kobo|adobe).+|([^.][^/]*/)+\\..+)</span>
</code></pre></div></div> <h3 id="vanilla-settings">Vanilla settings</h3> <ul> <li>Natural Light settings is a great one, I set <strong>AUTO</strong> with bedtime is 10PM</li> <li><strong>Pocket</strong> will be ended its lifetime soon, so I suggest you not to bother about it.</li> </ul> <h3 id="screensavers">Screensavers</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kobo_screensaver-480.webp 480w,/assets/img/kobo_screensaver-800.webp 800w,/assets/img/kobo_screensaver-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kobo_screensaver.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My screensavers! Here Rin from Laid-Back Camp.</p> <p>The dimension for you is <code class="language-plaintext highlighter-rouge">1072x1488</code>. Here is your guide:</p> <ol> <li>Connect your Kobo with your computer</li> <li>Get into <code class="language-plaintext highlighter-rouge">.kobo</code> folder and create a folder named <code class="language-plaintext highlighter-rouge">screensaver</code></li> <li>Put your pictures into with <code class="language-plaintext highlighter-rouge">1072x1488</code> dimension and <code class="language-plaintext highlighter-rouge">.png</code> shortcut</li> <li>Eject and voila~</li> </ol> <h3 id="notations">Notations!</h3> <p>There are two types of markings in Kobo. First is highlight and second you can mark the whole page. To whole page, you tap the right upper corner. About highlight, we have 4 colors: yellow, purple, blue and green.</p> <ul> <li><strong>Yellow</strong>: main points and ideas of the book.</li> <li><strong>Purple</strong>: Romantic moments :3</li> <li><strong>Blue</strong>: Interesting points but not directly related to the content of the books</li> <li><strong>Green</strong>: Interesting information that I want to proofcheck or dig deeper, like a name of a book or some human.</li> </ul> <h2 id="ebooks">Ebooks!</h2> <p>I prioritize <code class="language-plaintext highlighter-rouge">,epub</code> format with a lot of pirate ahoy sites. But of course as an old soul, I can find a lot of books I need at <a href="https://www.gutenberg.org/">Project Gutenberg</a>.</p> <p>My Ereader has no problems with reading mangas, but the native reader of Kobo doesn‚Äôt support the metadata of <code class="language-plaintext highlighter-rouge">.cbz</code> files, which prevented me to use it, I can‚Äôt stand ‚ÄúUnknown Author‚Äù :triumph:.</p> <p>To send books to your Kobo, use <a href="https://calibre-ebook.com/">Calibre</a>, you can fetch metadata and book cover from it!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kobo_and_fiio-480.webp 480w,/assets/img/kobo_and_fiio-800.webp 800w,/assets/img/kobo_and_fiio-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/kobo_and_fiio.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>A night of blissful weekend</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="books"/><category term="nerdies"/><category term="english"/><summary type="html"><![CDATA[some random thoughts about my first ereader]]></summary></entry><entry><title type="html">Graphs-of-Heads - Literature Review 2 - GaCLLM</title><link href="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-1/" rel="alternate" type="text/html" title="Graphs-of-Heads - Literature Review 2 - GaCLLM"/><published>2025-07-01T06:30:00+00:00</published><updated>2025-07-01T06:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-1</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-1/"><![CDATA[<p>In this second part, we will focus mainly on GaCLLM <a class="citation" href="#du2024large">(Du et al., 2024)</a> and my new take on the system I imagined after digging deeper the work.</p> <p>Read the previous part here: <a href="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0/">Graphs-of-Heads - The First Literature Review</a>.</p> <hr/> <h2 id="gacllm">GaCLLM</h2> <p>In this work, authors proposed 4 main modules:</p> <ul> <li>Conduct SFT (Supervised Fine-tuning) for the LLM</li> <li>Propose LLM-based graph-aware convolutional inference</li> <li>Align and fuse generated description into graph-based embeddings</li> <li>Introduce the objective function and learning process</li> </ul> <h3 id="supervised-fine-tuning">Supervised Fine-tuning</h3> <p>In first stage, they fine-tune LLM with domain-specific data. This involves training of LLM with descriptions from matched user-item pairs. They will construct the prompt template for items. The optimization process involves minimizing the negative log-likelihood loss for these templates.</p> <p>They adopt LoRa <a class="citation" href="#hu2022lora">(Hu et al., 2022)</a> for parameter-efficient learning to improve time and computational efficiency.</p> <h3 id="llm-based-convolutional-inference-in-graph">LLM-based Convolutional Inference in Graph</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/graphs_gacllm-480.webp 480w,/assets/img/graphs_gacllm-800.webp 800w,/assets/img/graphs_gacllm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/graphs_gacllm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My understanding about proposed graphs.</p> <p>The graph will be $G = (V, E)$ with $V$ are the users and items. The edges $E$ are constructed by the interactions between users and items. Each node has a textual description.</p> <p>We have prompt template to rewrite/generate the description of users and items. Let call it $PROMPT_{user}$ and $PROMPT_{item}$.</p> <p>To improve the quality of descriptions <strong>for users</strong>, we will rewrite user‚Äôs raw description $T_u$ by,</p> \[T'_u = LLM(PROMPT_{user}(T_u, \{T_i:(u,i) \in E\})),\] <p>Similarity, to improve the quality of descriptions <strong>for items</strong>, we will rewrite user‚Äôs raw description $T_i$ by,</p> \[T'_i = LLM(PROMPT_{item}(T_i, \{T_u:(u,i) \in E\})),\] <p>To do this, they enhance the layers by running LLM in a loop with first layer is the raw input. LLM now is taking the role as an ‚Äúaggregator‚Äù in the graph convolutional processing. After $L$ interations, we obtain different layers of user and item descriptions for both users and items.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/token_llm-480.webp 480w,/assets/img/token_llm-800.webp 800w,/assets/img/token_llm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/token_llm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Different ways for token usage in LLMs <a class="citation" href="#du2024large">(Du et al., 2024)</a>.</p> <p>They claim the proposed LLM-based convolutional strategy has 2 main strengths:</p> <ul> <li>High quality descriptions of users and items in a least-to-most manner</li> <li>Effectively capture graph-related info with limited context of length <ul> <li>Break down the task of description enhancement into multiple steps</li> <li>Each step (layer) only integrates the descriptions of one-decrease in the number of tokens needed</li> </ul> </li> <li>Efficiently alleviate the redundancy of describing the graph for target nodes</li> </ul> <h3 id="aligning-gcn-based-embeddings-for-recommendation">Aligning GCN-based Embeddings for Recommendation</h3> <p>To bridge text info and structural info, they align the descriptions of users and items with their <strong>embeddings</strong> in a unified manner.</p> <p>First layer represents the initial step for both users‚Äô and items‚Äô representation. Each user and item will associated with the embedding from a specific ID. Then they encode them into the constant text-based embedding by encoder.</p> <p>There will be an <em>objective function</em> to measure the matching scores between users and items for final predictions.</p> <h2 id="some-of-thoughts">Some of Thoughts</h2> <p>I am still confused and not clarify my idea yet. Do you have any thoughts? But I guess re-enact the work of GaCLLM will be interesting.</p> <p>This afternoon I will have a meeting with my team to discuss about this, I will update the progress in upcoming posts.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="goh"/><category term="english"/><summary type="html"><![CDATA[the second literature review of my series writting about my work I called Graph-of-Heads]]></summary></entry><entry><title type="html">Graphs-of-Heads - Literature Review 1 - Transformer and MoH</title><link href="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0/" rel="alternate" type="text/html" title="Graphs-of-Heads - Literature Review 1 - Transformer and MoH"/><published>2025-06-27T11:30:00+00:00</published><updated>2025-06-27T11:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/goh-literature-review-0/"><![CDATA[<p>At work I am assigned to learn about Mixture-of-Experts (MoE) but my mentor wants another specific, tailor-made approach to our problem.</p> <p>I name it <strong>Graphs-of-Heads</strong> (GoH).</p> <p>I have a vague idea in my mind but I think I need a <strong>Literature Review</strong> to make my idea becomes realistic as most as possible.</p> <p>However, I don‚Äôt follow the ordinary literature review in academic research, as I am working in industry. I will try to adapt and build the code along the way.</p> <p>This is a series of posts about this project, and this first post is about 2 first literature review of mine.</p> <h2 id="plan">Plan</h2> <p>In my first plan, I want to use MoH structure <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> as the base development. Then I will apply the experts network based on <a class="citation" href="#du2024large">(Du et al., 2024)</a>.</p> <p>The Python project structure I apply is the <a href="https://github.com/navdeep-G/samplemod">navdeep-G/samplemod</a>.</p> <p>The training framework will be Megatron-LM <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> with continual, meta and multi-task learning. Federated Learning will be developed when the application is deployed for many users.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/plan_llm-480.webp 480w,/assets/img/plan_llm-800.webp 800w,/assets/img/plan_llm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/plan_llm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>My imagined structure.</p> <h2 id="attention-is-all-you-need">Attention is all you need!!</h2> <p>We will start from <code class="language-plaintext highlighter-rouge">Transformer</code> Structure <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>I am pretty bad at Python, so I will learn and reference a lot of repositories, both in how they structure the file and the coding methodology.</p> <p>I reference <a href="https://github.com/SCCSMARTCODE/attention-is-all-you-need-from-scratch">SCCSMARTCODE/attention-is-all-you-need-from-scratch</a> and <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">jadore801120/attention-is-all-you-need-pytorch</a> to re-made the <code class="language-plaintext highlighter-rouge">transformer</code> structure.</p> <p><code class="language-plaintext highlighter-rouge">Transformer</code> is an architecture rely entirely on an attention mechanism to draw <strong>global dependencies</strong> between I/O. <code class="language-plaintext highlighter-rouge">Transformer</code> allows significantly more parallelization.</p> <p>I believe my wanted structure is far from this work, but a thousand miles start from a step.</p> <h3 id="encoder--decoder">Encoder &amp; Decoder</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/attention_architecture-480.webp 480w,/assets/img/attention_architecture-800.webp 800w,/assets/img/attention_architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/attention_architecture.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><code class="language-plaintext highlighter-rouge">Transformer</code> architecture. Source: <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>With the illustration, you can see the <code class="language-plaintext highlighter-rouge">Transformer</code> has 2 main modules are <strong>Encoder</strong> and <strong>Decoder</strong>. There is also something worth noticing is <strong>Positional Encoding</strong>.</p> <p><strong>Encoder</strong> and <strong>Decoder</strong> has a stack of $N=6$ layer. Encoder‚Äôs layer has 2 sub-layers:</p> <ul> <li>Multi-Head Attention</li> <li>Feed Forward</li> </ul> <p>While Decoder‚Äôs layer has 3 sub-layers:</p> <ul> <li>Multi-Head Attention</li> <li>Feed Forward</li> <li>Masked Multi-Head Attention</li> </ul> <p>We employ residual connection around each of sub-layers, followed by layer normalization. The dimension is $d_{model} = 512$.</p> <h3 id="attention">Attention</h3> <p>To me, this is the heart of this work.</p> <p>Attention function is mapping a query and a set of key-value pairs to vectors output.</p> <ul> <li>Output is weighted sum of values</li> <li>The weight has another compatibility function to calculate</li> </ul> <h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4> <ul> <li>Input: <ul> <li><strong>Q</strong>: queries</li> <li><strong>K</strong>: keys of dimension $d_k$</li> <li><strong>V</strong>: values of dimension $d_v$</li> </ul> </li> </ul> \[Attention (Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V\] <ul> <li>$\frac{1}{\sqrt{d_k}}$: scaling factor. Why? To avoid <code class="language-plaintext highlighter-rouge">softmax</code> is pushed into regions result extremely small gradients</li> <li><strong>Dot-product attention</strong> faster and more space-efficient in practice than <strong>additive attention</strong></li> </ul> <h4 id="multi-head-attention">Multi-Head Attention</h4> <p>They perform the attention function in parallel, yielding $d_v$-dim output values. They are concatenated and projected, make final values.</p> <p>\(MultiHead(Q,K,V) = Concat(head_1,.... head_n)W^O\) \(head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\)</p> <h4 id="attention-in-transformer">Attention in Transformer</h4> <p>We will remind the input of attention. We will have Q (Queries), K (keys) and V (values). <code class="language-plaintext highlighter-rouge">Transformer</code> uses attention in 3 ways:</p> <ul> <li>‚Äúencoder-decoder attention‚Äù layer: Q from previous decoder layer, K, V from output of encoder.</li> <li>‚Äúencoder self-attention‚Äù layer: Q, K, V from previous encoder layer.</li> <li>‚Äúdecoder self-attention‚Äù layer: Similar to encoder one, however they masked out all values in the input of <code class="language-plaintext highlighter-rouge">softmax</code> (set to $-\infty$) in scaled dot-product attention to illegal connections.</li> </ul> <h3 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h3> <p>FFN has 2 linear transformations with a ReLU activation<d-footnote>Rectified Linear Unit (ReLU) is a piecewise linear function that outputs the input directly if it is positive; otherwise, it outputs zero</d-footnote>:</p> \[FFN(x) = max(0, xW_1+b_1)W_2 +b_2\] <h3 id="positional-encoding">Positional Encoding</h3> <p>This is the method to inject information about the relative or absolute position of the tokens in the sequence. <strong>Positional Encoding</strong> has the same dimension $d_{model}$ as the embeddedings. In this work, they use sine and cosine functions</p> \[PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})\] \[PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})\] <h2 id="moh-mixture-of-heads">MoH (Mixture-of-Heads)</h2> <p>In my understanding, MoH <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> is a mix of Mixture-of-Experts (MoE) and <code class="language-plaintext highlighter-rouge">transformer</code> <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a>.</p> <p>They made 2 important changes, one, there is a TopK router to activate heads for each token. They also replace the standard summation in multi-head attention to weighted sum.</p> <p>They believe that with changes, they made 2 significant advantages:</p> <ul> <li>First, allows each token select most relevant attention heads, improve efficiency without sacrificing accuracy or increasing the params.</li> <li>Second, with weighted sum, MoH enhances the flexibility of attention mechanism.</li> </ul> <h3 id="design">Design</h3> <p>The core of the work is <strong>MoH</strong>, which treats attention heads as experts.</p> \[MoH(X, X') = \sum^h_{i=1} g_i H^i W^i_O\] <ul> <li>$X, X‚Äô$: input tokens</li> <li>$g_i$: routing score</li> <li>$H^i$: Head ith</li> <li>$W^i_O$: output of projection matrix</li> </ul> <p>Inspired by DeepSeek <a class="citation" href="#dai2024deepseekmoe">(Dai et al., 2024)</a>, MoH designs a subset of heads as <strong>shared heads</strong> that remain always activated. This will consolidate common knowledge within shared heads.</p> <p><strong>Two-Stage Routing</strong> for dynamically balance the weights between shared and routed heads. Routing scores are determined by both the <strong>score of each individual head</strong> and <strong>score associated with the head type</strong>. To avoid the unbalanced load, MoH applies <strong>Load Balance Loss</strong>.</p> <h3 id="training">Training</h3> <p>Training LLMs from scratch, they use Megatron-LM <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> with public datasets.</p> <p>With Continual Learning, they tuned <code class="language-plaintext highlighter-rouge">LLaMA3-8B</code>. 3 challenges when doing this:</p> <ol> <li>Determine the shared attention heads</li> <li>Add head routers</li> <li>Weight attention heads</li> </ol> <hr/> <p>That‚Äôs all for the day. The next post I will discuss about GaCLLM and how I imagine the system will work.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="goh"/><category term="english"/><summary type="html"><![CDATA[the first literature review of my series writting about my work I called Graph-of-Heads]]></summary></entry><entry><title type="html">My Life Setup - Summer 2025</title><link href="https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25/" rel="alternate" type="text/html" title="My Life Setup - Summer 2025"/><published>2025-06-26T17:30:00+00:00</published><updated>2025-06-26T17:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/my-life-setup-summer-25/"><![CDATA[<p>When I write this, my life is a mess. I am drown in endless fantasies. I write to encourage and clarify for myself. I hope that I can follow what I expect here.</p> <h2 id="routines-tasks-and-mental-health">Routines, Tasks and Mental Health</h2> <h3 id="routineflow">RoutineFlow</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://routineflow.app/">RoutineFlow App</a></li> </ul> <p>I highly recommend this app, the author is GOAT. I have used this app since beta, and purchased its lifetime premium after that shortly.</p> <p>Its clean UI and concept attract me greatly. If you have read <a href="https://jamesclear.com/atomic-habits">Atomic Habit</a> and want to have an app implemented to work with the principles of this book, this app is for you.</p> <p>The app is improved through the time, now it is really polished and smooth with template routines and widget.</p> <p>I use this app for morning and evening routine, mainly for personal hygiene and calm myself down.</p> <h3 id="ticktick">TickTick</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: <a href="https://ticktick.com/r?c=pfdhlnyn">TickTick</a></li> </ul> <p>The ultimate core of my system. I have use TickTick for more than 2 years, even though I have slacked off recently. But I am going to renew the yearly subcription soon! I use TickTick for everything and trust me bro, it‚Äôs the best app out there.</p> <p>The features are rolled out frequently, the price is acceptable, it is all-in-one you need! Task lists, Timer, Calendar, Habits and now is Countdown!</p> <p>Some people complaint about the UI but I feel it‚Äôs okay?</p> <ul> <li>Some tips and guides from TickTick Blog: <a href="https://www.ticktick.com/resources">TickTick Resources</a></li> </ul> <h3 id="tochi">Tochi</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://play.google.com/store/apps/details?id=com.lazyhippo.tochidiary&amp;hl=en-US&amp;pli=1">Tochi</a></li> </ul> <p>It is my Mood tracker, I also bought its lifetime premium. I love one-time payment app üòÜ It‚Äôs cute, has tags, can attach images and the orbs are cute.</p> <p>I can track my mood a lot of time through the day. Some apps only allow an input a day.</p> <h3 id="1money">1Money</h3> <ul> <li>Platform: Android</li> <li>Download: <a href="https://play.google.com/store/apps/details?id=org.pixelrush.moneyiq">1Money</a></li> </ul> <p>As I researched it seems has some drama around the app? But tbh I don‚Äôt care, the app is solid enough to track your money expense. The UI is clean and the functions is enough.</p> <h2 id="self-study">Self-Study</h2> <h3 id="chatgpt">ChatGPT</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: Everyone knows it üòÑ</li> </ul> <p>I guess this is kinda like a love-hate relationship? Sometimes I feel I depend on it, sometimes it helped me a lot. Recently, I find out I can be open with it more! I have spent a lot of my time talking with AI, from AI chatbot service for your fantasies to AI like BingAI or ChatGPT. I think that using it as an assistant, you provide options, I am the one who decide is good.</p> <h3 id="obsidian">Obsidian</h3> <ul> <li>Platform: Cross-Platform</li> <li>Download: <a href="https://obsidian.md/download">Obsidian</a></li> </ul> <p>It‚Äôs a famous and powerful free tool. It‚Äôs very beneficial when you know Markdown and community plugins. I can sync my Zotero notes with Obsidian. But ye, after finishing the Thesis, I deleted Zotero lol.</p> <p>I think beside storing your notes with easy markdown, I love the Spaced Repition of Obsidian. Unlike Anki, which I usually don‚Äôt like because the flashcards can be out of context, I can make flashcards from my note with Obsidian. Works very well tho.</p> <h3 id="languages">Languages</h3> <p>For English, my biggest problem is Speaking Skill. I use <a href="https://play.google.com/store/apps/details?id=us.nobarriers.elsa&amp;hl=vi&amp;gl=US&amp;pli=1"><strong>Elsa Speak</strong></a> for this. I have a Lifetime Elsa Pro account. The Premium version has AI features but I don‚Äôt think it‚Äôs necessary. I only need to enhance my pronunciation.</p> <p>And with German, beside having a small textbook, I also use <a href="https://www.memrise.com/">Memrise</a>, again, a lifetime premium account. Memrise has a lot of updates recently, I think it‚Äôs a solid app. I also used Busuu before, but after the yearly subcription, I didn‚Äôt extend it. Just because I am lazy, not because of the app. I think it‚Äôs also a good app for you to try. Duolingo? I stop using Duolingo as the free version is so limited and frustrated.</p> <h3 id="mooc">MOOC</h3> <p>Coursera is my go-to solution as I bought a Plus subcription :laughing:. I think it‚Äôs more academic than Udemy. I have Harmonica and Calculus courses on Udemy but haven‚Äôt finished (yet).</p> <p>I have some Python-related and Embedded Systems-related courses on Coursera right now. Hopefully I can finish some in this summer.</p> <h2 id="work">Work</h2> <p>Here I am, in the ecosystem of Microsoft.</p> <h3 id="ms-todo">MS Todo</h3> <p>MS Todo is a poor todo app to me comparing to my dear Ticktick. But it‚Äôs also not so bad with My Day feature. I guess it‚Äôs the only thing TickTick should adapt, Do Day is different than Due Day.</p> <p>Also, when you flag an Outlook email, it will appear in Todo, which helps you keep track of emails you need to take action.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="languages"/><category term="plans"/><category term="english"/><summary type="html"><![CDATA[my routines and tools used for summer 2025]]></summary></entry><entry><title type="html">Tutorial - Megatron-SWIFT and Qwen2.5 Installation</title><link href="https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation/" rel="alternate" type="text/html" title="Tutorial - Megatron-SWIFT and Qwen2.5 Installation"/><published>2025-06-24T09:45:00+00:00</published><updated>2025-06-24T09:45:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/megatron-swift-installation/"><![CDATA[<p>This tutorial based a lot on my experience with my company‚Äôs servers. So maybe there is some things not applicable in your case. Leave comment if you need anything to discuss.</p> <h2 id="purpose">Purpose</h2> <p>I am trying to fine-tunning a model with Mixture-of-Experts (MoE) <a class="citation" href="#sanseviero2023moe">(Sanseviero et al., 2023)</a> methodology. I choose <strong>Megatron-LM</strong> <a class="citation" href="#shoeybi2019megatron">(Shoeybi et al., 2019)</a> and <strong>SWIFT</strong> <a class="citation" href="#zhao2025swift">(Zhao et al., 2025)</a> as the framework.</p> <p>The tutorial I am following is: <a href="https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html">Megatron-SWIFT Training</a>.</p> <h2 id="prerequisites">Prerequisites</h2> <ul> <li>Operating System: <strong>Linux</strong></li> <li><strong>Python</strong> should be pre-installed. Check if your OS already has Python. <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 <span class="nt">--version</span>
</code></pre></div> </div> </li> <li>If your OS doesn‚Äôt have Python yet, run below commands to install (this apply for Ubuntu, if you use different distro, google the tutorial for your OS). <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>python3 python3-pip
</code></pre></div> </div> </li> <li>Using virtual environment is a good practice for Python, I use <code class="language-plaintext highlighter-rouge">anaconda</code> for this. Follow <a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html">this guide</a> to install <code class="language-plaintext highlighter-rouge">anaconda</code> on Linux.</li> <li>If you want to train with GPU, you need to install <code class="language-plaintext highlighter-rouge">cuda</code>: <a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">CUDA Installation Guide for Linux</a>. <strong>Recommended version</strong>: <code class="language-plaintext highlighter-rouge">12.1.0</code>.</li> <li>With this framework, you also need to install <code class="language-plaintext highlighter-rouge">cuDNN</code>: <a href="https://docs.nvidia.com/deeplearning/cudnn/installation/latest/linux.html">Installing cuDNN Backend on Linux</a>. <strong>Recommended version</strong>: <code class="language-plaintext highlighter-rouge">9</code>.</li> </ul> <h2 id="install-megatron-swift">Install Megatron-SWIFT</h2> <p>First, we will create a virtual environment with <code class="language-plaintext highlighter-rouge">conda</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">--name</span> &lt;ENV_NAME&gt; <span class="nv">python</span><span class="o">=</span>3.10
conda activate &lt;ENV_NAME&gt;
</code></pre></div></div> <p>Then we will install <code class="language-plaintext highlighter-rouge">pytorch</code> and <code class="language-plaintext highlighter-rouge">torchvision</code> first.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span><span class="nv">pytorch</span><span class="o">==</span>2.3.0 <span class="nv">torchvision</span><span class="o">==</span>0.18.0
</code></pre></div></div> <p>Next we need to install <code class="language-plaintext highlighter-rouge">apex</code>, <code class="language-plaintext highlighter-rouge">transformer-engine</code>, and <code class="language-plaintext highlighter-rouge">ms-swift</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/NVIDIA/apex
<span class="nb">cd </span>apex
pip <span class="nb">install</span> <span class="nt">-v</span> <span class="nt">--disable-pip-version-check</span> <span class="nt">--no-cache-dir</span> <span class="nt">--no-build-isolation</span> <span class="nt">--config-settings</span> <span class="s2">"--build-option=--cpp_ext"</span> <span class="nt">--config-settings</span> <span class="s2">"--build-option=--cuda_ext"</span> ./
pip <span class="nb">install </span>transformer-engine
pip <span class="nb">install </span>ms-swift
</code></pre></div></div> <h2 id="download-qwen25">Download Qwen2.5</h2> <h3 id="install-git-lfs">Install <code class="language-plaintext highlighter-rouge">git lfs</code></h3> <p>Check availability of <code class="language-plaintext highlighter-rouge">git</code> and <code class="language-plaintext highlighter-rouge">git lfs</code>.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git <span class="nt">--version</span>
git lfs version
</code></pre></div></div> <p>If your enviroment still not have <code class="language-plaintext highlighter-rouge">git-lfs</code> you need to install it</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">install </span>conda-forge::git-lfs
</code></pre></div></div> <h3 id="clone-model-repo">Clone model repo</h3> <p><strong>Qwen2.5</strong> <a class="citation" href="#qwen2.5">(Team, 2024)</a> is the model I use to train. First we will visit <a href="https://huggingface.co/">HuggingFace</a> to create an account. Then visit <strong>Profile &gt; Access Tokens &gt; Create new token</strong>. Choose <strong>Token Type</strong> is <strong>Write</strong>. Remember to <strong>copy the token</strong>.</p> <p>Return to our activated conda environment. Install <code class="language-plaintext highlighter-rouge">huggingface_hub</code>:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>huggingface_hub
</code></pre></div></div> <p>Then login into your <code class="language-plaintext highlighter-rouge">huggingface</code> token.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>huggingface-cli login <span class="nt">--token</span> &lt;your-token&gt;
</code></pre></div></div> <p>Finally, we can clone the model repo to our folder. Example: <a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen2.5-7B-Instruct</a></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> &lt;model folder&gt;
git lfs <span class="nb">install
</span>git clone https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
</code></pre></div></div> <h2 id="test">Test</h2> <p>Create a <code class="language-plaintext highlighter-rouge">test.sh</code> file to run test.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>0 <span class="se">\</span>
swift <span class="nb">export</span> <span class="se">\</span>
    <span class="nt">--model</span> &lt;model_dir&gt;/Qwen2.5-7B-Instruct <span class="se">\</span>
    <span class="nt">--to_mcore</span> <span class="nb">true</span> <span class="se">\</span>
    <span class="nt">--torch_dtype</span> bfloat16 <span class="se">\</span>
    <span class="nt">--output_dir</span> Qwen2.5-7B-Instruct-mcore
</code></pre></div></div> <hr/> <p>I am afraid that because I wrote the tutorial after finishing setup, so maybe there is some incompatible version and tweak steps that I forgot. So comment to tell me if you can‚Äôt follow the tutorial.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="english"/><summary type="html"><![CDATA[my tutorial to install Megatron-SWIFT to train Qwen2.5 locally]]></summary></entry><entry><title type="html">Mixture-of-Experts - first diggin‚Äô</title><link href="https://vtrnnhlinh.github.io/blog/2025/moe-overview/" rel="alternate" type="text/html" title="Mixture-of-Experts - first diggin‚Äô"/><published>2025-06-18T08:30:00+00:00</published><updated>2025-06-18T08:30:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/moe-overview</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/moe-overview/"><![CDATA[<p>At Bosch, I have a chance to discover about Machine Learning, specifically LLM and MoE. In this post I will share the content of my first presentation about Mixture-of-Experts (MoE). I take the structure and content mainly from a survey in 2025 <a class="citation" href="#mu2025comprehensive">(Mu &amp; Lin, 2025)</a> and some information from a survey in 2024 <a class="citation" href="#cai2024survey">(Cai et al., 2024)</a>.</p> <blockquote> <p><strong>Disclaimer</strong>: I am very noob in this field, I am not sure what I wrote in this post is true. But if I find out any problems, I will update.</p> </blockquote> <h2 id="why-moe">Why MoE?</h2> <p>AI applications are developing fast, we can say some popular names like ChatGPT, Gemini, DeepSeek,‚Ä¶ But developing it also faces some problems, 2 major problems are:</p> <ul> <li>Computational cost of training and deploying</li> <li>Integrating conflicting or heterogeneous knowledge within a single model</li> </ul> <p>So here we are, MoE is proposed to tackle these 2 problems. You can imagine MoE as a ‚Äúdivide-and-conquer‚Äù approach.</p> <h2 id="moe-components">MoE components</h2> <p>In MoE structure, we have two main parts: <strong>Experts</strong> and <strong>Router</strong>.</p> <h3 id="router">Router</h3> <p>Router works as a distributor to route data to suitable expert.</p> <p>We have <strong>Gating Function</strong> is the mathematical implementation of the Router. A good Gating Function meets 2 criteria:</p> <ul> <li>Accurately discern characteristics of both input data and experts</li> <li>Distribute evenly as possible among the predefined experts</li> </ul> <p>We can categorise Gating Function into 3 types:</p> <ul> <li><strong>Linear Gating</strong>: Using <code class="language-plaintext highlighter-rouge">softmax</code> function</li> <li><strong>Non-linear Gating</strong>: Using <strong>cosine similarity</strong> in assigning experts</li> <li><strong>Soft MoE</strong>: Combining tokens to avoid dropping tokens issues</li> </ul> <h3 id="experts">Experts</h3> <p>Experts are small LLM models that specialise in solving a defined dataset. The <strong>Experts Network</strong> based on Transformer <a class="citation" href="#vaswani2017attention">(Vaswani et al., 2017)</a> structure.</p> <p>There are 3 popular experts network method:</p> <ul> <li>Replace FFN layer in Transformer with an MoE layer <ul> <li>Suitable to incorporate sparse activation mechanisms</li> <li>Ideal choice for introducing the MoE mechanism</li> </ul> </li> <li>Apply MoE to the attention module in Transformer <ul> <li><strong>MoA</strong> <a class="citation" href="#wang2024moa">(Wang et al., 2024)</a> Mixture-of-Attention ‚Äì gating network to dynamically select the most relevant attention</li> <li><strong>MoH</strong> <a class="citation" href="#jin2024moh">(Jin et al., 2024)</a> Mixture-of-Head attention ‚Äì has great potential</li> </ul> </li> <li>Apply MoE to CNN layer <ul> <li>Fully leverage CNN‚Äôs strengths in local feature extraction</li> <li>Apply mainly in Computer Vision field</li> </ul> </li> </ul> <h2 id="moe-paths">MoE Paths</h2> <h3 id="routing-strategy">Routing Strategy</h3> <p><strong>Routing Strategy</strong> based on:</p> <ul> <li>Token-Level</li> <li>Modality-Level</li> <li>Task-Level</li> <li>Context-Level</li> <li>Attribute-Level</li> </ul> <h3 id="training-strategy">Training Strategy</h3> <p><strong>Training Strategy</strong> has 3 steps:</p> <ul> <li>Auxiliary Loss Function Design: balance usage and distribute load</li> <li>Expert Selection: choose expert for data input. Some popular methods like <em>TopK, Top1, TopP,‚Ä¶</em></li> <li>Pipeline Design: optimize resource allocation and distribute data among experts</li> </ul> <h2 id="my-current-work">My current work</h2> <p>I am trying to use <a href="https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html">Megatron-SWIFT</a> Framework to train <strong>Qwen2.5-7B-Instruct</strong> <a class="citation" href="#qwen2.5">(Team, 2024)</a>. It is really strugging even from first step is setup environment, when have some results, I will write post sharing about that. Hopefully I can write proper tutorial the next time we meet.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="ml"/><category term="moe"/><category term="english"/><summary type="html"><![CDATA[my first take when discovering MoE]]></summary></entry><entry><title type="html">I am done my Thesis, so what‚Äôs next?</title><link href="https://vtrnnhlinh.github.io/blog/2025/thesis-reflection/" rel="alternate" type="text/html" title="I am done my Thesis, so what‚Äôs next?"/><published>2025-06-18T04:00:00+00:00</published><updated>2025-06-18T04:00:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/thesis-reflection</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/thesis-reflection/"><![CDATA[<p>I submitted the hardcover of my Thesis to my Faculty, so what will I do in near future? I once shared some details in <a href="https://vtrnnhlinh.github.io/blog/2024/quantum-first-reflection/">Quantum first reflection</a>.</p> <p>But I think there are a lot of changes comparing to the post, so I will share some details, to clarify myself and give you some more information.</p> <h2 id="past">past</h2> <p>I believe I didn‚Äôt contribute much in my Thesis. That it prevents a lot of my vision if I pursue further education. My GPA is also too low in academic standard. I don‚Äôt want to struggle financially, to me, finance stable is the first thing I consider. But without scholarship, everything will be harder.</p> <p>After having an intership at BGSV, I feel like I prefer the environment that I have to sit (more than) 8 hours a day. My discipline is too horrible, lol. So maybe I won‚Äôt learn master, at least for now. But I also have other plans.</p> <h2 id="current">current</h2> <p>I am researching about Mixture-of-Experts at company. At university, I still don‚Äôt graduate yet, still have some subjects left.</p> <p>My plan is focusing on my work at Bosch and gain some certificates on Coursera in this summer. After the internship, I expect to make 2 CVs: one in Static Testing and one in Machine Learning.</p> <p>I believe I should finish all my study program within 2025.</p> <h2 id="future">future</h2> <p>I don‚Äôt expect to be kept at Bosch, so I want to find a proper company and still accept that I don‚Äôt graduate yet.</p> <p>I want to pursue another bachelor degree, maybe in Linguistics or Psychology, still not decide yet.</p> <p>Mathematics is also an aspect I believe I should spend time on.</p> <p>That‚Äôs all.</p>]]></content><author><name></name></author><category term="Journal-of-Sciences"/><category term="cse"/><category term="plans"/><category term="english"/><summary type="html"><![CDATA[my sharing after finishing my thesis]]></summary></entry><entry><title type="html">I use Arch, btw</title><link href="https://vtrnnhlinh.github.io/blog/2025/arch/" rel="alternate" type="text/html" title="I use Arch, btw"/><published>2025-03-04T19:45:00+00:00</published><updated>2025-03-04T19:45:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2025/arch</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2025/arch/"><![CDATA[<p>At 2am, you know, holy hour of random ideas. I feel frustrated that I am using <strong>Pop!_OS 22.04</strong>. It‚Äôs already 2025, we should have 24.04 version. Took me a while to do the research, and I heard that the dev team is migrating Pop!_OS to COSMOS. A uncerntain feeling captured my mind.</p> <blockquote> <p>‚ÄúWhat if they just drop out half way?‚Äù</p> </blockquote> <h2 id="what-are-my-options">What are my options?</h2> <p>My mind wants to change to a new OS. What about Ubuntu? But it‚Äôs too basic and Gnome has too much flaws, the idea of installing extensions and stuffs make me exhausted.</p> <p>Fedora? Kali Linux? Debian? Too many distros to choose, life is too short.</p> <p>I want a fun and challenging OS to play with. And yes, I choose <strong>Arch</strong>.</p> <p>Finally becoming an Arch user seems cool tbh.</p> <h2 id="embrace-the-adventure">Embrace the adventure</h2> <p>It wasn‚Äôt easy to install arch. I tried to follow the tutorial from chatGPT but it sucks when come to partitioning part.</p> <p>I had to install arch twice to be able to boot in. The dual boot story made me stuck for a while.</p> <p>When I remove the media, boot into arch. <strong>Boom</strong>. It‚Äôs a black screen, no GUI no internet. Yup, I didn‚Äôt believe that I have to install internet service manually.</p> <p>Then I had to insert the USB third time to install internet and GUI. I choose KDE as it seems better than GNOME. (Or maybe the grass is greener on the other side).</p> <h2 id="my-arch-setup">My Arch Setup</h2> <p>I use <strong>Sweet Theme and Candy icons</strong> of this chad <a href="https://github.com/EliverLara">EliverLara</a></p> <p>My main coding editor: <strong>nvim with AstroNvim+Konsole</strong>. I don‚Äôt reuse my nvim setup at Pop!_OS. I tried to switch to Nvchad but I am not familiar with its workflow. Kitty terminal‚Äôs NerdFont problem made me feel tired.</p> <p>Terminal emulator: <strong>Alacritty</strong>. I love its opacity :wink:</p> <p>My note-taking and research setup: <strong>Zotero+Obsidian</strong>. I tried to use Mendeley but it‚Äôs too slow to me. And maybe I will write a tutorial another day to show my workflow of Zotero+Obsidian.</p> <p>Mail Client: <strong>Thunderbird</strong>. Can‚Äôt find better solution.</p> <p>Browser: <strong>Firefox</strong>. But with the drama recently about its privacy. I am considering other options.</p>]]></content><author><name></name></author><category term="Linh-the-Engineer"/><category term="cse"/><category term="nerdies"/><category term="english"/><summary type="html"><![CDATA[a brief story of my try on archlinux]]></summary></entry><entry><title type="html">ƒê·∫°i ƒë·ªôi tr∆∞·ªüng c·ªßa t√¥i - c·ªßa anh, c·ªßa ch√∫ng ta</title><link href="https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi/" rel="alternate" type="text/html" title="ƒê·∫°i ƒë·ªôi tr∆∞·ªüng c·ªßa t√¥i - c·ªßa anh, c·ªßa ch√∫ng ta"/><published>2024-11-03T09:15:00+00:00</published><updated>2024-11-03T09:15:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/dai-doi-truong-cua-toi/"><![CDATA[<p>ƒê√¢y l√† v·ªü ch√®o ƒë∆∞·ª£c kh√°ch VIP nh√† ta v√¥ c√πng t√¢m ƒë·∫Øc gi·ªõi thi·ªáu n√™n m√¨nh kh√¥ng th·ªÉ kh√¥ng xem. V√† ng∆∞·ªùi ·∫•y qu·∫£ th·ª±c ƒë√£ kh√¥ng khi·∫øn m√¨nh th·∫•t v·ªçng, m·ªôt v·ªü ch√®o s√¢u s·∫Øc v·ªÅ m·∫∑t n·ªôi dung v√† ·∫•n t∆∞·ª£ng v·ªÅ m·∫∑t d√†n d·ª±ng s√¢n kh·∫•u.</p> <p>ƒê·∫ßu ti√™n ph·∫£i n√≥i ƒë√¢y kh√¥ng ph·∫£i l√† k·ªãch b·∫£n g·ªëc m√† ƒë∆∞·ª£c chuy·ªÉn th·ªÉ t·ª´ v·ªü k·ªãch c√πng t√™n, m√¨nh s·∫Ω kh√¥ng xem n·ªët v·ªü k·ªãch r·ªìi ƒë√¢m bang so s√°nh.</p> <p>V·ªÅ k·ªãch b·∫£n, t√°c ph·∫©m l√†m v·ªÅ ƒë·ªÅ t√†i chi·∫øn tranh nh∆∞ng kh√¥ng c·ª• th·ªÉ m·ªôt tr·∫≠n chi·∫øn n√†o c·∫£. N√≥ gi·ªëng nh∆∞ m·ªôt l√°t c·∫Øt r·∫•t nh·ªè trong h∆°n 20 nƒÉm tr·ªùi Kh√°ng chi·∫øn ch·ªëng M·ªπ. M√† v√¨ v·∫≠y, ta th·∫•y ƒë∆∞·ª£c s√¢u s·∫Øc h∆°n v·ªÅ n·ªôi t√¢m c·ªßa c√°c chi·∫øn sƒ©. B·ªüi cu·ªôc chi·∫øn l·ªõn n√†o c≈©ng v·∫≠y, h·ªç d·ªÖ b·ªã coi l√† con s·ªë h∆°n l√† con ng∆∞·ªùi.</p> <p>C√°ch d√†n d·ª±ng c√≥ hai nh√¢n v·∫≠t trung t√¢m v·ªõi m·ªôt con ng∆∞·ªùi c√≥ t√†i nƒÉng nh∆∞ng t∆∞ t∆∞·ªüng v·∫´n c√≥ ph·∫ßn khi·∫øm khuy·∫øt c√πng m·ªôt con ng∆∞·ªùi tuy c√≥ th·ªÉ kh√¥ng gi·ªèi b·∫±ng nh∆∞ng th·∫≥ng th·∫Øn, ch√¢n t√¨nh khi·∫øn m√¨nh kh√¥ng kh·ªèi nh·ªõ t·ªõi t√°c ph·∫©m Chi·∫øn tranh v√† H√≤a b√¨nh c·ªßa Lev Tolstoy.</p> <p>V·ªü ch√®o l√† di·ªÖn bi·∫øn, ph√°t tri·ªÉn n·ªôi t√¢m c·ªßa nhi·ªÅu nh√¢n v·∫≠t, t·ª´ anh ƒë·∫°i ƒë·ªôi tr∆∞·ªüng t·ªõi s∆∞ ƒëo√†n tr∆∞·ªüng, t·ª´ ng∆∞·ªùi con t·ªõi ng∆∞·ªùi cha. V·ªü ch√®o c√≤n l√† c√°ch x·ª≠ l√≠ c√°c m·ªëi quan h·ªá gi·ªØa ng∆∞·ªùi v·ªõi ng∆∞·ªùi trong th·ªùi ƒëi·ªÉm ƒë·∫°n bom √°c li·ªát, gi·ªØa anh y√™u em y√™u, gi·ªØa c·∫•p tr√™n c·∫•p d∆∞·ªõi, gi·ªØa cha con, v√† gi·ªØa ƒë·ªìng ƒë·ªôi v·ªõi nhau. ƒê·ªÉ r·ªìi b·∫±ng t√¨nh c·∫£m y√™u th∆∞∆°ng, th√°i ƒë·ªô th·∫≥ng th·∫Øn ch√¢n th√†nh m√† c√°c nh√¢n v·∫≠t c·ªßa ch√∫ng ta tr∆∞·ªüng th√†nh h∆°n, c√πng c·ªëng hi·∫øn cho th·∫Øng l·ª£i chung.</p> <p>V·ªÅ d√†n d·ª±ng s√¢n kh·∫•u, m√¨nh ·∫•n t∆∞·ª£ng nh·∫•t ·ªü 3 ph√¢n ƒëo·∫°n. ƒê·∫ßu ti√™n l√† 2 nam ch√≠nh ƒëang c√£i nhau hƒÉng m√°u th√¨ h·ªìi t∆∞·ªüng r·ªìi sang s√¥ng lu√¥n, √¥i th·ªÅ l√† n√≥ m∆∞·ª£t nh∆∞ sunsilk. Th·ª© hai l√† ƒëo·∫°n mi√™u t·∫£ n·ªôi t√¢m c·ªßa ƒë·ª©a con nh·ªõ m·∫π, b·ªüi v√¨ lo·∫°i h√¨nh s√¢n kh·∫•u th∆∞·ªùng n·∫∑ng v·ªÅ k·ªÉ, l√†m nh·ªØng ƒëo·∫°n ƒë·ªôc tho·∫°i cho hay kh√¥ng ph·∫£i d·ªÖ v√† h·ªç l√†m hay t·ªõi m·ª©c m√¨nh b·∫•t ng·ªù. Th·ª© ba l√† ph√¢n c·∫£nh 16+ nh·ªè c·ªßa ƒë√¥i chim cu d∆∞·ªõi h·∫ßm, d·ª±ng r·∫•t kh√©o, kh√°n gi·∫£ xem bi·∫øt ng∆∞·ªùi ta ƒëang chim chu·ªôt nh∆∞ng kh√¥ng b·ªã th√¥ thi·ªÉn. ƒê√°nh ƒë√®n c≈©ng r·∫•t c√≥ √Ω t·ª©, nhi·ªÅu c·∫£nh th√†nh b·∫°i l√† nh·ªù √°nh s√°ng lu√¥n. C√°ch ƒëi·ªÅu khi·ªÉn nh·ªãp ƒë·ªô c√¢n b·∫±ng gi·ªØa bi v√† h√†i, gi·ªØa l√∫c cƒÉng th·∫≥ng v√† b√¥ng l∆°i c≈©ng l√† m·ªôt ƒëi·ªÉm ƒë√°ng khen ng·ª£i, kh√¥ng khi·∫øn ng∆∞·ªùi xem th·∫•y nh√†m ch√°n hay cƒÉng th·∫≥ng qu√° ƒë·ªô.</p> <p>T·ª±u chung, ƒê·∫°i ƒë·ªôi tr∆∞·ªüng c·ªßa t√¥i c√≥ h√†m l∆∞·ª£ng ngh·ªá thu·∫≠t ƒë·ªß cao, c√≥ n·ªôi dung ƒë·ªß chi·ªÅu s√¢u v√† ph·ª©c t·∫°p ƒë·ªÉ ng·∫´m nghƒ©. M·ªôt t√°c ph·∫©m ƒë√°ng xem.</p> <p>C·∫£m ∆°n v√¨ ƒë·∫°i ca ƒë√£ gi·ªõi thi·ªáu th√™m m·ªôt t√°c ph·∫©m ƒë√°ng nh·ªõ v·ªÅ m·∫∑t n·ªôi dung l·∫´n ngh·ªá thu·∫≠t ch·ª© kh√¥ng ph·∫£i v√¨ ·ª©c ch·∫ø n√™n ƒë√°ng nh·ªõ. ƒê√∫ng l√† ƒë·∫°i ca c·ªßa b·ªçn em c√≥ kh√°c, tr·ªôm v√≠a, tr·ªôm v√≠a.</p>]]></content><author><name></name></author><category term="Stories-of-Culture"/><category term="ch√®o"/><category term="vietnamese"/><summary type="html"><![CDATA[C·∫£m nh·∫≠n v·ªÅ v·ªü ch√®o ƒê·∫°i ƒë·ªôi tr∆∞·ªüng c·ªßa t√¥i]]></summary></entry><entry><title type="html">KhƒÉn Pi√™u r∆°i ·ªü ch·ªën n√†o - ƒê·ªÉ ng∆∞·ªùi kh√°ch c≈© lao xao trong l√≤ng</title><link href="https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon/" rel="alternate" type="text/html" title="KhƒÉn Pi√™u r∆°i ·ªü ch·ªën n√†o - ƒê·ªÉ ng∆∞·ªùi kh√°ch c≈© lao xao trong l√≤ng"/><published>2024-09-10T14:14:00+00:00</published><updated>2024-09-10T14:14:00+00:00</updated><id>https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon</id><content type="html" xml:base="https://vtrnnhlinh.github.io/blog/2024/chiec-khan-pieu-ca-lon/"><![CDATA[<p>ƒê·∫°i vƒÉn h√†o Victor Hugo t·ª´ng c√≥ m·ªôt c√¢u n√≥i m√† m√¨nh t·∫°m d·ªãch l√†:<br/> ‚Äú<em>L·ªãch s·ª≠ l√† g√¨? L√† ti·∫øng v·ªçng c·ªßa qu√° kh·ª© t·ªõi t∆∞∆°ng lai; l√† ph·∫£n chi·∫øu c·ªßa t∆∞∆°ng lai v·ªÅ qu√° kh·ª©</em>‚Äù.</p> <p>Ng∆∞·ª£c d√≤ng th·ªùi gian, quay v·ªÅ v·ªõi ‚ÄúCh√≠n nƒÉm l√†m m·ªôt ƒêi·ªán Bi√™n/ N√™n c√†nh hoa ƒë·ªè n√™n thi√™n s·ª≠ v√†ng‚Äù, ta c√≥ nh·∫°c sƒ© Do√£n Nho th·ªùi trai tr·∫ª ph·ª•ng s·ª± cho C√°ch m·∫°ng, cho ƒê·ªôc l·∫≠p, T·ª± do c·ªßa D√¢n t·ªôc. Khi ·∫•y, t√¨nh y√™u qu√™ h∆∞∆°ng ƒë·∫•t n∆∞·ªõc, t√¨nh y√™u ƒë·ªìng b√†o ƒë√£ v∆∞·ª£t l√™n tr√™n t·∫•t c·∫£ th·ª© t√¨nh c·∫£m kh√°c.</p> <p>M√† nh·∫Øc t·ªõi ƒë√¢y, ai l·∫°i kh√¥ng nh·ªõ t·ªõi b√†i th∆° <em>Vi·ªát B·∫Øc</em> c·ªßa T·ªë H·ªØu:</p> <blockquote> <p>‚Äù- M√¨nh v·ªÅ m√¨nh c√≥ nh·ªõ ta?<br/> M∆∞·ªùi lƒÉm nƒÉm ·∫•y thi·∫øt tha m·∫∑n n·ªìng.<br/> M√¨nh v·ªÅ m√¨nh c√≥ nh·ªõ kh√¥ng<br/> Nh√¨n c√¢y nh·ªõ n√∫i, nh√¨n s√¥ng nh·ªõ ngu·ªìn?‚Ä¶‚Äù</p> </blockquote> <p>ƒê·∫øn 1‚Äì2 nƒÉm sau chi·∫øn th·∫Øng, nh·∫°c sƒ© m·ªõi quay l·∫°i ch·ªën x∆∞a, l√≤ng l·∫°i r·ªôn ch√∫t ‚Äút√¨nh ri√™ng‚Äù t·ª´ b√†i d√¢n ca d√¢n t·ªôc X√° <em>TƒÉng A Tim</em> ƒë·ªÉ vi·∫øt b√†i h√°t ‚ÄúChi·∫øc khƒÉn r∆°i‚Äù, sau ƒë·ªïi t√™n th√†nh ‚ÄúChi·∫øc khƒÉn Pi√™u‚Äù. KhƒÉn Pi√™u l√† chi·∫øc khƒÉn c·ªßa c√¥ g√°i d√¢n t·ªôc Th√°i d√πng ƒë·ªÉ l√†m t√≠n v·∫≠t cho t√¨nh y√™u c·ªßa m√¨nh.</p> <p>B√†i h√°t ƒë∆∞·ª£c ph√°t tri·ªÉn t·ª´ d√¢n ca d√¢n t·ªôc X√°, h√¨nh t∆∞·ª£ng ch√≠nh l√† chi·∫øc khƒÉn c·ªßa d√¢n t·ªôc Th√°i, ph·∫£i chƒÉng ƒë√¢y ch√≠nh l√† m·ªôt c√¢u tr·∫£ l·ªùi cho c√¢u ‚ÄúM√¨nh v·ªÅ m√¨nh c√≥ nh·ªõ ta?‚Äù c·ªßa ng∆∞·ªùi mi·ªÅn xu√¥i cho ng∆∞·ªùi mi·ªÅn ng∆∞·ª£c?</p> <hr/> <p><strong>Quay v·ªÅ hi·ªán t·∫°i t·ªõi v·ªõi ti·∫øt m·ª•c c·ªßa Nh√† C√° L·ªõn.</strong></p> <blockquote> <p>‚ÄúKhƒÉn Pi√™u d·ªát gi√≥ v∆∞∆°n c√†nh,</p> <p>Tr√°i tim ng∆∞·ªùi l√≠nh qu√¢n h√†nh x·ªën xang.</p> <p>B∆∞·ªõc ch√¢n m·∫•y ·∫£i quan san,</p> <p>Sau l∆∞ng ƒëi·ªÉm t·ª±a b·∫£n l√†ng qu√™ h∆∞∆°ng!‚Äù</p> </blockquote> <p>4 c√¢u th·ªÉ l·ª•c b√°t, th·ªÉ th∆° truy·ªÅn th·ªëng d√¢n t·ªôc ta m·ªü ƒë·∫ßu cho ti·∫øt m·ª•c Nh√† C√° L·ªõn.</p> <p>Nghe ch√∫ T·ª± Long ng√¢m, l√≤ng l·∫°i nghƒ© t·ªõi m·∫•y c√¢u trong <em>Chinh Ph·ª• Ng√¢m</em> c·ªßa ƒê·∫∑ng Tr·∫ßn C√¥n:</p> <blockquote> <p>‚ÄúTrang phong l∆∞u ƒëang ch·ª´ng ni√™n thi·∫øu,</p> <p>S√°nh nhau c√πng dan d√≠u ch·ªØ duy√™n.</p> <p>N·ª° n√†o ƒë√¥i l·ª©a thi·∫øu ni√™n,</p> <p>Quan san ƒë·ªÉ c√°ch h√†n huy√™n cho ƒë√†nh!‚Äù</p> </blockquote> <p><em>Chinh Ph·ª• Ng√¢m</em> l√† kh√∫c ai o√°n c·ªßa ng∆∞·ªùi thi·∫øu ph·ª• c√≥ ch·ªìng ra tr·∫≠n, mang ƒë·∫≠m n·ªói lo cho t√¨nh y√™u l·ª©a ƒë√¥i c·ªßa m√¨nh. V·ªõi Nh√† C√° L·ªõn, t√¨nh y√™u ƒë√¥i l·ª©a kh√¥ng d·ª´ng l·∫°i ·ªü ch√∫t t√¨nh ri√™ng nam n·ªØ, m√† ƒë∆∞·ª£c ph√°t tri·ªÉn l√™n th√†nh t√¨nh y√™u qu√™ h∆∞∆°ng ƒë·∫•t n∆∞·ªõc.</p> <hr/> <p>Trong t√°c ph·∫©m <em>Ho√†ng t·ª≠ b√©</em> c·ªßa Antoine de Saint-Exup√©ry c√≥ m·ªôt c√¢u nh∆∞ th·∫ø n√†y:<br/> ‚Äú<em>‚Ä¶Khi √¥ng nh√¨n tr·ªùi, b·ªüi v√¨ ·ªü m·ªôt trong nh·ªØng ng√¥i sao ƒë√≥ c√≥ t√¥i, b·ªüi v√¨ trong m·ªôt ng√¥i sao ƒë√≥ c√≥ t√¥i c∆∞·ªùi, n√™n √¥ng s·∫Ω t∆∞·ªüng ch·ª´ng t·∫•t c·∫£ c√°c ng√¥i sao ƒë·ªÅu c∆∞·ªùi‚Ä¶</em>‚Äù</p> <p>Ph·∫£i chƒÉng ·ªü ƒë√¢y Nh√† C√° L·ªõn c≈©ng nghƒ© nh∆∞ v·∫≠y ‚Äî khi nghƒ© v·ªÅ b·∫£n l√†ng, b·ªüi v√¨ ·ªü m·ªôt trong nh·ªØng b·∫£n l√†ng ƒë√≥ c√≥ ng∆∞·ªùi con g√°i trao anh chi·∫øc khƒÉn Pi√™u, n√™n t·∫•t c·∫£ b·∫£n l√†ng tr·ªü th√†nh ƒëi·ªÉm t·ª±a, l√≠ do ƒë·ªÉ anh chi·∫øn ƒë·∫•u.</p> <hr/> <p><strong>V√† ƒëo·∫°n X-part c·ªßa Soobin ƒë∆∞·ª£c vi·∫øt nh∆∞ sau:</strong></p> <blockquote> <p>‚ÄúT√¥i b∆∞·ªõc tr√™n con ƒë∆∞·ªùng gai ƒë·∫ßy</p> <p>Mang theo c·∫£ T·ªï qu·ªëc tr√™n vai v·∫´n c√≤n ƒëong ƒë·∫ßy</p> <p>Th√¢n nam m∆∞·ªùi t·∫•c anh ch·∫≥ng qu·∫£n n·∫Øng tr∆∞a</p> <p>BƒÉng ng√†n d·∫∑m ƒë·ªìi tr√πng m·ªãt m√πng ch·∫≥ng ng·∫°i gi√≥ m∆∞a</p> <p>KhƒÉn Pi√™u ch·ªù ai, n∆°i ƒë√¢y c·ªè h∆∞∆°ng ƒë√¢m ch·ªìi r√≥t m·∫≠t v√†o tai</p> <p>Ch·ªù m·ªôt ng√†y hai ta chung ƒë√¥i d√π cho hai n∆°i xa x√¥i‚Äù</p> </blockquote> <p>Ngo√†i t√≠nh bi·ªÉu tr∆∞ng r·∫•t cao v·ªÅ nh·ªØng ng∆∞·ªùi l√≠nh bi√™n ph√≤ng, v·ªÅ kh√°t v·ªçng t√¨nh y√™u l·ª©a ƒë√¥i, m√¨nh c√≤n kh√¥ng kh·ªèi li√™n t∆∞·ªüng t·ªõi h√¨nh ·∫£nh v·ªÅ m·ªôt b·∫≠c c√°i th·∫ø anh h√πng ‚Äî c·ª• th·ªÉ l√† T·ª´ H·∫£i trong <em>Truy·ªán Ki·ªÅu</em> c·ªßa ƒë·∫°i thi h√†o Nguy·ªÖn Du:</p> <blockquote> <p>‚ÄúR√¢u h√πm, h√†m √©n, m√†y ng√†i</p> <p>Vai nƒÉm t·∫•c r·ªông, th√¢n m∆∞·ªùi th∆∞·ªõc cao.‚Äù</p> </blockquote> <p>M√¨nh r·∫•t y√™u th√≠ch c√°ch nh·ªØng b√†i h√°t c√≥ b·ªÅ d√†y l·ªãch s·ª≠ ƒë∆∞·ª£c vi·∫øt th√™m ph·∫ßn X-part v·∫´n h√≤a h·ª£p v·ªõi b√†i g·ªëc b·ªüi s·ª± tinh t·∫ø v√† chi·ªÅu s√¢u trong t·ª´ng ch·ªØ ƒë∆∞·ª£c th√™m v√†o.</p> <hr/> <p>Ng√†y x∆∞a nh·∫°c sƒ© Do√£n Nho, ch√†ng l√≠nh tr·∫ª g·∫Øn b√≥ v·ªõi Vi·ªát B·∫Øc n·∫£y n·ªü rung ƒë·ªông ch√∫t t√¨nh ri√™ng t·ª´ c√°i t√¨nh chung vi·∫øt ra b√†i h√°t n√†y. M·∫•y m∆∞∆°i nƒÉm sau, Nh√† C√° L·ªõn th·ªÉ hi·ªán l·∫°i d·ª±a v√†o c√°i t√¨nh ri√™ng ƒë√≥ d·∫´n ƒë·∫øn t√¨nh chung, c√≤n ƒë∆∞·ª£c vi·∫øt d√†nh t·∫∑ng cho nh·ªØng ng∆∞·ªùi l√≠nh bi√™n ph√≤ng.</p> <p><strong>C√≥ ph·∫£i ƒë√¢y ch√≠nh l√† c√°ch qu√° kh·ª© v·ªçng t·ªõi t∆∞∆°ng lai, t∆∞∆°ng lai ph·∫£n chi·∫øu v·ªÅ qu√° kh·ª©?</strong></p> <p><strong>C√≥ ph·∫£i ƒë√¢y ch√≠nh l√† l√≠ do nh·∫°c sƒ© Do√£n Nho ‚Äúr·∫•t x√∫c ƒë·ªông‚Äù khi ƒë∆∞·ª£c nghe t√°c ph·∫©m n√†y?</strong></p> <p><strong>C√≥ ph·∫£i ƒë√¢y ch√≠nh l√† c√°ch m·ªôt t√°c ph·∫©m v∆∞·ª£t qua th·ª≠ th√°ch c·ªßa th·ªùi gian v√† c√°ch ng∆∞·ªùi ngh·ªá sƒ© th·ª±c th·ª• l√†m ngh·ªÅ?</strong></p> <hr/> <p>C√≤n ti t·ªâ th·ª© ƒë·ªÉ khen v·ªÅ c√°c anh Nh√† C√° L·ªõn trong b√†i h√°t n√†y nh∆∞ng m√¨nh xin ph√©p d·ª´ng l·∫°i ·ªü ƒë√¢y.</p> <p>R·∫•t x√∫c ƒë·ªông khi nghe b√†i h√°t, r·∫•t t·ª± h√†o v·ªÅ ch√∫ T·ª± Long khi ƒë∆∞·ª£c nghe ch√∫ chia s·∫ª v·ªÅ qu√° tr√¨nh l√†m b√†i h√°t n√†y.</p> <p><strong>P/S</strong>: Ng∆∞·ªùi vi·∫øt kh√¥ng c√≥ chuy√™n m√¥n v·ªÅ vƒÉn h√≥a - ngh·ªá thu·∫≠t n√™n kh√¥ng tr√°nh kh·ªèi sai s√≥t, r·∫•t m·ªü l√≤ng ƒë√≥n nh·∫≠n g√≥p √Ω.</p>]]></content><author><name></name></author><category term="Stories-of-Culture"/><category term="music"/><category term="vietnamese"/><summary type="html"><![CDATA[C·∫£m nh·∫≠n v·ªÅ ti·∫øt m·ª•c Chi·∫øc KhƒÉn Pi√™u c·ªßa nh√† C√° L·ªõn trong ch∆∞∆°ng tr√¨nh ATVNCG 2024]]></summary></entry></feed>