@article{mu2025comprehensive,
  title={A comprehensive survey of mixture-of-experts: Algorithms, theory, and applications},
  author={Mu, Siyuan and Lin, Sen},
  journal={arXiv preprint arXiv:2503.07137},
  year={2025}
}
@article{cai2024survey,
  title={A survey on mixture of experts},
  author={Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  journal={arXiv preprint arXiv:2407.06204},
  year={2024}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{jin2024moh,
  title={Moh: Multi-head attention as mixture-of-head attention},
  author={Jin, Peng and Zhu, Bo and Yuan, Li and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2410.11842},
  year={2024}
}
@inproceedings{wang2024moa,
  title={Moa: Mixture-of-attention for subject-context disentanglement in personalized image generation},
  author={Wang, Kuan-Chieh and Ostashev, Daniil and Fang, Yuwei and Tulyakov, Sergey and Aberman, Kfir},
  booktitle={SIGGRAPH Asia 2024 Conference Papers},
  pages={1--12},
  year={2024}
}
@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}
